[
  {
    "objectID": "projects/index.html",
    "href": "projects/index.html",
    "title": "Projects",
    "section": "",
    "text": "20-minute neighbourhoods: understanding housing density in Bradford\nDefining neighbourhoods using 800m buffers and identfying properties within these to understand housing density in the Bradford local authority district."
  },
  {
    "objectID": "projects/index.html#access-to-greenspace",
    "href": "projects/index.html#access-to-greenspace",
    "title": "Projects",
    "section": "Access to greenspace",
    "text": "Access to greenspace\n\n\n\nA spatial analysis of access to greenspace and deprivation in Bradford\nUsing a GWR model to analyse the relationship between nearest greenspace and dimensions of deprivation by Output Area within the Bradford local authority district."
  },
  {
    "objectID": "projects/access-to-greenspace/access-to-greenspace-bradford.html",
    "href": "projects/access-to-greenspace/access-to-greenspace-bradford.html",
    "title": "A spatial analysis of access to greenspace and deprivation in Bradford",
    "section": "",
    "text": "Return to the summary page or go back to the projects page."
  },
  {
    "objectID": "projects/access-to-greenspace/access-to-greenspace-bradford.html#introduction",
    "href": "projects/access-to-greenspace/access-to-greenspace-bradford.html#introduction",
    "title": "A spatial analysis of access to greenspace and deprivation in Bradford",
    "section": "1. Introduction",
    "text": "1. Introduction\nGreenspace is an important part of urban life, with benefits for humans and the environment. Analysing Bradford local authority district (LAD), this study builds on the growing literature addressing the challenge of greenspace accessibility by focusing on each deprivation dimension. Results show that considering deprivation and the relationship with greenspace in this greater level of detail is important for future policy and development."
  },
  {
    "objectID": "projects/access-to-greenspace/access-to-greenspace-bradford.html#background",
    "href": "projects/access-to-greenspace/access-to-greenspace-bradford.html#background",
    "title": "A spatial analysis of access to greenspace and deprivation in Bradford",
    "section": "2. Background",
    "text": "2. Background\n\n2.1. Why is greenspace important?\nNatural England (2023) estimates benefits of £6.6 billion in health, climate, and environmental gains from greenspace. However, a third of people do not have access to quality greenspace within a 15-minute walk of their home (Natural England, 2023). Public policy highlights commitments to improve this, with the Green Infrastructure Framework seeking to determine where greenspace is needed (Natural England, 2023).\n\n\n2.2. Why Bradford?\nBradford is the fifth largest district in England (CBMDC, 2021) and has a young population, ranking 4th for children aged 14 or less (21.4%) (CBMDC, 2022). Bradford is diverse – a third of people identify as Muslim, nearly 20% were born outside the UK, and 39% are Black, Asian, Mixed, or “Other” non-White ethnicities (ONS, 2023h). Over a fifth of the population have poor health, and a fifth are disabled (ONS, 2023h).\nGreenspace assets include extensive moorland and woodland as well as 36 council recognised parks (CBMDC, 2019). There are sports grounds and facilities, but not all are freely accessible. Limited utilisation data exists, but estimates suggest it is below national and regional averages, with surveys finding those most likely to utilise urban greenspace were ethnic minorities, households without a car, and people with children (CBMDC, 2019).\nThe council recognise the need to embed greenspace within long-term strategies. Key ambitions include ensuring greenspaces are safe and inclusive, improving health outcomes by prioritising investment for those most in need (CBMDC, 2021).\n\n\n2.3. Review of current literature\nAn increasing body of literature acknowledges the connection between greenspace access and overall health (Jia et al, 2021; Lachowycz and Jones, 2011; McCormick, 2017) and social benefits, including community cohesion (Barbosa et al, 2007). Methodologies vary, but most find deprived communities have the best access whilst utilisation levels are unclear, but perception of quality can limit use (Barbosa et al, 2007; Jones et al, 2009; Kessel et al, 2009; Roe et al, 2016). This quality issue is worse in deprived areas (Gidlow and Ellis, 2011; Mears et al, 2019).\nSeveral Bradford-specific studies exist. Mueller et al (2018) analysed health impacts and determined greenspace promotes an active lifestyle, and helps mitigate pollution, noise, and heat effects. McEachan et al (2018) demonstrated the positive impact greenspace had on children, but this impact differed by ethnicity, and perceived quality of greenspace was more important than quantity. Perception has also been linked to biodiversity and facilities (Wood et al, 2018)."
  },
  {
    "objectID": "projects/access-to-greenspace/access-to-greenspace-bradford.html#research-question",
    "href": "projects/access-to-greenspace/access-to-greenspace-bradford.html#research-question",
    "title": "A spatial analysis of access to greenspace and deprivation in Bradford",
    "section": "3. Research question",
    "text": "3. Research question\nNo literature reviewed analyses the dimensions of deprivation. This study aims to investigate whether there is a statistically significant relationship between greenspace access and each dimension of the 2021 UK Census deprivation measure. The null hypothesis is that there is not a statistically significant relationship present.\nThe remainder of this notebook will be structured as follows: Section 4 explains the datasets used; Section 5 outlines data wrangling processes preparing for analysis; Section 6 presents visual analyses and statistical descriptors of the data; Section 7 includes analysis and results of the OLS regression, Moran’s I and LISA cluster evaluation, and subsequent GWR model; Section 8 discusses findings, how this could influence policy, and further opportunities to improve this study; Section 9 draws conclusions from the findings."
  },
  {
    "objectID": "projects/access-to-greenspace/access-to-greenspace-bradford.html#data",
    "href": "projects/access-to-greenspace/access-to-greenspace-bradford.html#data",
    "title": "A spatial analysis of access to greenspace and deprivation in Bradford",
    "section": "4. Data",
    "text": "4. Data\n\n4.1. Import libraries\n# Set number of threads to recommended 7 to avoid errors in code during visualisation phase\nimport os\nos.environ[\"OMP_NUM_THREADS\"] = '7'\n# Dataframe libraries\nimport pandas as pd\nimport geopandas as gpd\n\n# Visualisation libraries\nimport matplotlib.pyplot as plt\nimport folium\nimport contextily as ctx\nimport seaborn as sns\nimport matplotlib.patches as mpatches\nimport matplotlib.colors as mcolors\nfrom matplotlib.lines import Line2D\n\n# Regression libraries\nimport statsmodels.api as sm # add statsmodels\nfrom statsmodels.sandbox.regression.predstd import wls_prediction_std # for predictions\n\n# Spatial analysis libraries\nfrom pysal.lib import weights\nfrom pysal.lib import io\nfrom pysal.explore import esda\nfrom pysal.viz import splot\nfrom splot.esda import moran_scatterplot, lisa_cluster, plot_local_autocorrelation\nfrom mgwr.gwr import GWR, MGWR\nfrom mgwr.sel_bw import Sel_BW\n\n# Other required libraries\n%matplotlib inline\nimport numpy as np\nfrom scipy.stats import probplot\nimport scipy.stats as stats\n\n\n4.2. Import each datafile\nLinks to datafiles are located in the appendix to this notebook and can be downloaded and saved locally using the links provided.\n\n4.2.1. Output Areas (OAs)\nOAs are the lowest census geography level, containing 40 to 250 households (ONS, 2021), enabling more accurate nearest greenspace measurements, and capturing localised differences in deprivation. Polygons and population weighted centroids (PWCs) are available from the ONS (2023a, 2023b). PWCs are used to measure distance to greenspace as it captures the nearest greenspace for most households. Polygons are aggregated to Bradford LAD level using the ONS (2023c) lookup. This is necessary to reduce the OA PWCs and greenspace to those within Bradford LAD.\n# OA polygons (BFC: Full resolution - clipped to the coastline (Mean High Water mark)) (ONS, 2023a)\nOA_polygons = gpd.read_file('Data/Output_Areas_2021_EW_BFC_V8/OA_2021_EW_BFC_V8.shp')\n\n# OA Population-weighted centroids (ONS, 2023b)\nOA_PWC = gpd.read_file('Data/Output_Areas_2021_PWC_V3/PopCentroids_EW_2021_V3.shp')\n# OA to LSOA to MSOA to LAD lookup (ONS, 2023c)\nOA_lookup = pd.read_csv('Data/Output_Area_Lookup_in_England_and_Wales_v3.csv')\n\n### Error can be ignored - it is because of Welsh spellings in a certain column.\n### This is not relevant to, nor will it affect, this study as these columns/rows will not be used.\n\n\n4.2.2. Greenspace\nGreenspace data (OS, 2023) include site (polygon) and access (point) data. However, not all sites have access data, hence only sites are used to ensure all eligible greenspace is captured. Distance from the PWC to nearest greenspace will be calculated using these polygons. Data includes parks, gardens, sports facilities, and religious grounds, but does not include forests, woodland, moorland or canal and river paths (OS, 2023).\n# Greenspace site polygons (OS, 2023)\nGB_greenspace_site = gpd.read_file('Data/OS Open Greenspace (ESRI Shape File) GB/data/GB_GreenspaceSite.shp')\n\n\n4.2.3. Independent variables\nEach deprivation dimension (education, employment, health and disability, and housing) can be obtained via custom ONS queries (ONS, 2023d; ONS, 2023e; ONS, 2023f; ONS, 2023g). These queries include only Bradford LAD OAs. Each file contains the number of households that are deprived or not deprived, from which percentage of households deprived in each dimension is calculated (Section 5.4.) creating the independent variables for regression modelling.\n# Data for Bradford OAs only, from 2021 Census\n\n# Household deprivation in the education dimension (ONS, 2023d)\nHHD_dep_education_raw = pd.read_csv('Data/2021 Census Data/Household deprived in the education dimension.csv')\n\n# Household deprivation in the employment dimension (ONS, 2023e)\nHHD_dep_employment_raw = pd.read_csv('Data/2021 Census Data/Household deprived in the employment dimension.csv')\n\n# Household deprivation in the health and disability dimension (ONS, 2023f)\nHHD_dep_health_raw = pd.read_csv('Data/2021 Census Data/Household deprived in the health and disability dimension.csv')\n\n# Household deprivation in the housing dimension (ONS, 2023g)\nHHD_dep_housing_raw = pd.read_csv('Data/2021 Census Data/Household deprived in the housing dimension.csv')"
  },
  {
    "objectID": "projects/access-to-greenspace/access-to-greenspace-bradford.html#data-wrangling",
    "href": "projects/access-to-greenspace/access-to-greenspace-bradford.html#data-wrangling",
    "title": "A spatial analysis of access to greenspace and deprivation in Bradford",
    "section": "5. Data wrangling",
    "text": "5. Data wrangling\n\n5.1. OAs\nOnly Bradford LAD OAs are needed. The OA lookup and geometry dataframes can be merged, retaining only Bradford as identified by the LAD22NM column. There are 1,575 OAs within Bradford LAD.\n# Remove unnecessary columns from OA lookup, keeping only required columns in new dataframe\nOA_lookup_trim = OA_lookup[['OA21CD','LAD22CD','LAD22NM']]\n# Filter OA lookup to just Bradford\nBradford_OA_lookup = OA_lookup_trim[OA_lookup_trim['LAD22NM'] == 'Bradford']\n# Check how many OAs are left - this number is an important reference point for further data wrangling tasks\nlen(Bradford_OA_lookup)\n1575\n# Merge Bradford_OA_lookup with OA polygons to reduce the geodataframe containing the polygons to only Bradford OAs\nBradford_OA_polygons = OA_polygons.merge(Bradford_OA_lookup, how='right', on='OA21CD')\n# Check number of OAs is same as in the reduced Bradford OA lookup dataframe\nprint(len(Bradford_OA_polygons))\nprint(len(Bradford_OA_polygons) == len(Bradford_OA_lookup))\n1575\nTrue\n# Merge Bradford_OA_lookup with OA PWCs to reduce the geodataframe containing the PWCs to only Bradford OAs\nBradford_OA_PWC = OA_PWC.merge(Bradford_OA_lookup, how='right', on='OA21CD')\n# Check number of OAs is same as in the reduced Bradford OA lookup dataframe\nprint(len(Bradford_OA_PWC))\nprint(len(Bradford_OA_PWC) == len(Bradford_OA_lookup))\n1575\nTrue\n# Drop unrequired columns\nBradford_OA_polygons_tidy = Bradford_OA_polygons.drop([\n    'LSOA21CD','LSOA21NM','LSOA21NMW','BNG_E','BNG_N','LAT','LONG','GlobalID','LAD22CD','LAD22NM'], axis=1)\nBradford_OA_PWC_tidy = Bradford_OA_PWC.drop(['GlobalID','LAD22CD','LAD22NM'], axis=1)\n\n# Rename geometry columns and set geometry\nBradford_OA_polygons = Bradford_OA_polygons_tidy.set_geometry('geometry').rename_geometry('Polygon')\nBradford_OA_PWC = Bradford_OA_PWC_tidy.set_geometry('geometry').rename_geometry('PWC')\n\n\n5.2. Greenspace\nOnly greenspace within Bradford LAD is required. A Bradford LAD polygon is created to spatially match the greenspace, retaining only sites within Bradford LAD.\n\n5.2.1. Clean greenspace dataframe\nSome types of greenspaces are arguably not for recreation (e.g. cemetery) and some are not always free to the public (e.g. golf course). To ensure this analysis includes only freely accessible greenspace for recreational use, types are reduced to ‘play space’, ‘playing field’ and ‘public park or garden’.\n# Remove unnecessary columns, keeping only required columns in new dataframe\nGB_greenspace_site_trim = GB_greenspace_site[['id','function','geometry']]\n\n# Rename remaining columns for clarity\nGB_greenspace_site_trim = GB_greenspace_site_trim.rename(columns={\"id\": \"Greenspace_ID\", \"function\": \"Type\"})\nGB_greenspace_site_trim = GB_greenspace_site_trim.set_geometry('geometry').rename_geometry('Polygon')\n# Check what types are included in the data\nGB_greenspace_site_trim.Type.value_counts()\n\n\n\nType\n\n\n\n\n\nPlay Space\n42972\n\n\nReligious Grounds\n22229\n\n\nPlaying Field\n21377\n\n\nOther Sports Facility\n15073\n\n\nAllotments Or Community Growing Spaces\n13002\n\n\nPublic Park Or Garden\n11982\n\n\nCemetery\n7559\n\n\nTennis Court\n6632\n\n\nBowling Green\n6589\n\n\nGolf Course\n3000\n\n\n\nName: count, dtype: int64\n# Keep only 'Play Space', 'Playing Field', and 'Public Park Or Garden'\nGB_greenspace_site_filtered = GB_greenspace_site_trim.loc[GB_greenspace_site_trim['Type'].\n                                                          isin(['Play Space','Playing Field','Public Park Or Garden'])]\n\n\n5.2.2. Greenspace within Bradford LAD\nAggregating OA polygons to a single polygon and spatially matching to the greenspace dataframe isolates greenspace within Bradford LAD. This will be used to calculate the dependent variable for the regression analysis.\n# Create a copy of the Bradford_OA_polygons dataframe which will become the Bradford polygon dataframe\nBradford = Bradford_OA_polygons\n# Add a 'Bradford' dummy column to aggregate on\nBradford['City'] = 'Bradford'\n# Create a single polygon aggregated to the city level using 'dissolve'\nBradford_polygon = Bradford.dissolve(by='City')\n# Drop OA21CD column as this is no longer relevant\nBradford_polygon = Bradford_polygon.drop(['OA21CD'], axis=1)\n# Spatial join for any greenspace that intersects the Bradford polygon\nBradford_greenspace_site = gpd.sjoin(GB_greenspace_site_filtered, Bradford_polygon, how='inner', predicate='intersects')\n# Drop index_right column as this is no longer relevant\nBradford_greenspace_site = Bradford_greenspace_site.drop(['index_right'], axis=1)\n# Check length of dataframe to determine how many greenspaces are included\nlen(Bradford_greenspace_site)\n436\n# Check that the unique number of Greenspace_IDs is in fact the length of the dataframe\nprint(Bradford_greenspace_site['Greenspace_ID'].nunique())\nprint(len(Bradford_greenspace_site) == Bradford_greenspace_site['Greenspace_ID'].nunique())\n436\nTrue\n\n\n\n5.3. Dependent variable\nThe dependent variable is the distance between the OA PWC and nearest greenspace polygon calculated using the Euclidian distance and given in metres.\n# Find the nearest greenspace to each OA population-weighted centroid\nnearest_greenspace = gpd.sjoin_nearest(\n    Bradford_OA_PWC, Bradford_greenspace_site, how='left', distance_col='Distance')\n\n# Drop the unrequired columns\nnearest_greenspace = nearest_greenspace.drop(['PWC','index_right'], axis=1)\n# Check length of dataframe is equal to the length of the OA dataframe - i.e. one greenspace per OA has been identified\nprint(len(nearest_greenspace))\nprint(len(nearest_greenspace) == len(Bradford_OA_lookup))\n1599\nFalse\nThe information above shows there are some duplicated rows. This is confirmed by checking the number of unique OA21CDs which should be 1,575.\n# Check number of OA21CDs\nprint(nearest_greenspace['OA21CD'].nunique())\nprint(nearest_greenspace['OA21CD'].nunique() == len(Bradford_OA_lookup))\n1575\nTrue\nDuplicates are caused by the same greenspace being allocated different ‘Types’ with unique IDs. These duplicates are removed, keeping the first record irrespective of ‘Type’ as no analysis is to be conducted on this.\n# Isolate duplicated rows and check cause\n# Confirmed as duplicated sites with different Greenspace_IDs due to different greenspace Type being recorded\nduplicated_rows = nearest_greenspace[nearest_greenspace.duplicated(subset='OA21CD', keep=False)]\nduplicated_rows.head(6)\n\n\n\n\n\n\nOA21CD\n\n\nGreenspace_ID\n\n\nType\n\n\nDistance\n\n\n\n\n\n\n11\n\n\nE00053364\n\n\n0295ED18-F337-5C37-E063-AAEFA00A445E\n\n\nPlay Space\n\n\n514.057891\n\n\n\n\n11\n\n\nE00053364\n\n\n0295ED18-F2F3-5C37-E063-AAEFA00A445E\n\n\nPlaying Field\n\n\n514.057891\n\n\n\n\n42\n\n\nE00053392\n\n\n0295ED18-F337-5C37-E063-AAEFA00A445E\n\n\nPlay Space\n\n\n364.166159\n\n\n\n\n42\n\n\nE00053392\n\n\n0295ED18-F2F3-5C37-E063-AAEFA00A445E\n\n\nPlaying Field\n\n\n364.166159\n\n\n\n\n43\n\n\nE00053393\n\n\n0295ED18-F337-5C37-E063-AAEFA00A445E\n\n\nPlay Space\n\n\n571.723915\n\n\n\n\n43\n\n\nE00053393\n\n\n0295ED18-F2F3-5C37-E063-AAEFA00A445E\n\n\nPlaying Field\n\n\n571.723915\n\n\n\n\n# Drop duplicated rows and keep first row for duplicated instances\nunique_nearest_greenspace = nearest_greenspace.drop_duplicates(subset='OA21CD', keep='first')\n# Check length of dataframe is now as expected\nprint(len(unique_nearest_greenspace))\nprint(len(unique_nearest_greenspace) == len(Bradford_OA_lookup))\n1575\nTrue\nThe ‘Bradford_greenspace_site’ dataframe created in 5.2. must also be updated to ensure a consistent view of greenspace.\n# Get unique list of Greenspace_IDs\nunique_Greenspace_IDs = unique_nearest_greenspace[['Greenspace_ID']].drop_duplicates()\n# Join onto Bradford_greenspace_site to reduce this dataframe to just those kept in the unique_nearest_greenspace dataframe\nBradford_greenspace_site_unique = Bradford_greenspace_site.merge(unique_Greenspace_IDs, how='right', on='Greenspace_ID')\n\n\n5.4. Independent variables\nDeprivation data needs to be pivoted to create columns of the datapoints by OA and converting these into percentages. This is achieved using a function to repeat the process for each dimension. Resulting dataframes are merged into a final dataframe containing all four independent variables.\ndef Independent_Variables_Setup(dataframe, dataframe_name):     # dataframe as object, dataframe_name as string\n    \n    # Create all required strings for naming dataframes throughout wrangling process\n    core_name = dataframe_name.rstrip('_raw') # Remove \"_raw\" from end of dataframe to get core name\n    dropped = core_name + '_dropped'          # Add \"_dropped\" to end of core name\n    pivot = core_name + '_pivot'              # Add \"_pivot\" to end of core name\n    percentage = core_name + '_PC'            # Add \"_PC\" to end of core name\n    \n    # Drop columns that are not required\n    cols = [1,3]     # List of column indexes of the columns to drop\n    dropped = dataframe.drop(dataframe.columns[cols], axis=1)     # Create new dataframe with columns dropped\n    \n    # Pivot the table on column[0], making the options from column[1] the new column headers and the data that from column[2]\n    pivot = dropped.pivot_table(index=dropped.columns[0],\n                                  columns=dropped.columns[1],\n                                  values=dropped.columns[2]).reset_index()\n    pivot.columns.name = None\n    \n    # Create a Total column from 3 new data columns[1,2,3]\n    pivot['Total'] = pivot.iloc[:, 1:4].sum(axis=1)\n    \n    # Calculate % HHDs Deprived by dividing column[3] \"Deprived\" by the Total calculated above\n    pivot[percentage] = (pivot.iloc[:,3]/pivot['Total'])\n    \n    # Create final dataframe keeping only the OA21CD and % HHDs deprived in the given dimension\n    final_cols = [0,5]\n    core_name = pivot.iloc[:, final_cols]\n    \n    return core_name\n# Household deprivation in the education dimension\nHHD_dep_education = Independent_Variables_Setup(HHD_dep_education_raw, 'HHD_dep_education_raw')\n# Household deprivation in the employment dimension\nHHD_dep_employment = Independent_Variables_Setup(HHD_dep_employment_raw, 'HHD_dep_employment_raw')\n# Household deprivation in the health and disability dimension\nHHD_dep_health = Independent_Variables_Setup(HHD_dep_health_raw, 'HHD_dep_health_raw')\n# Household deprivation in the housing dimension\nHHD_dep_housing = Independent_Variables_Setup(HHD_dep_housing_raw, 'HHD_dep_housing_raw')\n# Check all 4 dataframes are the same length and that length is as expected\nlen(HHD_dep_education) == len(HHD_dep_employment) == len(HHD_dep_health) == len(HHD_dep_housing) == len(Bradford_OA_lookup)\nTrue\n# Merge education and employment\nInd_vars = HHD_dep_education.merge(HHD_dep_employment, how='inner', on='Output Areas Code')\n# Add health\nInd_vars = Ind_vars.merge(HHD_dep_health, how='inner', on='Output Areas Code')\n# Add housing\nInd_vars = Ind_vars.merge(HHD_dep_housing, how='inner', on='Output Areas Code')\n# Final length check\nprint(len(Ind_vars))\nprint(len(Ind_vars) == len(Bradford_OA_lookup))\n1575\nTrue\n\n\n5.5. Final dataframe for regression analysis\nA dataframe containing the dependent and independent variables, OA code and polygons is required for the regression models and spatial analysis.\n# Merge the unique_nearest_greenspace and Ind_vars dataframes to get a final dataframe for subsequent analysis\nfinal_df = unique_nearest_greenspace.merge(Ind_vars, how='inner', left_on='OA21CD', right_on='Output Areas Code')\nfinal_df = final_df.merge(Bradford_OA_polygons, how='inner', on='OA21CD')\n\n# Drop unrequired columns\nfinal_df = final_df.drop(['Output Areas Code', 'Greenspace_ID', 'Type', 'City'], axis=1)\n\n# Set geometry\nfinal_df = final_df.set_geometry('Polygon')\nVisual checks of each dataframe can be found in Appendix 1."
  },
  {
    "objectID": "projects/access-to-greenspace/access-to-greenspace-bradford.html#visualisation-and-statistics",
    "href": "projects/access-to-greenspace/access-to-greenspace-bradford.html#visualisation-and-statistics",
    "title": "A spatial analysis of access to greenspace and deprivation in Bradford",
    "section": "6. Visualisation and Statistics",
    "text": "6. Visualisation and Statistics\n\n6.1. OAs\nFigure 6.1.1. shows OA boundaries and PWCs within Bradford LAD. PWCs locations can vary from near the edge to the centre of OAs.\n\nFigure 6.1.1. Map of OA polygons and population-weighted centroids within the Bradford LAD\n\nData Source: ONS, 2023a; ONS, 2023b\n\n# Plot polygons\nfig, ax = plt.subplots(figsize=(10, 10))\nBradford_OA_polygons.plot(ax=ax, color='Grey', edgecolor='black', linewidth=0.25, alpha=0.1)\n\n# Plot points\nBradford_OA_PWC.plot(ax=ax, markersize=1, color='#134f5c')\n\n# Customize plot\nax.set_axis_off()\n\n# Add basemap\nctx.add_basemap(ax, crs=Bradford_OA_polygons.crs.to_string(), source=ctx.providers.CartoDB.Positron)\n\n# Show plot\nplt.show()\n\n\n\npng\n\n\n\n\n\n6.2. Greenspace\nFigure 6.2.1. shows greenspace by type. However, as detailed in Section 5.3., duplicates were removed by keeping first instances of duplicated cases hence counts in figure 6.2.2. are skewed. In most instances, ‘playing field’ was retained, and so counts are higher for this type. This does not impact subsequent analysis as type does not feature in the regression modelling.\n\nFigure 6.2.1. Map of greenspace within the Bradford LAD\n\nData Source: OS, 2023\n\n# Plot polygons\nfig, ax = plt.subplots(figsize=(15, 15))\n\n# Plot Bradford_polygon\nBradford_polygon.plot(ax=ax, color='Grey', edgecolor='black', linewidth=0.25, alpha=0.1)\n\n# Plot Bradford_greenspace_site_unique\nBradford_greenspace_site_unique.plot(ax=ax, column='Type', cmap='viridis',\n                                     legend=True, markersize=50, alpha=0.75, edgecolor='black', linewidth=0.25)\n\n# Add basemap\nctx.add_basemap(ax, crs=Bradford_polygon.crs.to_string(), source=ctx.providers.CartoDB.Positron)\n\n# Customize plot\nax.set_axis_off()\n\n# Show plot\nplt.show()\n\n\n\npng\n\n\n\n\nFigure 6.2.2. Count of greenspace by type within the Bradford LAD\n\nData Source: OS, 2023; Author's calculations\n\ngreenspace_types = Bradford_greenspace_site_unique['Type'].unique().tolist()     # Get list of greenspace types\ngreenspace_counts = list(Bradford_greenspace_site_unique['Type'].value_counts()) # Get list of greenspace type counts\n\ncolours = ['#21918c', '#440154', '#fde725']     # Set colours from the map for visual consistency\n\n# Create a pie chart\nfig, ax = plt.subplots()\nwedges, labels = ax.pie(greenspace_counts, labels=greenspace_types,     # counts as data, types as labels\n                        autopct=None, startangle=90,                    # turn off data labels, set start angle to 90degrees\n                        wedgeprops=dict(width=0.5), colors=colours)     # set size of donut hole, set colour scheme\n\n# Add labels within each pie piece\nfor label, value in zip(labels, greenspace_counts):\n    label.set(size=10, text=f'{label.get_text()}: {value}')     # Set the labels to be \"Type: Count\"\n\n\n# Equal aspect ratio to make sure the chart is drawn as a circle\nax.axis('equal')  \n\n# Add a title\nplt.title('Count of greenspace by type within the Bradford LAD',\n          fontweight='bold', fontsize=12, fontfamily='sans-serif') # Change title to specified font settings\n\n# Add a footnote with the data source and specify location on visual\nplt.text(0.5, -0.15, \"Data Source: OS, 2023; Author's calculations\",\n         ha=\"center\", va=\"center\", transform=plt.gca().transAxes,\n         fontsize=8, color=\"gray\")\n\n# Display the pie chart\nplt.show()\n\n\n\npng\n\n\n\n\n\n6.3. Deprivation\ndef create_deprivation_choropleth(variable, final_df):\n    # Create a copy of final_df so data and column names can be made visual-appropriate\n    OA_choropleth = final_df.copy()\n    \n    # Multiply data by 100 to create percentage in readable format for the choropleth\n    OA_choropleth[['HHD_dep_education_PC', 'HHD_dep_employment_PC', \n                   'HHD_dep_health_PC', 'HHD_dep_housing_PC']] = (\n        OA_choropleth[['HHD_dep_education_PC', 'HHD_dep_employment_PC', \n                       'HHD_dep_health_PC', 'HHD_dep_housing_PC']] * 100).round(1)\n    \n    # Create a dictionary of current independent variable column names with long-form for use on map\n    columns_dict = {\"HHD_dep_education_PC\": \"Households deprived in the education dimension (%)\",\n                    \"HHD_dep_employment_PC\": \"Households deprived in the employment dimension (%)\",\n                    \"HHD_dep_health_PC\": \"Households deprived in the health dimension (%)\",\n                    \"HHD_dep_housing_PC\": \"Households deprived in the housing dimension (%)\"}\n    \n    # Rename the columns to the long-form version\n    OA_choropleth.rename(columns=columns_dict, inplace=True)\n    \n    # Write long-form column name to a new variable to use in the choropleth code\n    variable_longform = columns_dict[variable]\n\n    # Plot choropleth\n    fig, ax = plt.subplots(figsize=(15, 15))\n    OA_choropleth.plot(column=variable_longform, \n                       ax=ax, \n                       scheme='EqualInterval', \n                       k=5, \n                       cmap='YlGnBu', \n                       legend=True)\n    \n    # Add basemap\n    ctx.add_basemap(ax, crs=OA_choropleth.crs.to_string(), source=ctx.providers.CartoDB.Positron)\n\n    # Customize plot\n    ax.set_axis_off()\n    \n    # Add title to legend\n    legend_title = variable_longform\n    ax.get_legend().set_title(legend_title)\n\n    # Show plot\n    plt.show()\nFigure 6.3.1. shows areas of high education deprivation in the southeast and west. Lowest levels are in rural, affluent areas in the southwest and north.\n\nFigure 6.3.1. Percentage of households deprived in the education dimension by OA in Bradford LAD\n\nData Source: ONS, 2023d; Author's calculations\n\n# Call the function with the desired variable and the dataframe\ncreate_deprivation_choropleth('HHD_dep_education_PC', final_df)\n\n\n\npng\n\n\nFigure 6.3.2. shows similar spatial employment deprivation patterns to education. The range of employment deprivation is smaller though, from 0% to 54.7%.\n\n\nFigure 6.3.2. Percentage of households deprived in the employment dimension by OA in Bradford LAD\n\nData Source: ONS, 2023e; Author's calculations\n\n# Call the function with the desired variable and the dataframe\ncreate_deprivation_choropleth('HHD_dep_employment_PC', final_df)\n\n\n\npng\n\n\nFigure 6.3.3. shows health deprivation levels are higher and widely spread. As health deprivation includes poor health and disability, levels may be higher due to factors including elderly populations, as well as affluence-associated poor health.\n\n\nFigure 6.3.3. Percentage of households deprived in the health dimension by OA in Bradford LAD\n\nData Source: ONS, 2023f; Author's calculations\n\n# Call the function with the desired variable and the dataframe\ncreate_deprivation_choropleth('HHD_dep_health_PC', final_df)\n\n\n\npng\n\n\nContrastingly, figure 6.3.4. shows housing deprivation is largely isolated to Bradford city and around Keighley. Most OAs have less than 10% housing deprivation.\n\n\nFigure 6.3.4. Percentage of households deprived in the housing dimension by OA in Bradford LAD\n\nData Source: ONS, 2023g; Author's calculations\n\n# Call the function with the desired variable and the dataframe\ncreate_deprivation_choropleth('HHD_dep_housing_PC', final_df)\n\n\n\npng\n\n\nFigure 6.3.5. and 6.3.6. provide key summary statistics. Education deprivation has the largest range, from 2.2% in the least deprived OA to 67.8% in the most deprived. Only education and health dimensions exceed 0% in all OAs. For health, the lowest level of deprivation is 8.8% highlighting how even the most affluent areas are impacted by this.\nEducation, health, and housing have outliers at the higher end, recording maximum values around 65-68%. However, average deprivation levels vary across each dimension, with housing having the lowest average (11.0%) and health the highest (35.4%). There is no consistent distribution or spatial pattern to deprivation across all four dimensions.\n\n\nFigure 6.3.5. Boxplots of the percentage of households deprived in the given dimension by OA in Bradford LAD\n\nData Source: ONS, 2023d; ONS, 2023e; ONS, 2023f; ONS, 2023g; Author's calculations\n\n# Create a copy of the Ind_vars dataframe for the purposes of this boxplot\nInd_vars_boxplot = Ind_vars\n\n# Multiply data by 100 to create percentage in readable format\nInd_vars_boxplot[['HHD_dep_education_PC', 'HHD_dep_employment_PC', \n                  'HHD_dep_health_PC', 'HHD_dep_housing_PC']] = (\n    Ind_vars_boxplot[['HHD_dep_education_PC', 'HHD_dep_employment_PC', \n                      'HHD_dep_health_PC', 'HHD_dep_housing_PC']] * 100).round(1)\n\n# Rename the columns for the chart to be clearer\nInd_vars_boxplot = Ind_vars_boxplot.rename(columns={\n    \"HHD_dep_education_PC\": \"Education\",\n    \"HHD_dep_employment_PC\": \"Employment\",\n    \"HHD_dep_health_PC\": \"Health\",\n    \"HHD_dep_housing_PC\": \"Housing\"})\n\n# Setup figure and boxplot\nfig, ax = plt.subplots(figsize=(10, 6))\nwandering_forest_palette = ['#09435a', '#4e7d8e', '#9cc2b8', '#90958f']   # Create colour palette\nsns.set_palette(wandering_forest_palette)     # Set colour palette\nsns.boxplot(data=Ind_vars_boxplot)     # Add data\n\n# Add title and amend axes\nplt.title('Percentage of households deprived in the given dimension',\n          fontweight='bold', fontsize=12) # Change title to specified font settings\nplt.xlabel(\"Household deprivation dimension\", fontweight='bold') # set x-axis label and font\nplt.ylabel(\"Percentage of households deprived (%)\", fontweight='bold') # set y-axis label and font\n\n# Add a footnote with the data source and specify location on visual\nplt.text(0.5, -0.15, \"Data Source: ONS, 2023d; ONS, 2023e; ONS, 2023f; ONS, 2023g\",\n         ha=\"center\", va=\"center\", transform=plt.gca().transAxes,\n         fontsize=8, color=\"gray\")\n\nplt.show()\n\n\n\npng\n\n\n\n\nFigure 6.3.6. Deprivation statistics in the given dimension by OA in Bradford LAD\n\nData Source: ONS, 2023d; ONS, 2023e; ONS, 2023f; ONS, 2023g; Author's calculations\n\n# Get precise figures as shown in the boxplot\nInd_vars_boxplot.describe()\n\n\n\n\n\n\nEducation\n\n\nEmployment\n\n\nHealth\n\n\nHousing\n\n\n\n\n\n\ncount\n\n\n1575.000000\n\n\n1575.000000\n\n\n1575.000000\n\n\n1575.000000\n\n\n\n\nmean\n\n\n24.954095\n\n\n15.573143\n\n\n35.444508\n\n\n10.959619\n\n\n\n\nstd\n\n\n10.980256\n\n\n9.313979\n\n\n8.980795\n\n\n9.729166\n\n\n\n\nmin\n\n\n2.200000\n\n\n0.000000\n\n\n8.800000\n\n\n0.000000\n\n\n\n\n25%\n\n\n16.900000\n\n\n8.000000\n\n\n29.100000\n\n\n3.900000\n\n\n\n\n50%\n\n\n23.900000\n\n\n13.800000\n\n\n34.800000\n\n\n7.600000\n\n\n\n\n75%\n\n\n31.600000\n\n\n22.000000\n\n\n41.000000\n\n\n15.150000\n\n\n\n\nmax\n\n\n67.800000\n\n\n54.700000\n\n\n66.100000\n\n\n64.800000\n\n\n\n\n\n\n\n6.4. Distance to nearest greenspace\nFigure 6.4.1. shows distances to greenspace are greater in rural areas to the north and west. These areas are more affluent, with lower levels of deprivation (figures 6.3.1.-6.3.4). More deprived OAs have lower distances.\n\nFigure 6.4.1. Distance to nearest greenspace (metres) by OA in Bradford LAD\n\nData Source: ONS, 2023b; OS, 2023; Author's calculations\n\ndef create_distance_choropleth(final_df):\n    # Create a copy of final_df so data and column names can be made visual-appropriate\n    OA_dist_choropleth = final_df.copy()\n    \n    # Round distance to the nearest metre\n    OA_dist_choropleth['Distance'] = OA_dist_choropleth['Distance'].round().astype(int)\n    \n    # Create a dictionary of current distance column name with long-form for use on map\n    distance_dict = {\"Distance\": \"Distance to nearest greenspace (metres)\"}\n    \n    # Rename the column to the long-form version\n    rename_column = OA_dist_choropleth.rename(columns=distance_dict, inplace=True)\n    \n    # Write long-form column name to a new variable to use in the choropleth code\n    distance_longform = 'Distance to nearest greenspace (metres)'\n\n    # Plot choropleth\n    fig, ax = plt.subplots(figsize=(15, 15))\n    OA_dist_choropleth.plot(column=distance_longform, \n                            ax=ax, \n                            scheme='NaturalBreaks', \n                            cmap='YlGnBu', \n                            legend=True)\n    \n    # Add basemap\n    ctx.add_basemap(ax, crs=OA_dist_choropleth.crs.to_string(), source=ctx.providers.CartoDB.Positron)\n\n    # Customize plot\n    ax.set_axis_off()\n    \n    # Add title to legend\n    ax.get_legend().set_title(distance_longform)\n\n    # Show plot\n    plt.show()\n# Call the function with the DataFrame final_df\ncreate_distance_choropleth(final_df)\n\n\n\npng\n\n\nFigure 6.4.2. shows outliers at larger distances, confirming the distance variable is right skewed. Meanwhile, figure 6.4.3. shows the range, from 0m to over 2,000m. OAs with 0m distance are a result of PWCs falling within greenspace boundaries. On average, greenspace is around 275m from PWCs, which is below the previous UK government target of 300m (Houlden et al, 2019).\n\n\nFigure 6.4.2. Boxplot of distance to nearest greenspace (metres) by OA in Bradford LAD\n\nData Source: ONS, 2023b; OS, 2023; Author's calculations\n\n# Setup figure and boxplot\nfig, ax = plt.subplots(figsize=(10, 6))\nsns.boxplot(unique_nearest_greenspace, x='Distance', color='#e1fdd5')     # Add data and change colour\n\n# Add title and amend axes\nplt.title('Distance to nearest greenspace by OA in Bradford',\n          fontweight='bold', fontsize=12) # Change title to specified font settings\nax.set(xlabel=\"\") # Set ax xlabel to nothing so the variable name does not appear\nplt.xlabel(\"Distance to nearest greenspace (metres)\", fontweight='bold') # set x-axis label and font\nplt.ylabel(\"OAs\", fontweight='bold') # set y-axis label and font\n\n# Set the x-axis ticks at 250 increments\ntick_locations = np.arange(0, 2251, 250)\nax.set_xticks(tick_locations)\n\n# Add a footnote with the data source and specify location on visual\nplt.text(0.5, -0.15, \"Data Source: ONS, 2023b; OS, 2023; Author's calculations\",\n         ha=\"center\", va=\"center\", transform=plt.gca().transAxes,\n         fontsize=8, color=\"gray\")\n\nplt.show()\n\n\n\npng\n\n\n\n\nFigure 6.4.3. Distance to nearest greenspace (metres) statistics by OA in Bradford LAD\n\nData Source: ONS, 2023b; OS, 2023; Author's calculations\n\n# Get precise figures as shown in the boxplot\nunique_nearest_greenspace.describe()\n\n\n\n\n\n\nDistance\n\n\n\n\n\n\ncount\n\n\n1575.000000\n\n\n\n\nmean\n\n\n247.661399\n\n\n\n\nstd\n\n\n183.485184\n\n\n\n\nmin\n\n\n0.000000\n\n\n\n\n25%\n\n\n119.213902\n\n\n\n\n50%\n\n\n201.468622\n\n\n\n\n75%\n\n\n332.656088\n\n\n\n\nmax\n\n\n2090.986345\n\n\n\n\n\n\n\n6.5. Dependent and independent variables\nFigure 6.5.1. shows scatter plots and regression lines between the dependent and independent variables. There is one high distance outlier visible across all four dimensions, and all present a negative correlation with distance to nearest greenspace.\n\nFigure 6.5.1. Scatterplots of distance to nearest greenspace (metres) and household deprivation dimensions, by OA in Bradford LAD\n\nData Source: ONS, 2023b; ONS, 2023d; ONS, 2023e; ONS, 2023f; ONS, 2023g; OS, 2023; Author's calculations\n\n# Set up a figure with 2 rows and 2 columns for subplots\nfig, axs = plt.subplots(2, 2, figsize=(15, 10), sharey=True, sharex=True) # Share axes across 4 plots for visual consistency\n\n# Add title, x-axis subtitle, and y-axis labels\nplt.title('Distance to nearest greenspace compared to Household Deprivation Dimensions', \n          x=-0.15, y=2.25, fontweight='bold', fontsize=12)\nfig.suptitle('Household Deprivation Dimension', y=0.05, fontweight='bold', fontsize=12)\naxs[0,0].set_ylabel('Distance to nearest greenspace (metres)', fontweight='bold', fontsize=12)\naxs[1,0].set_ylabel('Distance to nearest greenspace (metres)', fontweight='bold', fontsize=12)\n\n\n### Education scatter\naxs[0,0].scatter(x=final_df['HHD_dep_education_PC'], y=final_df['Distance'], s=3, c='#09435a') # add data and set colour\naxs[0,0].set_xlabel('Education (%)', fontsize=12) # add axis label and set font\n\n# Calculate the regression line\nslope0, intercept0 = np.polyfit(final_df['HHD_dep_education_PC'], final_df['Distance'], 1)\nregression_line0 = np.polyval([slope0, intercept0], final_df['HHD_dep_education_PC'])\n\n# Add this to the chart\naxs[0,0].plot(final_df['HHD_dep_education_PC'], regression_line0, color='red')\n\n\n### Employment scatter\naxs[0,1].scatter(x=final_df['HHD_dep_employment_PC'], y=final_df['Distance'], s=3, c='#4e7d8e') # add data and set colour\naxs[0,1].set_xlabel('Employment (%)', fontsize=12) # add axis label and set font\n\n# Calculate the regression line\nslope1, intercept1 = np.polyfit(final_df['HHD_dep_employment_PC'], final_df['Distance'], 1)\nregression_line1 = np.polyval([slope1, intercept1], final_df['HHD_dep_employment_PC'])\n\n# Add this to the chart\naxs[0,1].plot(final_df['HHD_dep_employment_PC'], regression_line1, color='red')\n\n\n### Health scatter\naxs[1,0].scatter(x=final_df['HHD_dep_health_PC'], y=final_df['Distance'], s=3, c='#9cc2b8') # add data and set colour\naxs[1,0].set_xlabel('Health (%)', fontsize=12) # add axis label and set font\n\n# Calculate the regression line\nslope2, intercept2 = np.polyfit(final_df['HHD_dep_health_PC'], final_df['Distance'], 1)\nregression_line2 = np.polyval([slope2, intercept2], final_df['HHD_dep_health_PC'])\n\n# Add this to the chart\naxs[1,0].plot(final_df['HHD_dep_health_PC'], regression_line2, color='red')\n\n\n### Housing scatter\naxs[1,1].scatter(x=final_df['HHD_dep_housing_PC'], y=final_df['Distance'], s=3, c='#90958f') # add data and set colour\naxs[1,1].set_xlabel('Housing (%)', fontsize=12) # add axis label and set font\n\n# Calculate the regression line\nslope3, intercept3 = np.polyfit(final_df['HHD_dep_housing_PC'], final_df['Distance'], 1)\nregression_line3 = np.polyval([slope3, intercept3], final_df['HHD_dep_housing_PC'])\n\n# Add this to the chart\naxs[1,1].plot(final_df['HHD_dep_housing_PC'], regression_line3, color='red')\n\n\n# Add a footnote with the data source and specify location on visual\nplt.text(-0.15, -0.3,\n         \"Data Source: ONS, 2023b; ONS, 2023d; ONS, 2023e; ONS, 2023f; ONS, 2023g; OS, 2023; Author's calculations\",\n         ha=\"center\", va=\"center\", transform=plt.gca().transAxes,\n         fontsize=10, color=\"gray\")\n\n\nplt.show()\n\n\n\npng"
  },
  {
    "objectID": "projects/access-to-greenspace/access-to-greenspace-bradford.html#analysis-and-results",
    "href": "projects/access-to-greenspace/access-to-greenspace-bradford.html#analysis-and-results",
    "title": "A spatial analysis of access to greenspace and deprivation in Bradford",
    "section": "7. Analysis and results",
    "text": "7. Analysis and results\n\n7.1. OLS Regression\nChecks for collinearity are conducted using a correlation matrix (figure 7.1.1.). Deprivation variables have a positive correlation with one another, particularly employment and health. Figure 7.1.2. is a coefficient matrix which confirms that the employment and health dimensions have a coefficient of 0.68 which is high and could indicate collinearity. People with poorer health or disabilities are more likely to be out of work (ONS, 2023i) and so, coupled with the high coefficient, it has been determined that collinearity is present between these variables.\n\nFigure 7.1.1. Correlation matrix of independent and dependent variables by OA in Bradford LAD\n\nData Source: ONS, 2023b; ONS, 2023d; ONS, 2023e; ONS, 2023f; ONS, 2023g; OS, 2023; Author's calculations\n\n# Initial correlations to check for collinearity\noutput = pd.plotting.scatter_matrix(final_df, alpha=0.2, figsize=(12, 12), diagonal='kde')\n\n\n\npng\n\n\n\n\nFigure 7.1.2. Coefficient matrix of independent and dependent variables by OA in Bradford LAD\n\nData Source: ONS, 2023b; ONS, 2023d; ONS, 2023e; ONS, 2023f; ONS, 2023g; OS, 2023; Author's calculations\n\n# Correlation coefficient matrix\nfinal_df.corr(numeric_only=True)\n\n\n\n\n\n\nDistance\n\n\nHHD_dep_education_PC\n\n\nHHD_dep_employment_PC\n\n\nHHD_dep_health_PC\n\n\nHHD_dep_housing_PC\n\n\n\n\n\n\nDistance\n\n\n1.000000\n\n\n-0.182102\n\n\n-0.181204\n\n\n-0.160042\n\n\n-0.148355\n\n\n\n\nHHD_dep_education_PC\n\n\n-0.182102\n\n\n1.000000\n\n\n0.620664\n\n\n0.643816\n\n\n0.358545\n\n\n\n\nHHD_dep_employment_PC\n\n\n-0.181204\n\n\n0.620664\n\n\n1.000000\n\n\n0.680197\n\n\n0.624160\n\n\n\n\nHHD_dep_health_PC\n\n\n-0.160042\n\n\n0.643816\n\n\n0.680197\n\n\n1.000000\n\n\n0.272581\n\n\n\n\nHHD_dep_housing_PC\n\n\n-0.148355\n\n\n0.358545\n\n\n0.624160\n\n\n0.272581\n\n\n1.000000\n\n\n\n\n\n\nAddressing collinearity\nFigure 7.1.3. shows the p-value for employment is highest, meaning it is the least statistically significant variable, so employment is removed to eliminate the identified collinearity.\n# Create the independent Y variable\ny = final_df['Distance']\n\n# Create the dependent X variables\nX = final_df[['HHD_dep_education_PC','HHD_dep_employment_PC','HHD_dep_health_PC','HHD_dep_housing_PC']]\n\n# Add the constant\nx1 = sm.add_constant(X)\n# Create OLS regression model\nmod = sm.OLS(y, x1)\n# Fit the model and save results\nresults = mod.fit()\n\n\nFigure 7.1.3. Results of OLS model including all four deprivation dimensions\n\nData Source: ONS, 2023b; ONS, 2023d; ONS, 2023e; ONS, 2023f; ONS, 2023g; OS, 2023; Author's calculations\n\n# Print results summary\nprint(results.summary())\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:               Distance   R-squared:                       0.044\nModel:                            OLS   Adj. R-squared:                  0.042\nMethod:                 Least Squares   F-statistic:                     18.13\nDate:                Mon, 13 May 2024   Prob (F-statistic):           1.43e-14\nTime:                        19:16:44   Log-Likelihood:                -10408.\nNo. Observations:                1575   AIC:                         2.083e+04\nDf Residuals:                    1570   BIC:                         2.085e+04\nDf Model:                           4                                         \nCovariance Type:            nonrobust                                         \n=========================================================================================\n                            coef    std err          t      P&gt;|t|      [0.025      0.975]\n-----------------------------------------------------------------------------------------\nconst                   351.9268     20.166     17.452      0.000     312.373     391.481\nHHD_dep_education_PC   -165.8278     57.102     -2.904      0.004    -277.831     -53.824\nHHD_dep_employment_PC   -78.8281     86.937     -0.907      0.365    -249.352      91.696\nHHD_dep_health_PC       -99.3122     77.368     -1.284      0.199    -251.068      52.443\nHHD_dep_housing_PC     -140.5889     61.893     -2.271      0.023    -261.991     -19.187\n==============================================================================\nOmnibus:                      685.269   Durbin-Watson:                   1.660\nProb(Omnibus):                  0.000   Jarque-Bera (JB):             5645.947\nSkew:                           1.830   Prob(JB):                         0.00\nKurtosis:                      11.523   Cond. No.                         26.7\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\nOLS regression model\nThe model includes education, health, and housing deprivation dimensions. Figure 7.1.4. shows that all three are statistically significant at the 95% confidence level. The model has an R-squared value of 0.044, meaning only 4.4% of variance is accounted for. Further analysis of the residuals is warranted.\n# Create the independent Y variable\ny = final_df['Distance']\n\n# Create the dependent X variables\nX = final_df[['HHD_dep_education_PC','HHD_dep_health_PC','HHD_dep_housing_PC']]\n\n# Add the constant\nx1 = sm.add_constant(X)\n# Create OLS regression model\nmod = sm.OLS(y, x1)\n# Fit the model and save results\nresults = mod.fit()\n\nFigure 7.1.4. Results of OLS model including education, health, and housing deprivation dimensions\n\n\nData Source: ONS, 2023b; ONS, 2023d; ONS, 2023f; ONS, 2023g; OS, 2023; Author's calculations\n\n# Print results summary\nprint(results.summary())\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:               Distance   R-squared:                       0.044\nModel:                            OLS   Adj. R-squared:                  0.042\nMethod:                 Least Squares   F-statistic:                     23.91\nDate:                Mon, 13 May 2024   Prob (F-statistic):           3.95e-15\nTime:                        19:16:44   Log-Likelihood:                -10408.\nNo. Observations:                1575   AIC:                         2.082e+04\nDf Residuals:                    1571   BIC:                         2.085e+04\nDf Model:                           3                                         \nCovariance Type:            nonrobust                                         \n========================================================================================\n                           coef    std err          t      P&gt;|t|      [0.025      0.975]\n----------------------------------------------------------------------------------------\nconst                  359.1799     18.510     19.405      0.000     322.873     395.487\nHHD_dep_education_PC  -177.5110     55.626     -3.191      0.001    -286.620     -68.402\nHHD_dep_health_PC     -135.9300     65.988     -2.060      0.040    -265.364      -6.496\nHHD_dep_housing_PC    -173.7491     49.930     -3.480      0.001    -271.686     -75.812\n==============================================================================\nOmnibus:                      689.258   Durbin-Watson:                   1.658\nProb(Omnibus):                  0.000   Jarque-Bera (JB):             5763.638\nSkew:                           1.837   Prob(JB):                         0.00\nKurtosis:                      11.621   Cond. No.                         19.1\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\nAnalysis of observed versus fitted values and residuals\n\n# Obtain residuals from results\nresiduals = results.resid\n# Creating a DataFrame with 'OA21CD' and residuals\nresiduals_data = pd.DataFrame({\n    'OA21CD': final_df['OA21CD'],\n    'Residuals': residuals\n})\n\n# Add Residuals to final_df\nfinal_df = pd.merge(final_df, residuals_data, on='OA21CD')\nFigure 7.1.5. illustrates the relationship between the observed distances and fitted values from the model. There is an outlier – the largest distance in the observed values is no longer the largest in the fitted values – and fitted values have a much smaller range with the largest distance being closer to 325m.\n\n\nFigure 7.1.5. Observed vs Fitted Values for the OLS regression model\n\nData Source: Author's calculations\n\n# Set up a figure\nfig, ax = plt.subplots(figsize=(10, 6))\n\n# Set axis labels and title\nplt.xlabel('Fitted values - Distance (metres)', fontweight='bold', fontsize=12)\nplt.ylabel('Observed values - Distance (metres)', fontweight='bold', fontsize=12)\nplt.title('Observed vs Fitted Values for the OLS regression model', fontweight='bold')\n\n# Scatter plot showing Fitted against Observed values\nplt.scatter(x=results.fittedvalues, y=y, label='Data', s=3, c='#09435a') # add data and set colour and size of circles\n\n# Plot the line y=x\nplt.plot([min(results.fittedvalues), max(results.fittedvalues)],\n         [min(results.fittedvalues), max(results.fittedvalues)], linestyle='--', color='red', label='y=x')\n\n\n# Set the y-axis ticks at 250 increments\ntick_locations = np.arange(0, 2251, 250)\nax.set_yticks(tick_locations)\n\n# Add legend\nplt.legend()\n\nplt.show()\n\n\n\npng\n\n\nFigure 7.1.6. shows the variance in the residuals. No distinct relationship is visible with positive and negative residuals ranging from around -250 to +250, across all fitted distance values. This suggests the assumption that the relationship is linear is reasonable, and the variance of the error terms are equal. However, only larger positive residuals exist, meaning the model is not working as well for largest distances in the observed values.\n\n\nFigure 7.1.6. Residuals vs Fitted Values for the OLS regression model\n\nData Source: Author's calculations\n\n# Set up a figure\nfig, ax = plt.subplots(figsize=(10, 6))\n\n# Set axis labels and title\nplt.xlabel('Fitted values - Distance (metres)', fontweight='bold', fontsize=12)\nplt.ylabel('Residuals', fontweight='bold', fontsize=12)\nplt.title('Residuals vs Fitted Values for the OLS regression model', fontweight='bold')\n\n# Scatter plot showing Fitted values against Residuals\nplt.scatter(x=results.fittedvalues, y=final_df['Residuals'], label='Data', s=3, c='#09435a') # add data and set colour/size\n\n# Add dashed horizontal line at y=0\nplt.axhline(0, linestyle='--', color='black', linewidth=0.8)\n\n# Set the x-axis ticks at 250 increments\ntick_locations = np.arange(-250, 2001, 250)\nax.set_yticks(tick_locations)\n\nplt.show()\n\n\n\npng\n\n\nTo confirm this, figure 7.1.6. shows a histogram of the residuals, highlighting a normal distribution centred around 0, with a slight right skew.\n\n\nFigure 7.1.7. Histogram of residuals for the OLS regression model\n\nData Source: Author's calculations\n\n# Set up a figure\nfig, ax = plt.subplots(figsize=(10, 6))\n\n# Plot histogram of residuals\nsns.histplot(x=residuals, bins=50, ax=ax)\n\n# Set the x-axis ticks at 250 increments\ntick_locations = np.arange(-250, 2001, 250)\nax.set_xticks(tick_locations)\n\n# Label axes\nax.set_xlabel('Residuals', fontweight='bold')\nax.set_ylabel('Count', fontweight='bold')\n\n# Title and show the plot\nplt.title('Histogram of Residuals from OLS model', fontweight='bold', fontsize=12)\nplt.show()\n\n\n\npng\n\n\nLastly, a Q-Q plot is used to confirm the presence of outliers. One extreme outlier was visible in the previous charts, but a Q-Q plot allows for confirmation of other outliers closer to the rest of the data points. Figure 7.1.8. confirms that there are outliers in the residuals at both the top and bottom end.\n\n\nFigure 7.1.8. Q-Q Plot\n\nData Source: Author's calculations\n\n# Create a figure and axis\nfig, ax = plt.subplots(figsize=(10, 6))\n\n# Create a Q-Q plot\nprobplot(residuals, dist=\"norm\", plot=plt)\n\n# Add title and label axes\nplt.title(\"Q-Q Plot\", fontweight='bold')\nplt.xlabel(\"Theoretical Quantiles\", fontweight='bold')\nplt.ylabel(\"Ordered Values\", fontweight='bold')\n\n# Change colour of circles\nax.lines[0].set_markerfacecolor('#4e7d8e')\nax.lines[0].set_markeredgecolor('#4e7d8e')\n\nplt.show()\n\n\n\npng\n\n\n\n\n\n7.2. Moran’s I\nMoran’s I is used to detect high and low values concentrated spatially, and the spatial relationship between neighbours is random if zero association is found (Paez and Scott, 2004). Queens weights are used, meaning any OA bordering the given OA is included.\nThe I value is 0.46 and the p-value is 0.001. These are positive and significant respectively, implying distance to nearest greenspace is spatially correlated - higher values and lower values cluster spatially. Figure 7.2.1. shows this on a scatterplot.\n# Calculate the queen weights based on the OA\nw_queen = weights.Queen.from_dataframe(final_df, ids = 'OA21CD')\n# Calculate Moran's I using queen weights\nmi = esda.Moran(final_df['Distance'], w_queen)\n# I value\nprint(f'I value: {mi.I}')\n\n# Significance (p value)\nprint(f'P value: {mi.p_sim}')\n\n\n\nI value\n0.45882055605281585\n\n\n\n\nP value\n0.001\n\n\n\n\nFigure 7.2.1. Moran Scatterplot\n\nData Source: Author's calculations\n\nfig, ax = plt.subplots(figsize=(9, 9))  # Set figure size\nmoran_scatterplot(mi, ax=ax)  # Create Moran's I scatterplot\n\n# Change the color and size of points\nscatter_plot = ax.get_children()[0]\nscatter_plot.set_facecolor('#09435a')\nscatter_plot.set_edgecolor('#09435a')\nscatter_plot.set_sizes([10])\n\n# Show the plot\nplt.show()\n\n\n\npng\n\n\nTo minimise outlier impact, the distance value for this OA will be reduced to the second highest value.\n# Find maximum distance\nfinal_df['Distance'].max()\n2090.9863451326555\n# Find the second highest distance\nfinal_df[final_df['Distance'] &lt; final_df['Distance'].max()]['Distance'].max()\n1268.1070267044204\ndistance_sort = final_df.Distance.tolist() # Create a list of distances\ndistance_sort.remove(max(distance_sort)) # Remove maximum value\n# Set the new max (second highest)\nfinal_df['Distance_Clean'] = final_df.Distance #create a new column\nfinal_df.loc[final_df['Distance']&gt;1500, 'Distance_Clean'] = max(distance_sort) \n# If the distance is greater than 1500 in the original variable, set it equal to the maximum value of our cleaned variable\n# Use 1500 as a buffer to the actual 2nd highest distance shown in the previous cell\n# Check new max is expected value\nprint(final_df['Distance_Clean'].max())\nprint(final_df['Distance_Clean'].max() == final_df[final_df['Distance'] &lt; final_df['Distance'].max()]['Distance'].max())\n1268.1070267044204\nTrue\nMoran’s I is re-calculated and the I value is now 0.47, and the p-value remains 0.001. These results are still positive and significant respectively, so distance to nearest greenspace remains spatially correlated. Figure 7.2.2. shows the outlier is removed.\n# Calculate new Moran's I using queen weights and cleaned Distance variable\nmi_adj = esda.Moran(final_df['Distance_Clean'], w_queen)\n# I value\nprint(f'I value: {mi_adj.I}')\n\n# Significance (p value)\nprint(f'P value: {mi_adj.p_sim}')\n\n\n\nI value\n0.47144858850487537\n\n\n\n\nP value\n0.001\n\n\n\n\n\nFigure 7.2.2. Moran Scatterplot (Adjusted)\n\nData Source: Author's calculations\n\nfig, ax = plt.subplots(figsize=(9, 9))  # Set figure size\nmoran_scatterplot(mi_adj, ax=ax)  # Create Moran's I scatterplot\n\n# Change the color and size of points\nscatter_plot = ax.get_children()[0]\nscatter_plot.set_facecolor('#09435a')\nscatter_plot.set_edgecolor('#09435a')\nscatter_plot.set_sizes([10])\n\n# Show the plot\nplt.show()\n\n\n\npng\n\n\n\n\n\n7.3. LISA Clusters\nSpatial association will be tested and visualised using Local Indicators of Spatial Association (LISA) clusters. This will be conducted on the dependent variable and residuals from Section 7.1.\n\nDistance\nSignificance is calculated to the 95% level and a LISA cluster assigned. Figure 7.3.1. shows each LISA cluster, with clear spatial patterns emerging. To the north and south, and north of Bradford city centre, there are High-High clusters, meaning these OAs have high distances to the nearest greenspace as do their neighbouring OAs. Meanwhile, Low-Low clusters can be observed around the city of Bradford suburbs and Keighley town centre.\n# Calculate LISA clusters\nlisa_Distance = esda.Moran_Local(final_df['Distance_Clean'], w_queen)\nfinal_df['Sig_Distance'] = lisa_Distance.p_sim &lt; 0.05 # Calculate a variable to show which are significant at the 95% level\nfinal_df['Quad_Distance'] = lisa_Distance.q # Calculate a variable indicating the respective quadrant\n# Create categorical column for the map visual\nfinal_df['LISA_Distance'] = np.select(\n    [\n        (final_df['Sig_Distance'] == False),\n        (final_df['Sig_Distance'] == True) & (final_df['Quad_Distance'] == 1),\n        (final_df['Sig_Distance'] == True) & (final_df['Quad_Distance'] == 2),\n        (final_df['Sig_Distance'] == True) & (final_df['Quad_Distance'] == 3),\n        (final_df['Sig_Distance'] == True) & (final_df['Quad_Distance'] == 4)\n    ],\n    [\n        'Not Significant',\n        'High-High',\n        'Low-High',\n        'Low-Low',\n        'High-Low'\n    ],\n    default='ERROR'  # Return \"ERROR\" string if conditions above are not met\n)\n\n\nFigure 7.3.1. Map of distance LISA Clusters by OA in Bradford LAD\n\nData Source: Author's calculations\n\ncustom_cmap = mcolors.ListedColormap(['#E14D2A','#FACF5A','#4F9DA6','#233142','lightGrey'])\n\nfig, ax = plt.subplots(figsize=(15, 15))\nfinal_df.plot(column='LISA_Distance', \n              ax=ax, \n              categorical=True,\n              legend=True, \n              cmap=custom_cmap)\n\n# Add basemap\nctx.add_basemap(ax, crs=final_df.crs.to_string(), source=ctx.providers.CartoDB.Positron)\n\n# Customize plot\nax.set_axis_off()\n\n# Show plot\nplt.show()\n\n\n\npng\n\n\n\n\nResiduals\nFigure 7.3.2. shows similar spatial patterns for the residual LISA clusters. There is one noticeable OA to the north that is Low-High - this OA has a small residual error compared to neighbouring OAs – and fewer Low-High clusters in central areas. Despite these differences, spatial association is still present across much of the district.\n# Calculate LISA clusters\nlisa_Residuals = esda.Moran_Local(final_df['Residuals'], w_queen)\nfinal_df['Sig_Residuals'] = lisa_Residuals.p_sim &lt; 0.05 # Calculate a variable to show which are significant at the 95% level\nfinal_df['Quad_Residuals'] = lisa_Residuals.q # Calculate a variable indicating the respective quadrant\n# Create categorical column for the map visual\nfinal_df['LISA_Residuals'] = np.select(                                           # Select rows meeting following rules\n    [\n        (final_df['Sig_Residuals'] == False),                                     # Not significant\n        (final_df['Sig_Residuals'] == True) & (final_df['Quad_Residuals'] == 1),  # Significant and Quadrant 1\n        (final_df['Sig_Residuals'] == True) & (final_df['Quad_Residuals'] == 2),  # Significant and Quadrant 2\n        (final_df['Sig_Residuals'] == True) & (final_df['Quad_Residuals'] == 3),  # Significant and Quadrant 3\n        (final_df['Sig_Residuals'] == True) & (final_df['Quad_Residuals'] == 4)   # Significant and Quadrant 4\n    ],\n    [\n        'Not Significant',                                                        # Assign string based on above rules\n        'High-High',\n        'Low-High',\n        'Low-Low',\n        'High-Low'\n    ],\n    default='ERROR'  # Return \"ERROR\" string if conditions above are not met\n)\n\n\nFigure 7.3.2. Map of residual LISA Clusters by OA in Bradford LAD\n\nData Source: Author's calculations\n\nfig, ax = plt.subplots(figsize=(15, 15))\n\ncustom_cmap = mcolors.ListedColormap(['#E14D2A','#FACF5A','#4F9DA6','#233142','lightGrey'])\n\n# Plot choropleth map\nfinal_df.plot(column='LISA_Residuals', \n              ax=ax, \n              categorical=True,\n              legend=True, \n              cmap=custom_cmap)\n\n# Add basemap\nctx.add_basemap(ax, crs=final_df.crs.to_string(), source=ctx.providers.CartoDB.Positron)\n\n# Customize plot\nax.set_axis_off()\n\n# Add title to legend\nax.get_legend().set_title('LISA Clusters for Residuals')\n\n# Show plot\nplt.show()\n\n\n\npng\n\n\nResults from the Moran’s I and LISA cluster analysis confirms a Geographically Weighted Regression (GWR) model is required to ensure spatial association is considered.\n\n\n\n7.4. GWR model\nBandwidth is the optimal number of datapoints that will be included in each local regression model. The bandwidth calculated is 54.0 – the 54 nearest datapoints of the 1,575 OAs in the dataset will be included, hence each local model will be using roughly 3% of the available data.\n# Split the geometry into X and Y columns\nBradford_OA_PWC_tidy['X'] = Bradford_OA_PWC_tidy['geometry'].x\nBradford_OA_PWC_tidy['Y'] = Bradford_OA_PWC_tidy['geometry'].y\n# Add this to the final_df\nfinal_df = final_df.merge(Bradford_OA_PWC_tidy, how='inner', on='OA21CD').drop(['geometry'], axis=1)\ng_coords = list(zip(final_df.X, final_df.Y)) # Create a list of x and y coordinates\n\n# Create y variable\ng_y = np.asarray(final_df.Distance_Clean).reshape((-1,1))\n\n# Create x variables\ng_X = final_df[['HHD_dep_education_PC', 'HHD_dep_health_PC', 'HHD_dep_housing_PC']]\ng_X = np.asarray(g_X)\n\ngwr_selector = Sel_BW(g_coords, g_y, g_X) #set parameters for calculating the bandwidth\n# Calculate optimised bandwidth\nbw = gwr_selector.search()\nprint(bw)\n54.0\nThe GWR model (figure 7.4.1) includes global regression results reflecting the OLS model (section 7.1), whilst GWR results give details of the local models. The R-squared value is 0.458 (45.8% of variance is accounted for) which is higher than the global regression model.\n# Create GWR model\ngwr = GWR(g_coords, g_y, g_X, bw)\n\n# Fit the model\ngwr_results = gwr.fit()\n\nFigure 7.4.1. Results of GWR model\n\nData Source: Author's calculations\n\n# View results summary\nprint(gwr_results.summary())\n===========================================================================\nModel type                                                         Gaussian\nNumber of observations:                                                1575\nNumber of covariates:                                                     4\n\nGlobal Regression Results\n---------------------------------------------------------------------------\nResidual sum of squares:                                       48341131.294\nLog-likelihood:                                                  -10371.107\nAIC:                                                              20750.214\nAICc:                                                             20752.253\nBIC:                                                           48329565.576\nR2:                                                                   0.045\nAdj. R2:                                                              0.043\n\nVariable                              Est.         SE  t(Est/SE)    p-value\n------------------------------- ---------- ---------- ---------- ----------\nX0                                 356.449     18.078     19.717      0.000\nX1                                -188.300     54.328     -3.466      0.001\nX2                                -123.805     64.449     -1.921      0.055\nX3                                -168.246     48.766     -3.450      0.001\n\nGeographically Weighted Regression (GWR) Results\n---------------------------------------------------------------------------\nSpatial kernel:                                           Adaptive bisquare\nBandwidth used:                                                      54.000\n\nDiagnostic information\n---------------------------------------------------------------------------\nResidual sum of squares:                                       27460005.829\nEffective number of parameters (trace(S)):                          269.555\nDegree of freedom (n - trace(S)):                                  1305.445\nSigma estimate:                                                     145.034\nLog-likelihood:                                                   -9925.735\nAIC:                                                              20392.580\nAICc:                                                             20505.314\nBIC:                                                              21843.301\nR2:                                                                   0.458\nAdjusted R2:                                                          0.346\nAdj. alpha (95%):                                                     0.001\nAdj. critical t value (95%):                                          3.380\n\nSummary Statistics For GWR Parameter Estimates\n---------------------------------------------------------------------------\nVariable                   Mean        STD        Min     Median        Max\n-------------------- ---------- ---------- ---------- ---------- ----------\nX0                      292.885    208.447   -231.865    259.474   1217.183\nX1                     -165.196    519.824  -1890.328   -113.466   1930.027\nX2                        6.041    578.986  -2557.624    -23.072   2446.111\nX3                     -291.652    856.329  -4271.564   -262.118   5015.975\n===========================================================================\n\nNone\nVisualising results requires defining the t-value significance. Figure 7.4.1. shows there are 1,305 degrees of freedom which, using a standard t-table, gives a significance value of -/+1.96 at the 0.05 level. This is the parameter used to assign significance.\nOn each choropleth, only OAs with a thick outline are significant in the t-value results. This highlights where there are, or are not, significant results in the local models. The colour shows whether these relationships (coefficients) are positive or negative.\n# Add GWR coeffients to final_df for each predictor variable\nfinal_df['GWR_Education_Coefficient'] = gwr_results.params[:,1]\nfinal_df['GWR_Health_Coefficient'] = gwr_results.params[:,2]\nfinal_df['GWR_Housing_Coefficient'] = gwr_results.params[:,3]\n# Add GWR t-values to final_df for each predictor variable\nfinal_df['GWR_Education_tvalue'] = gwr_results.tvalues[:,1]\nfinal_df['GWR_Health_tvalue'] = gwr_results.tvalues[:,2]\nfinal_df['GWR_Housing_tvalue'] = gwr_results.tvalues[:,3]\n# Function to create categorical column for the map visual based on t-value significance\ndef create_categorical(Variable):      # takes variable as string\n    new_col = Variable + '_significance'   # create new column name\n    final_df[new_col] = np.select(                                          # Select rows meeting following rules\n        [\n            (final_df[Variable] &lt; -1.96),                                   # t-value significance less than -1.96\n            (final_df[Variable] &gt; 1.96),                                    # t-value significance greater than 1.96\n            (final_df[Variable] &gt;= -1.96) & (final_df[Variable] &lt;= 1.96)    # t-value not significant\n        ],\n        [\n            'Significant (negative t-value)',                               # Assign string based on above rules\n            'Significant (positive t-value)',\n            'Not significant'\n        ],\n        default='ERROR'  # Return \"ERROR\" string if conditions above are not met\n    )\n# Add columns to dataframe using the function above\ncreate_categorical('GWR_Education_tvalue')\ncreate_categorical('GWR_Health_tvalue')\ncreate_categorical('GWR_Housing_tvalue')\n# Create subsets of the data for the purposes of the map visual\n# Only rows where the t-value is significant are needed to highlight the outline of the OAs on the map\nGWR_Emp_sig = final_df[(final_df['GWR_Education_tvalue_significance'] == 'Significant (negative t-value)') |\n                      (final_df['GWR_Education_tvalue_significance'] == 'Significant (positive t-value)')]\n\nGWR_Health_sig = final_df[(final_df['GWR_Health_tvalue_significance'] == 'Significant (negative t-value)') |\n                      (final_df['GWR_Health_tvalue_significance'] == 'Significant (positive t-value)')]\n\nGWR_Housing_sig = final_df[(final_df['GWR_Housing_tvalue_significance'] == 'Significant (negative t-value)') |\n                      (final_df['GWR_Housing_tvalue_significance'] == 'Significant (positive t-value)')]\ndef viz_coefficients_t_value(final_df, Coeff_variable, Sig_df, Sig_variable):\n    # Visualise the coefficients\n    fig, ax = plt.subplots(figsize=(15, 15))\n    \n    # Plot coefficients choropleth\n    coef_plot = final_df.plot(column=Coeff_variable, \n                              ax=ax, \n                              cmap='RdYlBu', \n                              scheme='userdefined', \n                              classification_kwds=dict(bins=[-1000, -500, 0, 500, 1000]), \n                              legend=True,\n                              legend_kwds={'title': 'Coefficients'},\n                              alpha=0.75)\n    \n    # Plot significance information\n    for index, row in Sig_df.iterrows():\n        if \"Significant\" in row[Sig_variable]:\n            if \"positive\" in row[Sig_variable]:\n                final_df[final_df['OA21CD'] == row['OA21CD']].plot(ax=ax, \n                                                                    color='none', \n                                                                    edgecolor='#000026', \n                                                                    linewidth=2)\n            elif \"negative\" in row[Sig_variable]:\n                final_df[final_df['OA21CD'] == row['OA21CD']].plot(ax=ax, \n                                                                    color='none', \n                                                                    edgecolor='#800020',  # Burgundy color\n                                                                    linewidth=2)\n        else:\n            final_df[final_df['OA21CD'] == row['OA21CD']].plot(ax=ax, \n                                                                color='none', \n                                                                edgecolor='none')\n\n    # Customize plot\n    ax.set_axis_off()\n    \n    # Add basemap\n    ctx.add_basemap(ax, crs=final_df.crs.to_string(), source=ctx.providers.CartoDB.Positron)\n    \n    # Extract legend labels and handles\n    try:\n        coef_legend_elements = coef_plot.get_legend().legendHandles\n        coef_legend_labels = [text.get_text() for text in coef_plot.get_legend().get_texts()]\n    except AttributeError:\n        coef_legend_elements, coef_legend_labels = coef_plot.get_legend_handles_labels()\n    \n    # Create legend for both coefficient values and significance\n    sig_legend_elements = [Line2D([0], [0], color='#000026', lw=2, label='Positive Significant'),\n                           Line2D([0], [0], color='#800020', lw=2, label='Negative Significant')]\n    legend_handles = coef_legend_elements + sig_legend_elements\n    legend_labels = coef_legend_labels + ['Significant - positive t-value', 'Significant - negative t-value']\n    ax.legend(handles=legend_handles, labels=legend_labels, loc='upper right', title=\"Coefficients\")\n    \n    # Show plot\n    plt.show()\nFigure 7.4.2. shows statistically significant spatial patterns for education deprivation, with clusters of negative relationships in the north, west and central areas. This means that in these areas, as education deprivation increases, the distance to the nearest greenspace decreases. By contrast, clusters of positive relationships can be seen in the northwest and southeast.\n\n\nFigure 7.4.2. Map of GWR model results and significance for education deprivation, by OA in the Bradford LAD\n\nData Source: Author's calculations\n\n# Call the function with the required parameters\nviz_coefficients_t_value(final_df, 'GWR_Education_Coefficient', GWR_Emp_sig, 'GWR_Education_tvalue_significance')\n\n\n\npng\n\n\nHealth deprivation (figure 7.4.3.) displays clusters of negative relationships in the north and Bradford city centre. Positive relationships exist in the northwest, south and suburbs north of Bradford city centre. This contrasts education deprivation but is not unexpected given higher levels of health deprivation shown in figure 6.3.3. across the district and that the spatial pattern of this varied compared to other deprivation dimensions.\n\n\nFigure 7.4.3. Map of GWR model results and significance for health deprivation, by OA in the Bradford LAD\n\nData Source: Author's calculations\n\n# Call the function with the required parameters\nviz_coefficients_t_value(final_df, 'GWR_Health_Coefficient', GWR_Emp_sig, 'GWR_Health_tvalue_significance')\n\n\n\npng\n\n\nFigure 7.4.4. shows housing deprivation has large negative clusters in the north and south, and Bradford city centre. There are fewer statistically significant positive relationships across the district, with a small number in the north and east.\n\n\nFigure 7.4.4. Map of GWR model results and significance for housing deprivation, by OA in the Bradford LAD\n\nData Source: Author's calculations\n\n# Call the function with the required parameters\nviz_coefficients_t_value(final_df, 'GWR_Housing_Coefficient', GWR_Emp_sig, 'GWR_Housing_tvalue_significance')\n\n\n\npng\n\n\nLastly, mapping residuals of the GWR model demonstrate the change from the OLS regression model (figure 7.3.2.). Figure 7.4.5. shows the GWR model removed some residual autocorrelation but not all of it. This means some spatial variation has still not been captured. This is confirmed by recalculating the Moran’s I value and significance. The I value has decreased to 0.23 showing clustering has been reduced but is still present. The p-value of 0.001 shows this is statistically significant.\n# Add GWR residuals to the final_df\nfinal_df['GWR_residuals'] = gwr_results.resid_response\n# Calculate LISA clusters as before but with GWR_residuals\nlisa_GWR_Residuals = esda.Moran_Local(final_df['GWR_residuals'], w_queen)\n# Calculate the 95% significance and quadrants as before but for the GWR_Residuals\nfinal_df['Sig_GWR_Residuals'] = lisa_GWR_Residuals.p_sim &lt; 0.05\nfinal_df['Quad_GWR_Residuals'] = lisa_GWR_Residuals.q\n# Create categorical column for the map visual using if/and rules as before\nfinal_df['LISA_GWR_Residuals'] = np.select(\n    [\n        (final_df['Sig_GWR_Residuals'] == False),\n        (final_df['Sig_GWR_Residuals'] == True) & (final_df['Quad_GWR_Residuals'] == 1),\n        (final_df['Sig_GWR_Residuals'] == True) & (final_df['Quad_GWR_Residuals'] == 2),\n        (final_df['Sig_GWR_Residuals'] == True) & (final_df['Quad_GWR_Residuals'] == 3),\n        (final_df['Sig_GWR_Residuals'] == True) & (final_df['Quad_GWR_Residuals'] == 4)\n    ],\n    [\n        'Not Significant',\n        'High-High',\n        'Low-High',\n        'Low-Low',\n        'High-Low'\n    ],\n    default='ERROR'  # Return \"ERROR\" string if conditions above are not met\n)\n\n\nFigure 7.4.5. Map of GWR residual LISA Clusters by OA in Bradford LAD\n\nData Source: Author's calculations\n\n# Define custom colormap\ncustom_cmap = mcolors.ListedColormap(['#E14D2A','#FACF5A','#4F9DA6','#233142','lightGrey'])\n\n# Plot choropleth\nfig, ax = plt.subplots(figsize=(15, 15))\nfinal_df.plot(column='LISA_GWR_Residuals', \n              ax=ax, \n              categorical=True,\n              legend=True,\n              cmap=custom_cmap,  \n              edgecolor='black',\n              linewidth=0.25,\n              legend_kwds={'loc': 'upper right', 'title': 'LISA GWR Residuals'})\n\n# Add basemap\nctx.add_basemap(ax, crs=final_df.crs.to_string(), source=ctx.providers.CartoDB.Positron)\n\n# Customize plot\nax.set_axis_off()\n\n# Show map\nplt.show()\n\n\n\npng\n\n\n# Calculate Moran's I\nmi = esda.Moran(final_df['GWR_residuals'], w_queen)\n# I value\nprint(f'I value: {mi.I}')\n\n# Significance (p value)\nprint(f'P value: {mi.p_sim}')\n\n\n\nI value\n0.23403007956028024\n\n\n\n\nP value\n0.001"
  },
  {
    "objectID": "projects/access-to-greenspace/access-to-greenspace-bradford.html#discussion",
    "href": "projects/access-to-greenspace/access-to-greenspace-bradford.html#discussion",
    "title": "A spatial analysis of access to greenspace and deprivation in Bradford",
    "section": "8. Discussion",
    "text": "8. Discussion\nResults showed deprivation and distance to greenspace are negatively correlated, reflecting much of the literature, and rejects the null hypothesis posed. However, the relationship differs by dimension and spatially. Any policy or intervention needs to consider this.\nKey differentiators of this study are using deprivation dimensions and OA geographies, attempting to consider distance at a local level appropriate to the Natural England (2023) targets. However, there are limitations and opportunities for improvement.\nFirstly, not all greenspace is considered. Many woodlands, moorlands, river and canal paths are not included but are publicly accessible. Including this would better reflect true greenspace but would also raise consideration of type. Different greenspace serves different purposes – for example, a children’s park versus unmaintained moorland – and so understanding access to different types may further unlock insight into what is within an accessible distance and for whom.\nSecondly, the distance calculation could be improved. Utilising address data to estimate household locations, alongside road and path networks, would allow more accurate calculation of distance to greenspace. This could be an average walk time which would align better to Natural England (2023) targets.\nLastly, two limitations identified in the literature remain: quality and usage. Quality can be subjective and dependent on the greenspace’s purpose. However, a survey recording facilities, amenities, and biodiversity could develop a qualitative understanding. The Natural Environment Scoring Tool (NEST) used in Ferguson et al’s (2018) study provides a starting point. Technological advancements could improve usage data, such as GPS which was used by Mears et al (2021), however data protection, scale, and availability challenges exist.\nDespite these limitations, actions could result from the findings. Deprivation differs by dimension and policy needs to consider this, as well as communities and their needs. These needs will vary by area. This is supported by research conducted on community perceptions (McEachan et al, 2018), highlighting the importance of including communities in decision-making processes.\nFinally, two areas of future research are identified. Firstly, including a broader set of demographic measures beyond deprivation. Age and ethnicity are key, as shown in the literature, but no research addressed in this study has considered variables such as profession which could have an impact on how and when someone accesses greenspace. Secondly, there is opportunity to reflect on the impact of new greenspace, or improvements to current greenspace, to learn and adapt plans. CBMDC (2021) have clear goals they wish to achieve, but understanding the effectiveness of interventions is needed to ensure investments add value. Deprivation can be a relatively slow to change measure. Further studies analysing more reactive variables may enable a comprehensive review of what works in different communities."
  },
  {
    "objectID": "projects/access-to-greenspace/access-to-greenspace-bradford.html#conclusion",
    "href": "projects/access-to-greenspace/access-to-greenspace-bradford.html#conclusion",
    "title": "A spatial analysis of access to greenspace and deprivation in Bradford",
    "section": "9. Conclusion",
    "text": "9. Conclusion\nThis study sought to build on previous research into the relationship between distance to greenspace and deprivation by splitting deprivation into the component dimensions. The findings mirror the literature that there is a statistically significant, negative correlation between distance to greenspace and deprivation, but the use of dimensions and understanding this spatially has shown that the results vary by area. Policy implications are that decisions should be on a local scale, and interventions may vary depending on the community and their needs.\nHowever, this is not only tied to deprivation and this study has argued that a more holistic view of demographics within communities is required, and consideration for the type of greenspace and its uses. This will allow planners to develop better suited solutions to improve community health and wellbeing, with greenspace that meets the needs and wants of the communities they serve."
  },
  {
    "objectID": "projects/access-to-greenspace/access-to-greenspace-bradford.html#references",
    "href": "projects/access-to-greenspace/access-to-greenspace-bradford.html#references",
    "title": "A spatial analysis of access to greenspace and deprivation in Bradford",
    "section": "References",
    "text": "References\nBarbosa, O., Tratalos. J.A., Armsworth, P.R., Davies, R.G., Fuller, R.A., Johnson, P. and Gaston, K.J. 2007. Who benefits from access to green space? A case study from Sheffield, UK. Landscape and Urban Planning. 83(2-3), pp. 187-195.\nCity of Bradford Metropolitan District Council (CBMDC). 2019. Public Health - Joint Strategic Needs Assessment. [Online]. Bradford: City of Bradford Metropolitan District Council. [Accessed 09 January 2024]. Available from: https://jsna.bradford.gov.uk/\nCity of Bradford Metropolitan District Council (CBMDC). 2021. Our Council Plan: Priorities and Principles 2021-2025. [Online]. Bradford: City of Bradford Metropolitan District Council. [Accessed 09 January 2024]. Available from: https://www.bradford.gov.uk/media/6508/bradfordcouncilplan.pdf\nCity of Bradford Metropolitan District Council (CBMDC). 2022. 2021 Census: Bradford District. [Online]. [Accessed 09 January 2024]. Available from: https://ubd.bradford.gov.uk/about-us/2021-census/\nFerguson, M., Roberts, H.E., McEachan, R.R.C. and Dallimer, M. 2018. Contrasting distributions of urban green infrastructure across social and ethno-racial groups. Landscape and Urban Planning. 175, pp.136-148.\nGidlow, C.J. and Ellis, N.J. 2011. Neighbourhood green space in deprived urban communities: issues and barriers to use. The International Journal of Justice and Sustainability. 16(10), pp.989-1002.\nHoulden, V., Porto de Albuquerque, J., Weich, S. and Jarvis, S. 2019. A spatial analysis of proximate greenspace and mental wellbeing in London. Applied Geography. 109, pp.102036.\nJia, P., Cao, X., Yang, H., Dai, S., He, P., Huang, G., Wu, T. and Wang, Y. 2021. Green space access in the neighbourhood and childhood obesity. Obesity Reviews. 22(51), pp.13100.\nJones, A., Hillsdon, M. and Coombes, E. 2009. Greenspace access, use, and physical activity: Understanding the effects of area deprivation. Preventive Medicine. 49(6), pp.500-505.\nKessel, A., Green, J., Pinder, R., Wilkinson, P., Grundy, C. and Lachowycz, K. 2009. Multidisciplinary research in public health: A case study of research on access to green space. Public Health. 123(1), pp.32-38.\nLachowycz, K. and Jones, A.P. 2011. Greenspace and obesity: a systematic review of the evidence. Obesity Reviews. 12(5), pp.183-189.\nMcCormick, R. 2017. Does Access to Green Space Impact the Mental Well-being of Children: A Systematic Review. Journal of Pediatric Nursing. 37, pp.3-7.\nMcEachan, R.R.C., Yang, T.C., Roberts, H., Pickett, K.E., Arseneau-Powell, D., Gidlow, C.J., Wright, J. and Nieuwenhuijsen, M. 2018. Availability, use of, and satisfaction with green space, and children’s mental wellbeing at age 4 years in a multicultural, deprived, urban area: results from the Born in Bradford cohort study. The Lancet Planetary Health. 2(6), pp.244-254.\nMears, M., Brindley, P., Barrows, P., Richardson, M. and Maheswaran, R. 2021. Mapping urban greenspace use from mobile phone GPS data. PLoS ONE. 16(7), pp.0248622.\nMears, M., Brindley, P., Maheswaran, R. and Jorgensen, A. 2019. Understanding the socioeconomic equity of publicly accessible greenspace distribution: The example of Sheffield, UK. Geoforum. 103, pp.126-137.\nMueller, N., Rojas-Rueda, D., Khreis, H., Cirach, M., Milà, C., Espinosa, A., Foraster, M., McEachan, R.R.C., Kelly, B., Wright, J. and Nieuwenhuijsen, M. 2018. Socioeconomic inequalities in urban and transport planning related exposures and mortality: A health impact assessment study for Bradford, UK. Environment International. 121(1), pp.931-941.\nNatural England. 2023. Natural England unveils new Green Infrastructure Framework. [Press release]. [Accessed 09 January 2024]. Available from: https://www.gov.uk/government/news/natural-england-unveils-new-green-infrastructure-framework\nOffice for National Statistics (ONS). 2021. Census 2021 geographies. [Online]. [Accessed 09 January 2024]. Available from: https://www.ons.gov.uk/methodology/geography/ukgeographies/censusgeographies/census2021geographies\nOffice for National Statistics (ONS). 2023a. Output Areas (2021) Boundaries EW BFC. Open Geography Portal. [Online]. [Accessed 11 December 2023]. Available from: https://geoportal.statistics.gov.uk/datasets/ons::output-areas-2021-boundaries-ew-bfc/about\nOffice for National Statistics (ONS). 2023b. Output Areas (December 2021) PWC (V3). Open Geography Portal. [Online]. [Accessed 11 December 2023]. Available from: https://geoportal.statistics.gov.uk/datasets/ons::output-areas-december-2021-pwc-v3/about\nOffice for National Statistics (ONS). 2023c. Output Area to Lower layer Super Output Area to Middle layer Super Output Area to Local Authority District (December 2021) Lookup in England and Wales V2. Open Geography Portal. [Online]. [Accessed 14 December 2023]. Available from: https://geoportal.statistics.gov.uk/datasets/output-area-to-lower-layer-super-output-area-to-middle-layer-super-output-area-to-local-authority-district-december-2021-lookup-in-england-and-wales-v2-1/about\nOffice for National Statistics (ONS). 2023d. Household deprivation in the education dimension. Office for National Statistics. [Online]. [Accessed 14 December 2023]. Available from: https://www.ons.gov.uk/filters/d7bede85-c97b-4b85-84a8-87b5164d3add/dimensions\nOffice for National Statistics (ONS). 2023e. Household deprivation in the employment dimension. Office for National Statistics. [Online]. [Accessed 14 December 2023]. Available from: https://www.ons.gov.uk/filters/103a7748-96d9-4e79-97e2-e1cc42ef7024/dimensions\nOffice for National Statistics (ONS). 2023f. Household deprivation in the health dimension. Office for National Statistics. [Online]. [Accessed 14 December 2023]. Available from: https://www.ons.gov.uk/filters/15149637-c729-4aa5-b817-d687cad093d8/dimensions\nOffice for National Statistics (ONS). 2023g. Household deprivation in the housing dimension. Office for National Statistics. [Online]. [Accessed 14 December 2023]. Available from: https://www.ons.gov.uk/filters/cf7beddc-198f-411a-9b66-9c690d18e3bf/dimensions\nOffice for National Statistics (ONS). 2023h. How life has changed in Bradford: Census 2021. [Online]. [Accessed 09 January 2024]. Available from: https://www.ons.gov.uk/visualisations/censusareachanges/E08000032/\nOffice for National Statistics (ONS). 2023i. Rising ill-health and economic inactivity because of long-term sickness, UK: 2019 to 2023. [Online]. [Accessed 11 January 2024]. Available from: https://rb.gy/bsf8hg\nOrdnance Survey (OS). 2023. OS Open Greenspace. Ordnance Survey. [Online]. [Accessed 6 December 2023]. Available from: https://osdatahub.os.uk/downloads/open/OpenGreenspace\nPaez, A. and Scott, D.M. 2004. Spatial statistics for urban analysis: A review of techniques with examples. GeoJournal. 61, pp.53-67.\nRoe, J., Aspinall, P.A. and Thompson, C.W. 2016. Understanding Relationships between Health, Ethnicity, Place and the Role of Urban Green Space in Deprived Urban Communities. International Journal of Environmental Research and Public Health. 13(7), pp.681.\nWood, E., Harsant, A., Dallimer, M., Cronin de Chavez, A., McEachan, R.R.C. and Hassall, C. 2018. Not All Green Space Is Created Equal: Biodiversity Predicts Psychological Restorative Benefits From Urban Green Space. Frontiers in Psychology. 9, pp2320."
  },
  {
    "objectID": "projects/access-to-greenspace/access-to-greenspace-bradford.html#appendix-1---wrangled-dataframes",
    "href": "projects/access-to-greenspace/access-to-greenspace-bradford.html#appendix-1---wrangled-dataframes",
    "title": "A spatial analysis of access to greenspace and deprivation in Bradford",
    "section": "Appendix 1 - Wrangled dataframes",
    "text": "Appendix 1 - Wrangled dataframes\nThis appendix contains visual checks of the top five rows of the wrangled dataframes created in section 5.\n\nBradford OA polygons dataframe\nBradford_OA_polygons.head()\n\n\n\n\n\n\nOA21CD\n\n\nPolygon\n\n\nCity\n\n\n\n\n\n\n0\n\n\nE00053353\n\n\nPOLYGON ((415817.093 440872.597, 415821.094 44…\n\n\nBradford\n\n\n\n\n1\n\n\nE00053354\n\n\nPOLYGON ((415078.000 439967.001, 415058.323 43…\n\n\nBradford\n\n\n\n\n2\n\n\nE00053355\n\n\nPOLYGON ((416252.367 439816.041, 416253.270 43…\n\n\nBradford\n\n\n\n\n3\n\n\nE00053356\n\n\nPOLYGON ((416668.000 439392.028, 416667.653 43…\n\n\nBradford\n\n\n\n\n4\n\n\nE00053357\n\n\nPOLYGON ((415143.909 439176.235, 415143.000 43…\n\n\nBradford\n\n\n\n\n\n\nBradford OA population weighted centroids dataframe\nBradford_OA_PWC.head()\n\n\n\n\n\n\nOA21CD\n\n\nPWC\n\n\n\n\n\n\n0\n\n\nE00053353\n\n\nPOINT (413638.052 439495.615)\n\n\n\n\n1\n\n\nE00053354\n\n\nPOINT (414837.013 439813.246)\n\n\n\n\n2\n\n\nE00053355\n\n\nPOINT (416162.559 439674.009)\n\n\n\n\n3\n\n\nE00053356\n\n\nPOINT (416591.137 439417.227)\n\n\n\n\n4\n\n\nE00053357\n\n\nPOINT (414671.681 439110.823)\n\n\n\n\n\n\nBradford greenspace polygons (unique list)\nBradford_greenspace_site_unique.head()\n\n\n\n\n\n\nGreenspace_ID\n\n\nType\n\n\nPolygon\n\n\n\n\n\n\n0\n\n\n0295ED18-D538-5C37-E063-AAEFA00A445E\n\n\nPlaying Field\n\n\nPOLYGON Z ((414018.070 438415.690 0.000, 41399…\n\n\n\n\n1\n\n\n0295ED18-E5D4-5C37-E063-AAEFA00A445E\n\n\nPlaying Field\n\n\nPOLYGON Z ((415193.700 439129.550 0.000, 41519…\n\n\n\n\n2\n\n\n0295ED18-F2F3-5C37-E063-AAEFA00A445E\n\n\nPlaying Field\n\n\nPOLYGON Z ((415690.440 439919.060 0.000, 41568…\n\n\n\n\n3\n\n\n0295ECC9-0C25-5C37-E063-AAEFA00A445E\n\n\nPlay Space\n\n\nPOLYGON Z ((416739.570 439592.210 0.000, 41675…\n\n\n\n\n4\n\n\n0295ECC7-FBFC-5C37-E063-AAEFA00A445E\n\n\nPlay Space\n\n\nPOLYGON Z ((415991.960 438808.610 0.000, 41598…\n\n\n\n\n\n\nNearest greenspace to each OA (unique list)\nunique_nearest_greenspace.head()\n\n\n\n\n\n\nOA21CD\n\n\nGreenspace_ID\n\n\nType\n\n\nDistance\n\n\n\n\n\n\n0\n\n\nE00053353\n\n\n0295ED18-D538-5C37-E063-AAEFA00A445E\n\n\nPlaying Field\n\n\n1000.455806\n\n\n\n\n1\n\n\nE00053354\n\n\n0295ED18-E5D4-5C37-E063-AAEFA00A445E\n\n\nPlaying Field\n\n\n771.145845\n\n\n\n\n2\n\n\nE00053355\n\n\n0295ED18-F2F3-5C37-E063-AAEFA00A445E\n\n\nPlaying Field\n\n\n418.265674\n\n\n\n\n3\n\n\nE00053356\n\n\n0295ECC9-0C25-5C37-E063-AAEFA00A445E\n\n\nPlay Space\n\n\n194.292561\n\n\n\n\n4\n\n\nE00053357\n\n\n0295ED18-E5D4-5C37-E063-AAEFA00A445E\n\n\nPlaying Field\n\n\n432.352481\n\n\n\n\n\n\nIndependent variables dataframe\nInd_vars.head()\n\n\n\n\n\n\nOutput Areas Code\n\n\nHHD_dep_education_PC\n\n\nHHD_dep_employment_PC\n\n\nHHD_dep_health_PC\n\n\nHHD_dep_housing_PC\n\n\n\n\n\n\n0\n\n\nE00053353\n\n\n28.3\n\n\n11.5\n\n\n33.6\n\n\n7.8\n\n\n\n\n1\n\n\nE00053354\n\n\n17.3\n\n\n6.8\n\n\n30.8\n\n\n5.3\n\n\n\n\n2\n\n\nE00053355\n\n\n11.2\n\n\n4.9\n\n\n22.0\n\n\n1.6\n\n\n\n\n3\n\n\nE00053356\n\n\n21.5\n\n\n4.2\n\n\n34.0\n\n\n4.9\n\n\n\n\n4\n\n\nE00053357\n\n\n14.2\n\n\n3.5\n\n\n21.3\n\n\n0.7"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Hello, I’m Jess!",
    "section": "",
    "text": "🔍 I’m an insight and analytics specialist with nearly a decade of experience across lots of different roles and sectors.\n🧩 I mainly work with geospatial data, and solve problems to help people make better decisions.\n\n\n\n⏯️ In September 2023 I decided to take a career break and explore new opportunities.\n👩‍🎓 I’m currently pursuing an MSc in Urban Data Science & Analytics at the University of Leeds, whilst working as a Freelance Consultant.\n💡 My studies have deepened my analytical and research capabilities whilst giving me the time and space to delve into all things geospatial!\n\n\n\n\n\n\n🌱 My aim is to move into sustainable development and use my skills and expertise to help solve problems in our cities.\n\n\nYou can find me on GitHub and LinkedIn here:"
  },
  {
    "objectID": "index.html#background",
    "href": "index.html#background",
    "title": "Hello, I’m Jess!",
    "section": "",
    "text": "🔍 I’m an insight and analytics specialist with nearly a decade of experience across lots of different roles and sectors.\n🧩 I mainly work with geospatial data, and solve problems to help people make better decisions."
  },
  {
    "objectID": "index.html#what-im-currently-doing",
    "href": "index.html#what-im-currently-doing",
    "title": "Hello, I’m Jess!",
    "section": "",
    "text": "⏯️ In September 2023 I decided to take a career break and explore new opportunities.\n👩‍🎓 I’m currently pursuing an MSc in Urban Data Science & Analytics at the University of Leeds, whilst working as a Freelance Consultant.\n💡 My studies have deepened my analytical and research capabilities whilst giving me the time and space to delve into all things geospatial!"
  },
  {
    "objectID": "index.html#what-do-i-want-to-do",
    "href": "index.html#what-do-i-want-to-do",
    "title": "Hello, I’m Jess!",
    "section": "",
    "text": "🌱 My aim is to move into sustainable development and use my skills and expertise to help solve problems in our cities.\n\n\nYou can find me on GitHub and LinkedIn here:"
  },
  {
    "objectID": "projects/access-to-greenspace/index.html",
    "href": "projects/access-to-greenspace/index.html",
    "title": "A spatial analysis of access to greenspace and deprivation in Bradford",
    "section": "",
    "text": "The full study can be found here or you can go back to see more projects.\n\n\nSummary\nGreenspace is an important part of urban life, with benefits for humans and the environment. Analysing Bradford local authority district, this study builds on the growing literature addressing the challenge of greenspace accessibility by focusing on each deprivation dimension recorded in the 2021 Census.\nBradford is a large district, with a young and diverse population. Health and housing inequalities are known issues, whilst greenspace varies across the district in terms of quantity, type and size. The council has embedded greenspace within its long-term strategy, aiming to make sure they are safe, inclusive and that investment is delivered where it is needed most.\nThis study brings together 2021 Census data on each dimension of deprivation (education, employment, health and housing) by Output Area (OA), and combines this with Ordanance Survey data on greenspace in the district. After some initial data wrangling, a series of statistics and visualisations help to illustrate the data that will be used in the subsequent modelling.\nFirst, using an OLS regression model, the variables are analysed to understand the significance of their relationship with access to greenspace - defined here as the distance to the nearest greenspace. This shows that employment deprivation is not statistically significant, and is subsequently removed.\nFurther investigation using a range of techniques, including Moran’s I and LISA clusters, shows that there is spatial autocorrelation present and so a Geographically Weighted Regression model (GWR) is built. The results of this reject the null hypothesis that there is not a statistically significant relationship present between dimensions of deprivation and access to greenspace.\nThis study therefore argues that any future policy interventions need to take into consideration deprivation and its relationship with greenspace.\n\n\n\nDisclaimer\nThis notebook was submitted for assessment within the GEOG5402 Data Science for Urban Systems module as part of the MSc Urban Data Science & Analytics programme at the University of Leeds.\nThe content of this notebook is intended for educational and general use purposes only.\nAny use of the materials should adhere to the guidelines and policies of your educational institution.\nThe author does not take any responsibility for how the materials in this repository are utilised."
  },
  {
    "objectID": "projects/20min_neighbourhoods/index.html",
    "href": "projects/20min_neighbourhoods/index.html",
    "title": "20-minute neighbourhoods: understanding housing density in Bradford",
    "section": "",
    "text": "Go back to the projects page."
  },
  {
    "objectID": "projects/20min_neighbourhoods/index.html#disclaimer",
    "href": "projects/20min_neighbourhoods/index.html#disclaimer",
    "title": "20-minute neighbourhoods: understanding housing density in Bradford",
    "section": "Disclaimer",
    "text": "Disclaimer\nThis analysis was part of a larger piece of work on 20-minute neighbourhoods as part of the GEOG5403 Creative Coding for Urban Problems module on the MSc Urban Data Science & Analytics programme at the University of Leeds.\nThe content of this notebook is intended for educational and general use purposes only.\nAny use of the materials should adhere to the guidelines and policies of your educational institution.\nThe author does not take any responsibility for how the materials in this repository are utilised."
  },
  {
    "objectID": "projects/20min_neighbourhoods/index.html#summary",
    "href": "projects/20min_neighbourhoods/index.html#summary",
    "title": "20-minute neighbourhoods: understanding housing density in Bradford",
    "section": "Summary",
    "text": "Summary\nIn a 20-minute neighbourhood, residents can walk, wheel or cycle to key services and amenities within a return 20-minute journey. The concept is not new, but interest grew as COVID-19 lockdowns highlighted the liveability of local areas. One way to identify a 20-minute neighbourhood is to draw a suitable walk-time around an area, analyse what services exist, and then “fill in the gaps”. However, this approach does not consider wants and needs of local communities and how this can differ. The Town and Country Planning Association has built a comprehensive guide arguing most 20-minute neighbourhoods will include many similar features, like schools, greenspaces, and health facilities.\nThis analysis looks at potential 20-minute neighbourhoods in Bradford, with the aim of understanding housing density to determine where people live and therefore which areas might have the most households benefit from investment in the 20-minute neighbourhood concept.\nUsing population-weighted centroids (PWCs) for each output area (OA), an 800m Euclidian buffer is drawn around this point to reflect 20-minute neighbourhoods, as shown in Figure 1. OAs are the smallest census geography, with PWCs reflecting where people live compared to the centre-point of the polygon. However, OA polygons are used to visualise results for clarity, avoiding buffer overlap.\nFigure 1: Defining neighbourhoods\n\nFor housing, OA populations could have been used but would include everyone in any OA with a PWC inside the neighbourhood buffer, rather than the households within the neighbourhood itself. Instead, domestic EPC ratings since 2008 are used to find residential properties, spatially matching these to the buffers. Over 157,000 homes were identified, roughly 75% coverage given 210,000 households in Bradford.\nClear spatial patterns emerge of where housing is most dense, with these areas potentially being good candidates to investigate further in terms how well they are or are not setup for 20-minute living. There are limitations to this methodology, including:\n\n800m Euclidian buffers not reflecting actual path networks - a network analysis may be more useful in reflecting true 20-minute neighbourhoods\nPopulation is not included, and so the actual number of people living in these neighbourhoods is not calculated\n\nFigure 2: Housing density choropleth"
  },
  {
    "objectID": "projects/20min_neighbourhoods/index.html#import-and-setup-geographies",
    "href": "projects/20min_neighbourhoods/index.html#import-and-setup-geographies",
    "title": "20-minute neighbourhoods: understanding housing density in Bradford",
    "section": "Import and setup geographies",
    "text": "Import and setup geographies\nLinks to the raw data can be found in each code cell. OA data needs to be reduced to just Bradford, and then dataframes tidied ready for use.\n# Import required libraries\nimport pandas as pd\nimport geopandas as gpd\n# OA polygons (BFC: Full resolution - clipped to the coastline (Mean High Water mark))\n# https://geoportal.statistics.gov.uk/datasets/ons::output-areas-2021-boundaries-ew-bfc/about\nOA_polygons = gpd.read_file('Data/Output_Areas_2021_Polygon/OA_2021_EW_BFC_V8.shp')\n\n# OA Population-weighted centroids\n# https://geoportal.statistics.gov.uk/datasets/ons::output-areas-december-2021-pwc-v3/about\nOA_PWC = gpd.read_file('Data/Output_Areas_2021_Centroid/PopCentroids_EW_2021_V3.shp')\n# OA to LSOA to MSOA to LAD lookup\n# https://geoportal.statistics.gov.uk/datasets/output-area-to-lower-layer-super-output-area-to-middle-layer-super-output-area-to-local-authority-district-december-2021-lookup-in-england-and-wales-v2-1/about\nOA_lookup = pd.read_csv('Data/Output_Area_Lookup_in_England_and_Wales_v3.csv')\n# Remove unnecessary columns from OA lookup, keeping only required columns in new dataframe\nOA_lookup_trim = OA_lookup[['OA21CD','LAD22CD','LAD22NM']]\n# Filter OA lookup to just Bradford\nBradford_OA_lookup = OA_lookup_trim[OA_lookup_trim['LAD22NM'] == 'Bradford']\n# Check how many OAs are left - this number is an important reference point for further data wrangling tasks\nlen(Bradford_OA_lookup)\n1575\n# Merge Bradford_OA_lookup with OA polygons to reduce the geodataframe containing the polygons to only Bradford OAs\nBradford_OA_polygons = OA_polygons.merge(Bradford_OA_lookup, how='right', on='OA21CD')\n# Check number of OAs is same as in the reduced Bradford OA lookup dataframe\nprint(len(Bradford_OA_polygons))\nprint(len(Bradford_OA_polygons) == len(Bradford_OA_lookup))\n1575\nTrue\n# Merge Bradford_OA_lookup with OA PWCs to reduce the geodataframe containing the PWCs to only Bradford OAs\nBradford_OA_PWC = OA_PWC.merge(Bradford_OA_lookup, how='right', on='OA21CD')\n# Check number of OAs is same as in the reduced Bradford OA lookup dataframe\nprint(len(Bradford_OA_PWC))\nprint(len(Bradford_OA_PWC) == len(Bradford_OA_lookup))\n1575\nTrue\n# Drop unrequired columns\nBradford_OA_polygons_tidy = Bradford_OA_polygons.drop([\n    'LSOA21CD','LSOA21NM','LSOA21NMW','BNG_E','BNG_N','LAT','LONG','GlobalID','LAD22CD','LAD22NM'], axis=1)\nBradford_OA_PWC_tidy = Bradford_OA_PWC.drop(['GlobalID','LAD22CD','LAD22NM'], axis=1)\n\n# Rename geometry columns and set geometry\nBradford_OA_polygons = Bradford_OA_polygons_tidy.set_geometry('geometry').rename_geometry('Polygon')\nBradford_OA_PWC = Bradford_OA_PWC_tidy.set_geometry('geometry').rename_geometry('PWC')"
  },
  {
    "objectID": "projects/20min_neighbourhoods/index.html#build-20-minute-neighbourhoods-using-800m-euclidian-buffer",
    "href": "projects/20min_neighbourhoods/index.html#build-20-minute-neighbourhoods-using-800m-euclidian-buffer",
    "title": "20-minute neighbourhoods: understanding housing density in Bradford",
    "section": "Build 20-minute neighbourhoods using 800m Euclidian buffer",
    "text": "Build 20-minute neighbourhoods using 800m Euclidian buffer\nAs highlighted in the summary, an 800m Euclidian buffer around each PWC will be used to act as the 20-minute neighbourhoods.\n# Import required libraries\nfrom shapely.geometry import Point\n# Function to create a buffer around a point with a given radius\ndef create_buffer(point, radius):\n    return point.buffer(radius)\n\n# Define the radius of the circle (in meters)\nradius_meters = 800\n\n# Create a new column in the GeoDataFrame to store the buffer polygons\nBradford_OA_PWC['buffer'] = Bradford_OA_PWC['PWC'].apply(lambda point: create_buffer(point, radius_meters))\n\n# Now, 'buffer' column contains the buffer polygons around each point with a radius of 800 meters\n# Set the 'buffer' to be the geometry\nBradford_OA_PWC.set_geometry('buffer', inplace=True)"
  },
  {
    "objectID": "projects/20min_neighbourhoods/index.html#get-locations-of-residential-properties",
    "href": "projects/20min_neighbourhoods/index.html#get-locations-of-residential-properties",
    "title": "20-minute neighbourhoods: understanding housing density in Bradford",
    "section": "Get locations of residential properties",
    "text": "Get locations of residential properties\nTo obtain the location of the houses, EPC ratings since 2008 is combined with UPRN point data. Properties might have multiple EPC ratings, and so the data must be ordered newest to oldest, and duplicates removed keeping only the latest rating. The EPC ratings can then be merged with the UPRN data to obtain the point of each property, setting this as the geometry for further analysis.\n\nEPC ratings\n# EPC Ratings\n# https://epc.opendatacommunities.org/\nEPC_Ratings = pd.read_csv('Data/Housing/domestic-E08000032-Bradford/certificates.csv')\n# Step 1: Sort by date column in descending order\nEPC_sorted = EPC_Ratings.sort_values(by='LODGEMENT_DATETIME', ascending=False)\n\n# Step 2: Remove duplicates keeping the first instance based on 'UPRN' column\nEPC_unique = EPC_sorted.drop_duplicates(subset='UPRN', keep='first')\n# Check how properties are in the dataframe\nlen(EPC_unique)\n157255\n\n\nUPRNs\n# UPRNs\n# https://osdatahub.os.uk/downloads/open/OpenUPRN\nUPRNs = pd.read_csv('Data/Housing/osopenuprn_202404_csv/osopenuprn_202404.csv')\nUPRNs.head()\n\n\n\n\n\n\nUPRN\n\n\nX_COORDINATE\n\n\nY_COORDINATE\n\n\nLATITUDE\n\n\nLONGITUDE\n\n\n\n\n\n\n0\n\n\n1\n\n\n358260.66\n\n\n172796.5\n\n\n51.452601\n\n\n-2.602075\n\n\n\n\n1\n\n\n26\n\n\n352967.00\n\n\n181077.0\n\n\n51.526633\n\n\n-2.679361\n\n\n\n\n2\n\n\n27\n\n\n352967.00\n\n\n181077.0\n\n\n51.526633\n\n\n-2.679361\n\n\n\n\n3\n\n\n30\n\n\n354800.00\n\n\n180469.0\n\n\n51.521317\n\n\n-2.652862\n\n\n\n\n4\n\n\n31\n\n\n354796.00\n\n\n180460.0\n\n\n51.521236\n\n\n-2.652918\n\n\n\n\n\n\nMerge the two dataframes\n# Merge EPC Ratings and UPRNs\nMatched = EPC_unique.merge(UPRNs, how='inner', on='UPRN')\n# Keep only columns required for analysis\nMatched_trim = Matched[['UPRN','LATITUDE','LONGITUDE']].copy()\n# Convert the Latitude and Longitude columns into Point objects\ngeometry = [Point(xy) for xy in zip(Matched_trim['LONGITUDE'], Matched_trim['LATITUDE'])]\n\n# Convert to a GeoDataFrame\nMatched_final = gpd.GeoDataFrame(Matched_trim, geometry=geometry).set_crs('epsg:4326')\n# Change crs to match other data\nMatched_final = Matched_final.to_crs(27700)\nMatched_final.head()\n\n\n\n\n\n\nUPRN\n\n\nLATITUDE\n\n\nLONGITUDE\n\n\ngeometry\n\n\n\n\n\n\n0\n\n\n1.000512e+11\n\n\n53.763624\n\n\n-1.768966\n\n\nPOINT (415327.798 429721.658)\n\n\n\n\n1\n\n\n1.000512e+11\n\n\n53.801380\n\n\n-1.826545\n\n\nPOINT (411521.815 433911.593)\n\n\n\n\n2\n\n\n1.000512e+11\n\n\n53.802175\n\n\n-1.829245\n\n\nPOINT (411343.819 433999.592)\n\n\n\n\n3\n\n\n1.000512e+11\n\n\n53.781098\n\n\n-1.816461\n\n\nPOINT (412191.822 431656.624)\n\n\n\n\n4\n\n\n1.009167e+10\n\n\n53.823379\n\n\n-1.860954\n\n\nPOINT (409250.808 436354.194)\n\n\n\n\nlen(Matched_final)\n157252\n# Check length of the final dataframe is the same as the unique properties that were found\nlen(Matched_final) == len(EPC_unique)\nFalse\n# Check the difference to see how many properties in the EPC ratings did NOT match a UPRN (this could be because they were missing or entered incorrectly)\nlen(Matched_final) - len(EPC_unique)\n-3"
  },
  {
    "objectID": "projects/20min_neighbourhoods/index.html#mapping-properties",
    "href": "projects/20min_neighbourhoods/index.html#mapping-properties",
    "title": "20-minute neighbourhoods: understanding housing density in Bradford",
    "section": "Mapping properties",
    "text": "Mapping properties\nA quick map illustrates where the properties are located.\n# Import required libraries\nimport contextily as cx\nimport matplotlib.pyplot as plt\n# Plotting the GeoDataFrame with basemap\nax = Matched_final.plot(figsize=(10, 6), markersize=10, color='red')\n\n# Add basemap using contextily\ncx.add_basemap(ax, zoom=12, crs=Matched_final.crs.to_string())\n\n# Set title\nplt.title('Properties identified by EPC data')\n\n# Turn off the axes\nax.axis('off')\n\nplt.show()"
  },
  {
    "objectID": "projects/20min_neighbourhoods/index.html#spatially-match-properties-to-neighbourhood-buffers",
    "href": "projects/20min_neighbourhoods/index.html#spatially-match-properties-to-neighbourhood-buffers",
    "title": "20-minute neighbourhoods: understanding housing density in Bradford",
    "section": "Spatially match properties to neighbourhood buffers",
    "text": "Spatially match properties to neighbourhood buffers\nNext the properties can be spatially matched to the neighbourhood buffers to determine how many properties are in each neighbourhood. It is important to reiterate that this EPC rating methodology has roughly 75% coverage of all households, and does not include actual population counts which could vary depending on size of property.\n# Make a copy of the dataframe containing the buffers with only the required columns\nBradford_PWC_buffers = Bradford_OA_PWC[['OA21CD','buffer']].copy()\n# Spatially match the properties to the buffers\njoined = Bradford_PWC_buffers.sjoin(Matched_final, how=\"left\", predicate=\"intersects\")\n# Visual check of the dataframe - there are duplicated rows due to the overlap of some neighbourhood buffer zones\njoined\n\n\n\n\n\n\nOA21CD\n\n\nbuffer\n\n\nindex_right\n\n\nUPRN\n\n\nLATITUDE\n\n\nLONGITUDE\n\n\n\n\n\n\n0\n\n\nE00053353\n\n\nPOLYGON ((414438.052 439495.615, 414434.200 43…\n\n\n35797\n\n\n1.000519e+11\n\n\n53.847941\n\n\n-1.784022\n\n\n\n\n0\n\n\nE00053353\n\n\nPOLYGON ((414438.052 439495.615, 414434.200 43…\n\n\n63910\n\n\n2.000047e+11\n\n\n53.848048\n\n\n-1.783626\n\n\n\n\n0\n\n\nE00053353\n\n\nPOLYGON ((414438.052 439495.615, 414434.200 43…\n\n\n58772\n\n\n1.000513e+11\n\n\n53.845011\n\n\n-1.789341\n\n\n\n\n0\n\n\nE00053353\n\n\nPOLYGON ((414438.052 439495.615, 414434.200 43…\n\n\n11651\n\n\n1.000513e+11\n\n\n53.845155\n\n\n-1.789355\n\n\n\n\n0\n\n\nE00053353\n\n\nPOLYGON ((414438.052 439495.615, 414434.200 43…\n\n\n157213\n\n\n1.000513e+11\n\n\n53.845362\n\n\n-1.789309\n\n\n\n\n…\n\n\n…\n\n\n…\n\n\n…\n\n\n…\n\n\n…\n\n\n…\n\n\n\n\n1574\n\n\nE00177810\n\n\nPOLYGON ((410759.264 435503.188, 410755.412 43…\n\n\n119131\n\n\n1.009098e+10\n\n\n53.821992\n\n\n-1.855611\n\n\n\n\n1574\n\n\nE00177810\n\n\nPOLYGON ((410759.264 435503.188, 410755.412 43…\n\n\n50374\n\n\n1.000519e+11\n\n\n53.816365\n\n\n-1.848081\n\n\n\n\n1574\n\n\nE00177810\n\n\nPOLYGON ((410759.264 435503.188, 410755.412 43…\n\n\n31264\n\n\n1.000519e+11\n\n\n53.816429\n\n\n-1.848704\n\n\n\n\n1574\n\n\nE00177810\n\n\nPOLYGON ((410759.264 435503.188, 410755.412 43…\n\n\n100903\n\n\n1.000519e+11\n\n\n53.816465\n\n\n-1.848628\n\n\n\n\n1574\n\n\nE00177810\n\n\nPOLYGON ((410759.264 435503.188, 410755.412 43…\n\n\n154452\n\n\n1.000233e+10\n\n\n53.816501\n\n\n-1.848430\n\n\n\n\n\n3703801 rows × 6 columns\n\n# Group by OA21CD and count the unique UPRNs\nagg_df = joined.groupby('OA21CD')['UPRN'].nunique().reset_index()\n\n# Rename the column to indicate the count of UPRNs\nagg_df.rename(columns={'UPRN': 'UPRN_Count'}, inplace=True)\n# Visual check\nagg_df\n\n\n\n\n\n\nOA21CD\n\n\nUPRN_Count\n\n\n\n\n\n\n0\n\n\nE00053353\n\n\n214\n\n\n\n\n1\n\n\nE00053354\n\n\n1148\n\n\n\n\n2\n\n\nE00053355\n\n\n1405\n\n\n\n\n3\n\n\nE00053356\n\n\n965\n\n\n\n\n4\n\n\nE00053357\n\n\n1441\n\n\n\n\n…\n\n\n…\n\n\n…\n\n\n\n\n1570\n\n\nE00177867\n\n\n3780\n\n\n\n\n1571\n\n\nE00177868\n\n\n2910\n\n\n\n\n1572\n\n\nE00177869\n\n\n5265\n\n\n\n\n1573\n\n\nE00177870\n\n\n2096\n\n\n\n\n1574\n\n\nE00177871\n\n\n5169\n\n\n\n\n\n1575 rows × 2 columns"
  },
  {
    "objectID": "projects/20min_neighbourhoods/index.html#add-oa-polygons-back-in-and-map-choropleth",
    "href": "projects/20min_neighbourhoods/index.html#add-oa-polygons-back-in-and-map-choropleth",
    "title": "20-minute neighbourhoods: understanding housing density in Bradford",
    "section": "Add OA polygons back in and map choropleth",
    "text": "Add OA polygons back in and map choropleth\nThe results are easier to visualise using the OA polygons to remove any buffer overlap. Results show clear areas of higher density neighbourhoods, particuarly around large town in the district and the city centre.\n# Merge dataframes to add polygons back in\nfor_choropleth = pd.merge(Bradford_OA_polygons, agg_df, on='OA21CD', how='inner')\n# Visual check\nfor_choropleth\n\n\n\n\n\n\nOA21CD\n\n\nPolygon\n\n\nUPRN_Count\n\n\n\n\n\n\n0\n\n\nE00053353\n\n\nPOLYGON ((415817.093 440872.597, 415821.094 44…\n\n\n214\n\n\n\n\n1\n\n\nE00053354\n\n\nPOLYGON ((415078.000 439967.001, 415058.323 43…\n\n\n1148\n\n\n\n\n2\n\n\nE00053355\n\n\nPOLYGON ((416252.367 439816.041, 416253.270 43…\n\n\n1405\n\n\n\n\n3\n\n\nE00053356\n\n\nPOLYGON ((416668.000 439392.028, 416667.653 43…\n\n\n965\n\n\n\n\n4\n\n\nE00053357\n\n\nPOLYGON ((415143.909 439176.235, 415143.000 43…\n\n\n1441\n\n\n\n\n…\n\n\n…\n\n\n…\n\n\n…\n\n\n\n\n1570\n\n\nE00177806\n\n\nPOLYGON ((404057.041 446035.505, 404056.753 44…\n\n\n2069\n\n\n\n\n1571\n\n\nE00177807\n\n\nPOLYGON ((418884.032 438045.945, 418866.019 43…\n\n\n1790\n\n\n\n\n1572\n\n\nE00177808\n\n\nPOLYGON ((414782.332 437464.729, 414782.519 43…\n\n\n3114\n\n\n\n\n1573\n\n\nE00177809\n\n\nPOLYGON ((402252.845 444586.747, 402253.000 44…\n\n\n434\n\n\n\n\n1574\n\n\nE00177810\n\n\nPOLYGON ((410031.688 435427.501, 410026.053 43…\n\n\n489\n\n\n\n\n\n1575 rows × 3 columns\n\nfig, ax = plt.subplots(figsize=(10, 6))\n\n# Plot the choropleth map\nfor_choropleth.plot(column='UPRN_Count', cmap='inferno', legend=True, ax=ax, alpha=0.8)\n\n# Add basemap using contextily\ncx.add_basemap(ax, crs=for_choropleth.crs.to_string(), zoom=10)  # Set the coordinate reference system (CRS)\n\n# Set title\nplt.title('Housing density choropleth')\n\n# Turn off the axes\nax.axis('off')\n\n# Show the plot\nplt.show()\n\nplt.figure(figsize=(8, 6))\n\n# Create histogram\nplt.hist(for_choropleth['UPRN_Count'], bins=15, color='skyblue', edgecolor='black')  # Adjust the number of bins as needed\n\n# Add labels and title\nplt.xlabel('UPRN Count')\nplt.ylabel('Frequency')\nplt.title('Histogram of Housing Density')\n\n# Show the plot\nplt.show()"
  },
  {
    "objectID": "projects/index.html#minute-neighbourhoods",
    "href": "projects/index.html#minute-neighbourhoods",
    "title": "Projects",
    "section": "",
    "text": "20-minute neighbourhoods: understanding housing density in Bradford\nDefining neighbourhoods using 800m buffers and identfying properties within these to understand housing density in the Bradford local authority district."
  },
  {
    "objectID": "projects/EV_charging/index.html",
    "href": "projects/EV_charging/index.html",
    "title": "Analysis of public EV chargepoint provision",
    "section": "",
    "text": "Go back to the projects page."
  },
  {
    "objectID": "projects/EV_charging/index.html#summary",
    "href": "projects/EV_charging/index.html#summary",
    "title": "Analysis of public EV chargepoint provision",
    "section": "Summary",
    "text": "Summary\nThis analysis uses public EV chargepoint and plug-in vehicle (PiV) registration data to understand, by local authority district (LAD), which areas have the best and worst EV infrastructure.\n\nWhat data is included?\nFirst, four features are calculated to allow for initial exploration of the data. Details of these are as follows:\nCount of PiVs\nSource: DVLA and DfT dataset\n\nA simple count of registered PiVs by LAD is calculated.\nThis includes all PiVs with Private keepership. This excludes PiVs where the keepership is Company to avoid skewing the analysis with large quantities of fleet vehicles registered to a single address.\nWhere data has been suppressed in the raw file, the [c] value has been replaced with NaN\n\nCount of EV chargepoints\nSource: Public EV chargepoint registry\n\nA simple count of public EV chargepoints by LAD is calculated.\nThere has been no cleaning of this data beyond converting the latitude and longitude into a point. This is a public dataset and further accuracy has not been verified.\n\nRatio of PiVs to EV chargepoints\nSource: Author’s calculations\n\nTaking the count of PiVs in each LAD and dividing this by the count of public EV chargepoints in each LAD.\nA higher ratio means there are more PiVs to each chargepoint and therefore public infrastructure may be lacking.\n\nAverage distance to nearest EV chargepoint\nSource: Geographies from ONS via Open Geography Portal\n\nUsing the population-weighted centroids (PWCs) for each output area (OA) within an LAD, the nearest public EV chargepoint is calculated. OA is the smallest census geography and contains between 40 and 250 households.\nThen this is aggregated to LAD level by taking the average distance of all OAs within the LAD to obtain the average distance to the nearest public EV chargepoint by LAD.\n\n\n\nWhat do these features show?\nThe following choropleths map these four features. The absolute number of PiVs varies across England and Wales, with LADs across Wales, Lincolnshire and Cumbria having particularly low numbers. By contrast, the highest number of public EV chargepoints by LAD is concentrated in Greater London, with hotspots dotted elsewhere across the country. If we instead look at this as the ratio of PiVs to chargepoints, we again see the patterns change. Hotspots around the southeast of England highlight that there is potentially infrastructure lacking in these areas compared to PiV numbers. Lastly, we can see the average distance to the nearest chargepoint is perhaps unsurprisingly highest in rural areas, particularly in the north of England and Wales.\n\nCode to create an interactive version of this map can be found below.\n\n\nWhat is the relationship between the features?\nThe correlation matrix heatmap below shows the relationship between each feature. There is some positive correlation between chargepoints and PiVs, which is good as this suggests that where there are more PiVs there tend to be more chargepoints. There is also some positive correlation between average distance to the nearest chargepoint and the ratio of PiVs to chargepoints. This means that if the distance is greater, there are also more PiVs to chargepoints perhaps suggesting these areas lack infrastructure.\n\nThe following scatter plots illustrate this further, adding a regression line. We can see that there are outliers across both, with a cluster of LADs on the lower end.\n\n\n\nSo what does this mean for public EV infrastructure?\nTaking the ratio of PiVs to public EV chargepoints and the average distance to the nearest chargepoint, a simple ranking is created combining the two. Areas with a high ratio of PiVs to chargepoints and a large distance to the nearest chargepoint could be lacking in public EV infrastructure.\nTop 10 The best performing LADs were all in London, suggesting infrastructure in the capital is well established.\n\n\n\n\n\n\n\nLAD22CD\n\n\nLAD22NM\n\n\ncombined_rank\n\n\n\n\n\n\n288\n\n\nE09000013\n\n\nHammersmith and Fulham\n\n\n1.0\n\n\n\n\n308\n\n\nE09000033\n\n\nWestminster\n\n\n2.0\n\n\n\n\n303\n\n\nE09000028\n\n\nSouthwark\n\n\n3.0\n\n\n\n\n276\n\n\nE09000001\n\n\nCity of London\n\n\n4.0\n\n\n\n\n295\n\n\nE09000020\n\n\nKensington and Chelsea\n\n\n4.0\n\n\n\n\n299\n\n\nE09000024\n\n\nMerton\n\n\n6.0\n\n\n\n\n307\n\n\nE09000032\n\n\nWandsworth\n\n\n6.0\n\n\n\n\n282\n\n\nE09000007\n\n\nCamden\n\n\n8.0\n\n\n\n\n287\n\n\nE09000012\n\n\nHackney\n\n\n9.0\n\n\n\n\n294\n\n\nE09000019\n\n\nIslington\n\n\n9.0\n\n\n\n\nBottom 10 The worst performing LADs are more dispersed. Most are more rural areas, or include only one or two larger towns.\n\n\n\n\n\n\n\nLAD22CD\n\n\nLAD22NM\n\n\ncombined_rank\n\n\n\n\n\n\n99\n\n\nE07000074\n\n\nMaldon\n\n\n330.0\n\n\n\n\n89\n\n\nE07000064\n\n\nRother\n\n\n329.0\n\n\n\n\n235\n\n\nE07000242\n\n\nEast Hertfordshire\n\n\n328.0\n\n\n\n\n16\n\n\nE06000017\n\n\nRutland\n\n\n327.0\n\n\n\n\n100\n\n\nE07000075\n\n\nRochford\n\n\n326.0\n\n\n\n\n85\n\n\nE07000047\n\n\nWest Devon\n\n\n325.0\n\n\n\n\n177\n\n\nE07000169\n\n\nSelby\n\n\n324.0\n\n\n\n\n160\n\n\nE07000139\n\n\nNorth Kesteven\n\n\n323.0\n\n\n\n\n82\n\n\nE07000044\n\n\nSouth Hams\n\n\n321.0\n\n\n\n\n199\n\n\nE07000198\n\n\nStaffordshire Moorlands\n\n\n321.0\n\n\n\n\nThis information could be used by both public and private organisations alike to understand where public EV infrastructure is lacking and therefore incentivise installation of new infrastructure.\n\n\nConsiderations and future work\nThis analysis has demonstrated that current public EV structure may be lacking in some areas based on the current number of PiVs. This does not include:\n\nA temporal view of how PiV ownership has changed over time - there may be areas experiencing higher growth, with could arguably justify increased investment over other areas.\nEV infrastructure on private property (e.g. at home) - areas with a higher number of terraced houses or flats may require more public infrastructure as it can often be more challenging to install at these types of properties. Areas with higher rates of rented accomodation over private ownership could also face challenges as it would be the responsibility of the landlord to install EV chargepoints. Again, these areas may require more public infrastructure and so overlaying this data could be an interesting investigation to see how it affects the ranking.\n\nThe public EV chargepoint registry has also not been quality checked. Further investigation into this dataset and wrangling/cleaning as appropriate may impact results."
  },
  {
    "objectID": "projects/EV_charging/index.html#coding",
    "href": "projects/EV_charging/index.html#coding",
    "title": "Analysis of public EV chargepoint provision",
    "section": "Coding",
    "text": "Coding"
  },
  {
    "objectID": "projects/EV_charging/index.html#import-libraries",
    "href": "projects/EV_charging/index.html#import-libraries",
    "title": "Analysis of public EV chargepoint provision",
    "section": "Import libraries",
    "text": "Import libraries\nimport pandas as pd\nimport geopandas as gpd\nfrom shapely.geometry import Point\nimport numpy as np\nimport seaborn as sns\nfrom scipy.stats import zscore\nimport matplotlib.pyplot as plt\nimport contextily as ctx\nimport folium"
  },
  {
    "objectID": "projects/EV_charging/index.html#data-wrangling",
    "href": "projects/EV_charging/index.html#data-wrangling",
    "title": "Analysis of public EV chargepoint provision",
    "section": "Data wrangling",
    "text": "Data wrangling\n\nEV Chargepoint data\n\nPublic EV chargepoint registry, available from: https://www.gov.uk/guidance/find-and-use-data-on-public-electric-vehicle-chargepoints\n\n# Import csv\nchargepoints_RAW = pd.read_csv('Data/national-charge-point-registry_230524.csv', low_memory=False)\n# Reduce to just required columns\nchargepoints_TRIM = chargepoints_RAW[['chargeDeviceID','reference','name','latitude','longitude']]\n# Check length of df\nlen(chargepoints_TRIM)\n40580\n# Visual check\nchargepoints_TRIM.head()\n\n\n\n\n\n\nchargeDeviceID\n\n\nreference\n\n\nname\n\n\nlatitude\n\n\nlongitude\n\n\n\n\n\n\n0\n\n\nb86a77a42bb68c81946ec50cfc95e89d\n\n\n11172306P\n\n\nNetwork Rail Westwood Centre 1\n\n\n52.386590\n\n\n-1.587384\n\n\n\n\n1\n\n\ndc1c347d471f68e41ad2a9a1145941d6\n\n\nAPT-0296-0015/13P\n\n\nBrindley Drive Car Park Birmingham - 70524\n\n\n52.480918\n\n\n-1.907710\n\n\n\n\n2\n\n\n7d545ad9367ccb8a80c94a953314ae71\n\n\nCM123\n\n\nRenault Liverpool\n\n\n53.383579\n\n\n-2.977230\n\n\n\n\n3\n\n\n68c7fca1e3bba5e49ec90847dcdd456b\n\n\nCM164\n\n\nNCP Portman Square\n\n\n51.516201\n\n\n-0.157996\n\n\n\n\n4\n\n\nac7a21c48f5833b33a5b606b2089e6a9\n\n\nCM167\n\n\nNCP Prince Street Car Park\n\n\n51.450340\n\n\n-2.596704\n\n\n\n\n# Geocode locations using lat and long, set crs to 4326 and then convert to 27700 (British National Grid), drop lat and long\nchargepoints_gdf = gpd.GeoDataFrame(chargepoints_TRIM,\n                                    geometry=gpd.points_from_xy(chargepoints_TRIM.longitude, chargepoints_TRIM.latitude)\n                                   ).set_crs(epsg=4326, inplace=True).to_crs(epsg=27700).drop(columns=['latitude','longitude'])\n# Visual check\nchargepoints_gdf.head()\n\n\n\n\n\n\nchargeDeviceID\n\n\nreference\n\n\nname\n\n\ngeometry\n\n\n\n\n\n\n0\n\n\nb86a77a42bb68c81946ec50cfc95e89d\n\n\n11172306P\n\n\nNetwork Rail Westwood Centre 1\n\n\nPOINT (428179.411 276586.797)\n\n\n\n\n1\n\n\ndc1c347d471f68e41ad2a9a1145941d6\n\n\nAPT-0296-0015/13P\n\n\nBrindley Drive Car Park Birmingham - 70524\n\n\nPOINT (406364.849 287003.294)\n\n\n\n\n2\n\n\n7d545ad9367ccb8a80c94a953314ae71\n\n\nCM123\n\n\nRenault Liverpool\n\n\nPOINT (335096.815 387859.900)\n\n\n\n\n3\n\n\n68c7fca1e3bba5e49ec90847dcdd456b\n\n\nCM164\n\n\nNCP Portman Square\n\n\nPOINT (527908.652 181305.630)\n\n\n\n\n4\n\n\nac7a21c48f5833b33a5b606b2089e6a9\n\n\nCM167\n\n\nNCP Prince Street Car Park\n\n\nPOINT (358631.700 172541.813)\n\n\n\n\n\n\nEV Cars data\n\ndf_VEH0145: Licensed plug-in vehicles (PiVs) at the end of the quarter by fuel type and lower super output area (LSOA): United Kingdom, available from: https://www.gov.uk/government/statistical-data-sets/vehicle-licensing-statistics-data-files\n\n# Import csv\nPiVs_RAW = pd.read_csv('Data/df_VEH0145.csv', low_memory=False)\n# Reduce to just required columns\nPiVs_TRIM = PiVs_RAW[['LSOA11CD','LSOA11NM','Fuel','Keepership','2023 Q4']]\n# Visual check\nPiVs_TRIM.head()\n\n\n\n\n\n\nLSOA11CD\n\n\nLSOA11NM\n\n\nFuel\n\n\nKeepership\n\n\n2023 Q4\n\n\n\n\n\n\n0\n\n\n95AA01S1\n\n\nAldergrove 1\n\n\nBattery electric\n\n\nCompany\n\n\n[c]\n\n\n\n\n1\n\n\n95AA01S2\n\n\nAldergrove 2\n\n\nBattery electric\n\n\nCompany\n\n\n9\n\n\n\n\n2\n\n\n95AA01S3\n\n\nAldergrove 3\n\n\nBattery electric\n\n\nCompany\n\n\n[c]\n\n\n\n\n3\n\n\n95AA02W1\n\n\nBalloo\n\n\nBattery electric\n\n\nCompany\n\n\n5\n\n\n\n\n4\n\n\n95AA03W1\n\n\nBallycraigy\n\n\nBattery electric\n\n\nCompany\n\n\n[c]\n\n\n\n\n# Filter to all Fuel and private Keepership types, then drop those columns\nPiVs_FILTERED = PiVs_TRIM[(PiVs_TRIM['Fuel'] == 'Total') & \n                          (PiVs_TRIM['Keepership'] == 'Private')].drop(columns=['Fuel', 'Keepership'])\n# Replace the suppressed [c] data with NaN\nPiVs_FILTERED['2023 Q4'] = PiVs_FILTERED['2023 Q4'].replace('[c]', np.nan)\n\n# Convert the '2023 Q4' column to numeric data type\nPiVs_FILTERED['2023 Q4'] = pd.to_numeric(PiVs_FILTERED['2023 Q4'])\n# Visual check\nPiVs_FILTERED.head()\n\n\n\n\n\n\nLSOA11CD\n\n\nLSOA11NM\n\n\n2023 Q4\n\n\n\n\n\n\n189862\n\n\n95AA01S1\n\n\nAldergrove 1\n\n\nNaN\n\n\n\n\n189863\n\n\n95AA01S2\n\n\nAldergrove 2\n\n\n15.0\n\n\n\n\n189864\n\n\n95AA01S3\n\n\nAldergrove 3\n\n\n19.0\n\n\n\n\n189865\n\n\n95AA02W1\n\n\nBalloo\n\n\n10.0\n\n\n\n\n189866\n\n\n95AA03W1\n\n\nBallycraigy\n\n\nNaN\n\n\n\n\n\nLSOA lookup\nThe PiV data uses the LSOA codes from 2011 and so this will need to be changed to the LSOA 2021 codes.\n\nLSOA best fit lookup 2011 to 2021, available from: https://geoportal.statistics.gov.uk/datasets/b14d449ba10a48508bd05cd4a9775e2b_0/explore\n\n# Import csv\nLSOA_lookup = pd.read_csv(\n    'Data/LSOA_(2011)_to_LSOA_(2021)_to_Local_Authority_District_(2022)_Best_Fit_Lookup_for_EW_(V2).csv',\n    low_memory=False)\n# Reduce to just required columns\nLSOA_lookup = LSOA_lookup[['LSOA11CD','LSOA21CD','LAD22CD','LAD22NM']].copy()\n# Match 2021 to 2011 codes using lookup\nPiVs_FILTERED = pd.merge(PiVs_FILTERED, LSOA_lookup, on='LSOA11CD')\n# Visual check\nPiVs_FILTERED.head()\n\n\n\n\n\n\nLSOA11CD\n\n\nLSOA11NM\n\n\n2023 Q4\n\n\nLSOA21CD\n\n\nLAD22CD\n\n\nLAD22NM\n\n\n\n\n\n\n0\n\n\nE01000001\n\n\nCity of London 001A\n\n\n28.0\n\n\nE01000001\n\n\nE09000001\n\n\nCity of London\n\n\n\n\n1\n\n\nE01000002\n\n\nCity of London 001B\n\n\n30.0\n\n\nE01000002\n\n\nE09000001\n\n\nCity of London\n\n\n\n\n2\n\n\nE01000003\n\n\nCity of London 001C\n\n\n15.0\n\n\nE01000003\n\n\nE09000001\n\n\nCity of London\n\n\n\n\n3\n\n\nE01000005\n\n\nCity of London 001E\n\n\nNaN\n\n\nE01000005\n\n\nE09000001\n\n\nCity of London\n\n\n\n\n4\n\n\nE01000006\n\n\nBarking and Dagenham 016A\n\n\n18.0\n\n\nE01000006\n\n\nE09000002\n\n\nBarking and Dagenham\n\n\n\n\n\n\n\nLADs\n\nLADs 2021 polygons, available from: https://geoportal.statistics.gov.uk/datasets/305779d69bf44feea05eeaa78ca26b5f_0/explore\n\n# Import shp file\nLADs = gpd.read_file('Data/Local_Authority_Districts_December_2022_UK_BFC_V2_-177113771882051469/LAD_DEC_2022_UK_BFC_V2.shp')\n# Reduce to just required columns\nLADs = LADs[['LAD22CD','LAD22NM','geometry']].copy()\n# Visual check\nLADs.head()\n\n\n\n\n\n\nLAD22CD\n\n\nLAD22NM\n\n\ngeometry\n\n\n\n\n\n\n0\n\n\nE06000001\n\n\nHartlepool\n\n\nMULTIPOLYGON (((450154.599 525938.201, 450140….\n\n\n\n\n1\n\n\nE06000002\n\n\nMiddlesbrough\n\n\nMULTIPOLYGON (((446854.700 517192.700, 446854….\n\n\n\n\n2\n\n\nE06000003\n\n\nRedcar and Cleveland\n\n\nMULTIPOLYGON (((451747.397 520561.100, 451792….\n\n\n\n\n3\n\n\nE06000004\n\n\nStockton-on-Tees\n\n\nMULTIPOLYGON (((447177.704 517811.797, 447176….\n\n\n\n\n4\n\n\nE06000005\n\n\nDarlington\n\n\nPOLYGON ((423496.602 524724.299, 423497.204 52…\n\n\n\n\nlen(LADs)\n374\n\n\nOAs\nThis analysis will use the population-weighted centroids (PWCs) to find the nearest EV chargepoints. This will then be aggregated to LAD level, so the OA to LAD lookup is required.\n\nOA 2021 PWCs, available from: https://geoportal.statistics.gov.uk/datasets/b9b2b2440af240ce9d30a1d39a7507c2_0/explore\nOA to LAD lookup, available from: https://geoportal.statistics.gov.uk/datasets/b9ca90c10aaa4b8d9791e9859a38ca67_0/explore\n\n\nOA PWCs\n# Import shp file\nOA_PWC = gpd.read_file('Data/Output_Areas_2021_PWC_V3_-1981902074309169314/PopCentroids_EW_2021_V3.shp')\n# Reduce to just required columns\nOA_PWC = OA_PWC[['OA21CD','geometry']].copy()\n\n\nOA to LAD lookup\n# Import csv\nOA_lookup = pd.read_csv(\n    'Data/Output_Area_to_Lower_layer_Super_Output_Area_to_Middle_layer_Super_Output_Area_to_Local_Authority_District_(December_2021)_Lookup_in_England_and_Wales_v3.csv',\n    low_memory=False)\n# Keep only required columns\nOA_lookup = OA_lookup[['OA21CD','LSOA21CD','LAD22CD']].copy()\n# Visual check\nOA_lookup.head()\n\n\n\n\n\n\nOA21CD\n\n\nLSOA21CD\n\n\nLAD22CD\n\n\n\n\n\n\n0\n\n\nE00060358\n\n\nE01011968\n\n\nE06000001\n\n\n\n\n1\n\n\nE00060359\n\n\nE01011968\n\n\nE06000001\n\n\n\n\n2\n\n\nE00060360\n\n\nE01011968\n\n\nE06000001\n\n\n\n\n3\n\n\nE00060361\n\n\nE01011968\n\n\nE06000001\n\n\n\n\n4\n\n\nE00060362\n\n\nE01011970\n\n\nE06000001"
  },
  {
    "objectID": "projects/EV_charging/index.html#feature-engineering",
    "href": "projects/EV_charging/index.html#feature-engineering",
    "title": "Analysis of public EV chargepoint provision",
    "section": "Feature engineering",
    "text": "Feature engineering\n\nAggregate EV car ownership data to LAD\n# Aggregate to LAD and calculate total number of PiVs\nPiVs_LAD = PiVs_FILTERED.groupby('LAD22CD')['2023 Q4'].agg('sum').reset_index()\n\n# Rename column to PiVs\nPiVs_LAD.rename(columns={'2023 Q4': 'PiVs'}, inplace=True)\n# Visual check\nPiVs_LAD.head()\n\n\n\n\n\n\nLAD22CD\n\n\nPiVs\n\n\n\n\n\n\n0\n\n\nE06000001\n\n\n410.0\n\n\n\n\n1\n\n\nE06000002\n\n\n391.0\n\n\n\n\n2\n\n\nE06000003\n\n\n580.0\n\n\n\n\n3\n\n\nE06000004\n\n\n1308.0\n\n\n\n\n4\n\n\nE06000005\n\n\n719.0\n\n\n\n\nlen(PiVs_LAD)\n331\n\n\nCalculate distance to nearest chargepoint\n\nUsing the OA PWCs, calculate the nearest EV chargepoint.\nAggregate this to LAD level to get the average distance to the nearest chargepoint by LAD.\n\n# Find the nearest chargepoint to each OA PWC and calculate distance in metres\nOA_nearest_chargepoint = OA_PWC.sjoin_nearest(chargepoints_gdf, distance_col='distance', how='left')\n# Merge on OA_lookup_Leeds to get the LAD22CDs\nOA_nearest_chargepoint = pd.merge(OA_nearest_chargepoint, OA_lookup, on='OA21CD', how='inner')\n# Keep only required columns\nOA_nearest_chargepoint = OA_nearest_chargepoint[['OA21CD','geometry','distance','LAD22CD']].copy()\n# Aggregate to LAD and calculate average distance to nearest chargepoint\navg_dist_nearest_chargepoint_LAD = OA_nearest_chargepoint.groupby('LAD22CD')['distance'].agg('mean').reset_index()\n\n# Rename column to avg_distance\navg_dist_nearest_chargepoint_LAD.rename(columns={'distance': 'avg_distance'}, inplace=True)\n# Visual check\navg_dist_nearest_chargepoint_LAD.head()\n\n\n\n\n\n\nLAD22CD\n\n\navg_distance\n\n\n\n\n\n\n0\n\n\nE06000001\n\n\n1649.498055\n\n\n\n\n1\n\n\nE06000002\n\n\n1088.305461\n\n\n\n\n2\n\n\nE06000003\n\n\n1454.585351\n\n\n\n\n3\n\n\nE06000004\n\n\n911.259807\n\n\n\n\n4\n\n\nE06000005\n\n\n1188.923394\n\n\n\n\nlen(avg_dist_nearest_chargepoint_LAD)\n331\n\n\nCalculate number of EV chargepoints in each LAD\n\nAggregate the EV chargepoint data to LAD level to get total number of chargepoints in each LAD.\n\n# Spatially match\nchargepoints_LAD = gpd.sjoin(LADs, chargepoints_gdf, predicate='intersects')\n# Aggregate to LAD and calculate total number of chargepoints\nchargepoints_LAD = chargepoints_LAD.groupby('LAD22CD')['index_right'].agg('count').reset_index()\n\n# Rename column to total_chargepoints\nchargepoints_LAD.rename(columns={'index_right': 'total_chargepoints'}, inplace=True)\n# Visual check\nchargepoints_LAD.head()\n\n\n\n\n\n\nLAD22CD\n\n\ntotal_chargepoints\n\n\n\n\n\n\n0\n\n\nE06000001\n\n\n42\n\n\n\n\n1\n\n\nE06000002\n\n\n51\n\n\n\n\n2\n\n\nE06000003\n\n\n46\n\n\n\n\n3\n\n\nE06000004\n\n\n167\n\n\n\n\n4\n\n\nE06000005\n\n\n76\n\n\n\n\nlen(chargepoints_LAD)\n373\n\n\nCalculate ratio of EV cars to chargepoints in each LAD\n# Merge PiVs and chargepoint count dataframes\nratio_PiVs_to_chargepoints = pd.merge(PiVs_LAD, chargepoints_LAD, on='LAD22CD', how='left')\n\n# Replace NaN values with 0 where there are no chargepoints in an MSOA\nratio_PiVs_to_chargepoints.fillna({'total_chargepoints': 0}, inplace=True)\n# Calculate ratio\nratio_PiVs_to_chargepoints['ratio_PiVs_to_chargepoints'] = (\n    ratio_PiVs_to_chargepoints['PiVs'] / ratio_PiVs_to_chargepoints['total_chargepoints'])\n\n# Replace 'inf' with NaN\nratio_PiVs_to_chargepoints['ratio_PiVs_to_chargepoints'] = ratio_PiVs_to_chargepoints[\n    'ratio_PiVs_to_chargepoints'].replace([np.inf, -np.inf], np.nan)\n\n\nAdd average distance and LAD polygon to final dataframe\n# Add nearest chargepoint\nfinal_df = pd.merge(ratio_PiVs_to_chargepoints,avg_dist_nearest_chargepoint_LAD, on='LAD22CD')\n\n# Add LAD polygons\nfinal_df = pd.merge(LADs, final_df, on='LAD22CD')\n# Visual check\nfinal_df.head()\n\n\n\n\n\n\nLAD22CD\n\n\nLAD22NM\n\n\ngeometry\n\n\nPiVs\n\n\ntotal_chargepoints\n\n\nratio_PiVs_to_chargepoints\n\n\navg_distance\n\n\n\n\n\n\n0\n\n\nE06000001\n\n\nHartlepool\n\n\nMULTIPOLYGON (((450154.599 525938.201, 450140….\n\n\n410.0\n\n\n42.0\n\n\n9.761905\n\n\n1649.498055\n\n\n\n\n1\n\n\nE06000002\n\n\nMiddlesbrough\n\n\nMULTIPOLYGON (((446854.700 517192.700, 446854….\n\n\n391.0\n\n\n51.0\n\n\n7.666667\n\n\n1088.305461\n\n\n\n\n2\n\n\nE06000003\n\n\nRedcar and Cleveland\n\n\nMULTIPOLYGON (((451747.397 520561.100, 451792….\n\n\n580.0\n\n\n46.0\n\n\n12.608696\n\n\n1454.585351\n\n\n\n\n3\n\n\nE06000004\n\n\nStockton-on-Tees\n\n\nMULTIPOLYGON (((447177.704 517811.797, 447176….\n\n\n1308.0\n\n\n167.0\n\n\n7.832335\n\n\n911.259807\n\n\n\n\n4\n\n\nE06000005\n\n\nDarlington\n\n\nPOLYGON ((423496.602 524724.299, 423497.204 52…\n\n\n719.0\n\n\n76.0\n\n\n9.460526\n\n\n1188.923394\n\n\n\n\n\n\nHandle outliers\nThere are some outliers on the top end, so anything outside 2sd will be amended.\n# Loop through each variable and calculate mean, std, and threshold\nfor variable in ['PiVs', 'total_chargepoints', 'ratio_PiVs_to_chargepoints', 'avg_distance']:\n    mean_var = final_df[variable].mean()\n    std_var = final_df[variable].std()\n    threshold_var = mean_var + 2 * std_var\n    \n    # Create new column based on outlier condition\n    final_df[f'amended_{variable}'] = final_df[variable].apply(lambda x: threshold_var if x &gt; threshold_var else x)\n# Visual check\nfinal_df.head()\n\n\n\n\n\n\nLAD22CD\n\n\nLAD22NM\n\n\ngeometry\n\n\nPiVs\n\n\ntotal_chargepoints\n\n\nratio_PiVs_to_chargepoints\n\n\navg_distance\n\n\namended_PiVs\n\n\namended_total_chargepoints\n\n\namended_ratio_PiVs_to_chargepoints\n\n\namended_avg_distance\n\n\n\n\n\n\n0\n\n\nE06000001\n\n\nHartlepool\n\n\nMULTIPOLYGON (((450154.599 525938.201, 450140….\n\n\n410.0\n\n\n42.0\n\n\n9.761905\n\n\n1649.498055\n\n\n410.0\n\n\n42.0\n\n\n9.761905\n\n\n1649.498055\n\n\n\n\n1\n\n\nE06000002\n\n\nMiddlesbrough\n\n\nMULTIPOLYGON (((446854.700 517192.700, 446854….\n\n\n391.0\n\n\n51.0\n\n\n7.666667\n\n\n1088.305461\n\n\n391.0\n\n\n51.0\n\n\n7.666667\n\n\n1088.305461\n\n\n\n\n2\n\n\nE06000003\n\n\nRedcar and Cleveland\n\n\nMULTIPOLYGON (((451747.397 520561.100, 451792….\n\n\n580.0\n\n\n46.0\n\n\n12.608696\n\n\n1454.585351\n\n\n580.0\n\n\n46.0\n\n\n12.608696\n\n\n1454.585351\n\n\n\n\n3\n\n\nE06000004\n\n\nStockton-on-Tees\n\n\nMULTIPOLYGON (((447177.704 517811.797, 447176….\n\n\n1308.0\n\n\n167.0\n\n\n7.832335\n\n\n911.259807\n\n\n1308.0\n\n\n167.0\n\n\n7.832335\n\n\n911.259807\n\n\n\n\n4\n\n\nE06000005\n\n\nDarlington\n\n\nPOLYGON ((423496.602 524724.299, 423497.204 52…\n\n\n719.0\n\n\n76.0\n\n\n9.460526\n\n\n1188.923394\n\n\n719.0\n\n\n76.0\n\n\n9.460526\n\n\n1188.923394"
  },
  {
    "objectID": "projects/EV_charging/index.html#visualising-data",
    "href": "projects/EV_charging/index.html#visualising-data",
    "title": "Analysis of public EV chargepoint provision",
    "section": "Visualising data",
    "text": "Visualising data\nThe following code builds an interactive map of the features in final_df. This is saved down into an Outputs/ folder as an HTML file.\n# Create the individual layers\nm = final_df.explore(\n    column=\"amended_PiVs\",\n    scheme=\"naturalbreaks\",\n    cmap='YlOrRd',\n    legend=False,\n    k=10,\n    tooltip=[\"LAD22CD\", \"LAD22NM\", \"amended_PiVs\"],\n    style_kwds=dict(color='black', weight=0.5, fillOpacity=0.8),\n    highlight_kwds=dict(fillOpacity=1),\n    name=\"Plug-in Vehicles\",\n    show=True)\n\nfinal_df.explore(\n    m=m,\n    column=\"amended_total_chargepoints\",\n    scheme=\"naturalbreaks\",\n    cmap='YlOrRd',\n    legend=False,\n    k=10,\n    tooltip=[\"LAD22CD\", \"LAD22NM\", \"amended_total_chargepoints\"],\n    style_kwds=dict(color='black', weight=0.5, fillOpacity=0.8),\n    highlight_kwds=dict(fillOpacity=1),\n    name=\"Total EV Chargepoints\",\n    show=False)\n\nfinal_df.explore(\n    m=m,\n    column=\"amended_ratio_PiVs_to_chargepoints\",\n    scheme=\"naturalbreaks\",\n    cmap='YlOrRd',\n    legend=False,\n    k=10,\n    tooltip=[\"LAD22CD\", \"LAD22NM\", \"amended_ratio_PiVs_to_chargepoints\"],\n    style_kwds=dict(color='black', weight=0.5, fillOpacity=0.8),\n    highlight_kwds=dict(fillOpacity=1),\n    name=\"Ratio of Plug-in Vehicles to EV Chargepoints\",\n    show=False)\n\nfinal_df.explore(\n    m=m,\n    column=\"amended_avg_distance\",\n    scheme=\"naturalbreaks\",\n    cmap='YlOrRd',\n    legend=False,\n    k=10,\n    tooltip=[\"LAD22CD\", \"LAD22NM\", \"amended_avg_distance\"],\n    style_kwds=dict(color='black', weight=0.5, fillOpacity=0.8),\n    highlight_kwds=dict(fillOpacity=1),\n    name=\"Average distance to nearest EV Chargepoint by OA\",\n    show=False)\n\n# Add the map base\nfolium.TileLayer(\"CartoDB positron\", show=True).add_to(m)\n\n# Add layer control to map\nfolium.LayerControl().add_to(m)\n\n# Save the map to an HTML file\nm.save(\"Outputs/Interactive_map.html\")\nThe following code builds four choropleth maps of the features in final_df. This is saved down into an Outputs/ folder as a .png file.\n# Create 2x2 subplots\nfig, axs = plt.subplots(2, 2, figsize=(12, 12))\n\n# Define aliases for column names\ncolumn_aliases = {\n    'amended_PiVs': 'Plug-in Vehicles (PiVs)',\n    'amended_total_chargepoints': 'Total EV Chargepoints (EVCs)',\n    'amended_ratio_PiVs_to_chargepoints': 'Ratio of PiVs to EVCs',\n    'amended_avg_distance': 'Average distance to nearest EVC by OA (metres)'\n}\n\n# Loop through each variable and corresponding subplot\nvariables = list(column_aliases.keys())\nfor variable, ax in zip(variables, axs.flatten()):\n    final_df.plot(column=variable, cmap='YlOrRd', ax=ax, legend=True)\n    ax.set_title(f'{column_aliases[variable]}')  # Set the title using the alias\n    ax.set_axis_off()  # Turn off axis\n    \nplt.subplots_adjust(wspace=0.05, hspace=0.05)  # Adjust space between subplots\nplt.tight_layout()  # Adjust layout to prevent overlap\n\n# Export to Outputs/ folder\nplt.savefig('Outputs/Feature_choropleths.png', dpi=300)\n\n# Close figure\nplt.close(fig)\nThe following code creates two scatterplots with regression lines for selected features from the final_df. This is saved down into an Outputs/ folder as a .png file.\n# Create a subplot grid with 1 row and 2 columns\nfig, axs = plt.subplots(1, 2, figsize=(15, 6))\n\n# Plot the first scatter plot\nsns.scatterplot(x='amended_PiVs', y='amended_total_chargepoints', data=final_df, ax=axs[0])\nsns.regplot(x='amended_PiVs', y='amended_total_chargepoints', data=final_df, scatter=False, ax=axs[0])\naxs[0].set_xlabel(column_aliases['amended_PiVs'])  # Set x-axis label using alias\naxs[0].set_ylabel(column_aliases['amended_total_chargepoints'])  # Set y-axis label using alias\naxs[0].set_title('Plug-in Vehicles vs. Total EV Chargepoints')  # Set the title\n\n# Plot the second scatter plot\nsns.scatterplot(x='amended_ratio_PiVs_to_chargepoints', y='amended_avg_distance', data=final_df, ax=axs[1])\nsns.regplot(x='amended_ratio_PiVs_to_chargepoints', y='amended_avg_distance', data=final_df, scatter=False, ax=axs[1])\naxs[1].set_xlabel(column_aliases['amended_ratio_PiVs_to_chargepoints'])  # Set x-axis label using alias\naxs[1].set_ylabel(column_aliases['amended_avg_distance'])  # Set y-axis label using alias\naxs[1].set_title('Ratio vs. Distance')  # Set the title\n\n# Adjust layout\nplt.tight_layout()\n\n# Export to Outputs/ folder\nplt.savefig('Outputs/Scatter_Reg_Plots.png', dpi=300)\n\n# Close figure\nplt.close(fig)\nThe following code creates a correlation matrix heatmap of the features in final_df. This is saved down into an Outputs/ folder as a .png file.\n# Calculate the correlation matrix\ncorrelation_matrix = final_df[[\n    'amended_PiVs','amended_total_chargepoints',\n    'amended_ratio_PiVs_to_chargepoints','amended_avg_distance']].corr()\n\n# Generate a mask for the diagonal cells\nmask = np.triu(np.ones_like(correlation_matrix, dtype=bool))\n\n# Generate a heatmap\nplt.figure(figsize=(8, 7))\nsns.heatmap(correlation_matrix, cmap='RdYlGn', annot=True, \n            fmt=\".2f\", mask=mask, vmin=-1, vmax=1, cbar_kws={\"shrink\": 0.75}, linewidth=.5)\n\n# Rotate axis labels\nplt.xticks(rotation=45)\nplt.yticks(rotation=45)\n\n# Set axis labels using aliases and wrap text\nplt.xticks(ticks=range(len(correlation_matrix.columns)), \n           labels=['Plug-in Vehicles', \n                   'Total EV Chargepoints', \n                   'Ratio of PiVs to EVCs', \n                   'Avg dist to nearest EVC'])\n\nplt.yticks(ticks=np.arange(len(correlation_matrix.columns))+0.5, \n           labels=['Plug-in Vehicles', \n                   'Total EV Chargepoints', \n                   'Ratio of PiVs to EVCs', \n                   'Avg dist to nearest EVC'])\n\nplt.title('Correlation Matrix Heatmap', fontweight='bold')\n\nplt.tight_layout()\n\n# Export to Outputs/ folder\nplt.savefig('Outputs/CorrelationMatrix.png', dpi=300)\n\n# Close figure\nplt.close()"
  },
  {
    "objectID": "projects/EV_charging/index.html#creating-a-ranking",
    "href": "projects/EV_charging/index.html#creating-a-ranking",
    "title": "Analysis of public EV chargepoint provision",
    "section": "Creating a ranking",
    "text": "Creating a ranking\nThe following code creates a simple ranking of LADs based on:\n\nAverage distance to the nearest EV chargepoint\nRatio of Plug-in vehicles to EV chargepoints\n\nThe idea is that if the average distance is high, and the ratio is high, then these areas may be lacking public EV infrastructure.\n# Create a new dataframe containing just the amended features for the ranking\nranking = final_df.drop(final_df.columns[3:9], axis=1)\n# Rank the values in each column\nranking['rank_ratio'] = ranking['amended_ratio_PiVs_to_chargepoints'].rank(ascending=True, method='min')\nranking['rank_distance'] = ranking['amended_avg_distance'].rank(ascending=True, method='min')\n# Compute the combined ranking\nranking['combined_rank'] = (ranking['rank_ratio'] + ranking['rank_distance']) / 2\nranking['combined_rank'] = ranking['combined_rank'].rank(ascending=True, method='min')\nThe following code creates a map of the top 10 combined_rank LADs in ranking. This is saved down into an Outputs/ folder as a .png file.\n# Get top 10 areas\nranking[['LAD22CD','LAD22NM','combined_rank']].sort_values(by='combined_rank', ascending=True).head(10)\n\n\n\n\n\n\nLAD22CD\n\n\nLAD22NM\n\n\ncombined_rank\n\n\n\n\n\n\n288\n\n\nE09000013\n\n\nHammersmith and Fulham\n\n\n1.0\n\n\n\n\n308\n\n\nE09000033\n\n\nWestminster\n\n\n2.0\n\n\n\n\n303\n\n\nE09000028\n\n\nSouthwark\n\n\n3.0\n\n\n\n\n276\n\n\nE09000001\n\n\nCity of London\n\n\n4.0\n\n\n\n\n295\n\n\nE09000020\n\n\nKensington and Chelsea\n\n\n4.0\n\n\n\n\n299\n\n\nE09000024\n\n\nMerton\n\n\n6.0\n\n\n\n\n307\n\n\nE09000032\n\n\nWandsworth\n\n\n6.0\n\n\n\n\n282\n\n\nE09000007\n\n\nCamden\n\n\n8.0\n\n\n\n\n287\n\n\nE09000012\n\n\nHackney\n\n\n9.0\n\n\n\n\n294\n\n\nE09000019\n\n\nIslington\n\n\n9.0\n\n\n\n\n# Sort and select the top 10 areas\ntop_10_areas = ranking.sort_values(by='combined_rank', ascending=True).head(10)\n\n# Plot only the top 10 areas\nfig, ax = plt.subplots(1, 1, figsize=(8, 10))\ntop_10_areas.plot(ax=ax, color='#88C88E', edgecolor='black', linewidth=1, legend=True, alpha=0.5)\n\n# Annotate the top 10 areas with their rank\nfor idx, row in top_10_areas.iterrows():\n    plt.annotate(text=int(row['combined_rank']), xy=(row.geometry.centroid.x, row.geometry.centroid.y),\n                 horizontalalignment='center', fontsize=12, weight='bold', color='black')\n\n# Plot basemap\nctx.add_basemap(ax, crs=top_10_areas.crs.to_string(), source=ctx.providers.CartoDB.Positron)\n\n# Set title and remove axis\nax.set_title('Top 10 Areas by Combined Rank', fontsize=12, fontweight='bold')\nax.set_axis_off()\n\n# Adjust layout\nplt.tight_layout()\n\n# Export to Outputs/ folder\nplt.savefig('Outputs/Map_of_Top_10.png', dpi=300)\n\n# Close figure\nplt.close(fig)\nThe following code creates a map of the bottom 10 combined_rank LADs in ranking. This is saved down into an Outputs/ folder as a .png file.\n# Show bottom 10 areas\nranking[['LAD22CD','LAD22NM','combined_rank']].sort_values(by='combined_rank', ascending=False).head(10)\n\n\n\n\n\n\nLAD22CD\n\n\nLAD22NM\n\n\ncombined_rank\n\n\n\n\n\n\n99\n\n\nE07000074\n\n\nMaldon\n\n\n330.0\n\n\n\n\n89\n\n\nE07000064\n\n\nRother\n\n\n329.0\n\n\n\n\n235\n\n\nE07000242\n\n\nEast Hertfordshire\n\n\n328.0\n\n\n\n\n16\n\n\nE06000017\n\n\nRutland\n\n\n327.0\n\n\n\n\n100\n\n\nE07000075\n\n\nRochford\n\n\n326.0\n\n\n\n\n85\n\n\nE07000047\n\n\nWest Devon\n\n\n325.0\n\n\n\n\n177\n\n\nE07000169\n\n\nSelby\n\n\n324.0\n\n\n\n\n160\n\n\nE07000139\n\n\nNorth Kesteven\n\n\n323.0\n\n\n\n\n82\n\n\nE07000044\n\n\nSouth Hams\n\n\n321.0\n\n\n\n\n199\n\n\nE07000198\n\n\nStaffordshire Moorlands\n\n\n321.0\n\n\n\n\n# Sort and select the bottom 10 areas\nbottom_10_areas = ranking.sort_values(by='combined_rank', ascending=False).head(10)\n\n# Plot only the top 10 areas\nfig, ax = plt.subplots(1, 1, figsize=(10, 10))\nbottom_10_areas.plot(ax=ax, color='#DE6464', edgecolor='black', linewidth=1, legend=True, alpha=0.5)\n\n# Annotate the top 10 areas with their rank\nfor idx, row in bottom_10_areas.iterrows():\n    plt.annotate(text=int(row['combined_rank']), xy=(row.geometry.centroid.x, row.geometry.centroid.y),\n                 horizontalalignment='center', fontsize=12, weight='bold', color='black')\n\n# Plot basemap\nctx.add_basemap(ax, crs=bottom_10_areas.crs.to_string(), source=ctx.providers.CartoDB.Positron)\n\n# Set title and remove axis\nax.set_title('Bottom 10 Areas by Combined Rank', fontsize=12, fontweight='bold')\nax.set_axis_off()\n\n# Adjust layout\nplt.tight_layout()\n\n# Export to Outputs/ folder\nplt.savefig('Outputs/Map_of_Bottom_10.png', dpi=300)\n\n# Close figure\nplt.close(fig)"
  },
  {
    "objectID": "projects/index.html#ev-charging",
    "href": "projects/index.html#ev-charging",
    "title": "Projects",
    "section": "EV charging",
    "text": "EV charging\n\n\n\nAnalysis of public EV chargepoint provision\nUsing public EV chargepoint and plug-in vehicle registration data to understand, by local authority, which areas have the best and worst public EV infrastructure."
  },
  {
    "objectID": "projects/intergenerational_care_facilities/index.html",
    "href": "projects/intergenerational_care_facilities/index.html",
    "title": "Identifying potential priority local authorities in England for intergenerational care facilities",
    "section": "",
    "text": "Go back to the projects page."
  },
  {
    "objectID": "projects/intergenerational_care_facilities/index.html#summary",
    "href": "projects/intergenerational_care_facilities/index.html#summary",
    "title": "Identifying potential priority local authorities in England for intergenerational care facilities",
    "section": "Summary",
    "text": "Summary\nThis analysis uses public EV chargepoint and plug-in vehicle (PiV) registration data to understand, by local authority district (LAD), which areas have the best and worst EV infrastructure.\n\nWhat data is included?\nFirst, four features are calculated to allow for initial exploration of the data. Details of these are as follows:\nCount of PiVs\nSource: DVLA and DfT dataset\n\nA simple count of registered PiVs by LAD is calculated.\nThis includes all PiVs with Private keepership. This excludes PiVs where the keepership is Company to avoid skewing the analysis with large quantities of fleet vehicles registered to a single address.\nWhere data has been suppressed in the raw file, the [c] value has been replaced with NaN\n\nCount of EV chargepoints\nSource: Public EV chargepoint registry\n\nA simple count of public EV chargepoints by LAD is calculated.\nThere has been no cleaning of this data beyond converting the latitude and longitude into a point. This is a public dataset and further accuracy has not been verified.\n\nRatio of PiVs to EV chargepoints\nSource: Author’s calculations\n\nTaking the count of PiVs in each LAD and dividing this by the count of public EV chargepoints in each LAD.\nA higher ratio means there are more PiVs to each chargepoint and therefore public infrastructure may be lacking.\n\nAverage distance to nearest EV chargepoint\nSource: Geographies from ONS via Open Geography Portal\n\nUsing the population-weighted centroids (PWCs) for each output area (OA) within an LAD, the nearest public EV chargepoint is calculated. OA is the smallest census geography and contains between 40 and 250 households.\nThen this is aggregated to LAD level by taking the average distance of all OAs within the LAD to obtain the average distance to the nearest public EV chargepoint by LAD.\n\n\n\nWhat do these features show?\nThe following choropleths map these four features. The absolute number of PiVs varies across England and Wales, with LADs across Wales, Lincolnshire and Cumbria having particularly low numbers. By contrast, the highest number of public EV chargepoints by LAD is concentrated in Greater London, with hotspots dotted elsewhere across the country. If we instead look at this as the ratio of PiVs to chargepoints, we again see the patterns change. Hotspots around the southeast of England highlight that there is potentially infrastructure lacking in these areas compared to PiV numbers. Lastly, we can see the average distance to the nearest chargepoint is perhaps unsurprisingly highest in rural areas, particularly in the north of England and Wales.\n\nCode to create an interactive version of this map can be found below.\n\n\nWhat is the relationship between the features?\nThe correlation matrix heatmap below shows the relationship between each feature. There is some positive correlation between chargepoints and PiVs, which is good as this suggests that where there are more PiVs there tend to be more chargepoints. There is also some positive correlation between average distance to the nearest chargepoint and the ratio of PiVs to chargepoints. This means that if the distance is greater, there are also more PiVs to chargepoints perhaps suggesting these areas lack infrastructure.\n\nThe following scatter plots illustrate this further, adding a regression line. We can see that there are outliers across both, with a cluster of LADs on the lower end.\n\n\n\nSo what does this mean for public EV infrastructure?\nTaking the ratio of PiVs to public EV chargepoints and the average distance to the nearest chargepoint, a simple ranking is created combining the two. Areas with a high ratio of PiVs to chargepoints and a large distance to the nearest chargepoint could be lacking in public EV infrastructure.\nTop 10 The best performing LADs were all in London, suggesting infrastructure in the capital is well established.\n\n\n\n\n\n\n\nLAD22CD\n\n\nLAD22NM\n\n\ncombined_rank\n\n\n\n\n\n\n288\n\n\nE09000013\n\n\nHammersmith and Fulham\n\n\n1.0\n\n\n\n\n308\n\n\nE09000033\n\n\nWestminster\n\n\n2.0\n\n\n\n\n303\n\n\nE09000028\n\n\nSouthwark\n\n\n3.0\n\n\n\n\n276\n\n\nE09000001\n\n\nCity of London\n\n\n4.0\n\n\n\n\n295\n\n\nE09000020\n\n\nKensington and Chelsea\n\n\n4.0\n\n\n\n\n299\n\n\nE09000024\n\n\nMerton\n\n\n6.0\n\n\n\n\n307\n\n\nE09000032\n\n\nWandsworth\n\n\n6.0\n\n\n\n\n282\n\n\nE09000007\n\n\nCamden\n\n\n8.0\n\n\n\n\n287\n\n\nE09000012\n\n\nHackney\n\n\n9.0\n\n\n\n\n294\n\n\nE09000019\n\n\nIslington\n\n\n9.0\n\n\n\n\nBottom 10 The worst performing LADs are more dispersed. Most are more rural areas, or include only one or two larger towns.\n\n\n\n\n\n\n\nLAD22CD\n\n\nLAD22NM\n\n\ncombined_rank\n\n\n\n\n\n\n99\n\n\nE07000074\n\n\nMaldon\n\n\n330.0\n\n\n\n\n89\n\n\nE07000064\n\n\nRother\n\n\n329.0\n\n\n\n\n235\n\n\nE07000242\n\n\nEast Hertfordshire\n\n\n328.0\n\n\n\n\n16\n\n\nE06000017\n\n\nRutland\n\n\n327.0\n\n\n\n\n100\n\n\nE07000075\n\n\nRochford\n\n\n326.0\n\n\n\n\n85\n\n\nE07000047\n\n\nWest Devon\n\n\n325.0\n\n\n\n\n177\n\n\nE07000169\n\n\nSelby\n\n\n324.0\n\n\n\n\n160\n\n\nE07000139\n\n\nNorth Kesteven\n\n\n323.0\n\n\n\n\n82\n\n\nE07000044\n\n\nSouth Hams\n\n\n321.0\n\n\n\n\n199\n\n\nE07000198\n\n\nStaffordshire Moorlands\n\n\n321.0\n\n\n\n\nThis information could be used by both public and private organisations alike to understand where public EV infrastructure is lacking and therefore incentivise installation of new infrastructure.\n\n\nConsiderations and future work\nThis analysis has demonstrated that current public EV structure may be lacking in some areas based on the current number of PiVs. This does not include:\n\nA temporal view of how PiV ownership has changed over time - there may be areas experiencing higher growth, with could arguably justify increased investment over other areas.\nEV infrastructure on private property (e.g. at home) - areas with a higher number of terraced houses or flats may require more public infrastructure as it can often be more challenging to install at these types of properties. Areas with higher rates of rented accomodation over private ownership could also face challenges as it would be the responsibility of the landlord to install EV chargepoints. Again, these areas may require more public infrastructure and so overlaying this data could be an interesting investigation to see how it affects the ranking.\n\nThe public EV chargepoint registry has also not been quality checked. Further investigation into this dataset and wrangling/cleaning as appropriate may impact results."
  },
  {
    "objectID": "projects/intergenerational_care_facilities/index.html#coding",
    "href": "projects/intergenerational_care_facilities/index.html#coding",
    "title": "Identifying potential priority local authorities in England for intergenerational care facilities",
    "section": "Coding",
    "text": "Coding"
  },
  {
    "objectID": "projects/intergenerational_care_facilities/index.html#import-libraries",
    "href": "projects/intergenerational_care_facilities/index.html#import-libraries",
    "title": "Identifying potential priority local authorities in England for intergenerational care facilities",
    "section": "Import libraries",
    "text": "Import libraries\nimport pandas as pd\nimport geopandas as gpd\nfrom shapely.geometry import Point\nimport numpy as np\nimport seaborn as sns\nfrom scipy.stats import zscore\nimport matplotlib.pyplot as plt\nimport contextily as ctx\nimport folium"
  },
  {
    "objectID": "projects/intergenerational_care_facilities/index.html#data-wrangling",
    "href": "projects/intergenerational_care_facilities/index.html#data-wrangling",
    "title": "Identifying potential priority local authorities in England for intergenerational care facilities",
    "section": "2. Data wrangling",
    "text": "2. Data wrangling\n\n2.1. LAs\nOnly LAs in England are needed. The CTYUA22CD column is a unique code for each LA, with those in England starting with the letter “E”, Northern Ireland “N”, Scotland “S” and Wales “W”. This code can therefore be used to filter the data to just England (those starting with “E”), leaving the 152 LAs in England as per the ONS (2023a) figure.\n# Remove unnecessary columns from LA data to keep only the required columns in a new dataframe\nLAs_trim = LAs.loc[:, ['CTYUA22CD', 'CTYUA22NM', 'geometry']]\n# Reduce to just LAs in England by filtering on codes that start with 'E'\nLAs_Eng = LAs_trim[LAs_trim['CTYUA22CD'].str.startswith('E')]\n# Check the length of the dataframe, which should be 152 which is the full LA list for England\nprint(len(LAs_Eng))\nprint(len(LAs_Eng) == 152)\n152 True\n# Visual check of dataframe\nLAs_Eng.head()\n\n\n\n\n\n\nCTYUA22CD\n\n\nCTYUA22NM\n\n\ngeometry\n\n\n\n\n\n\n0\n\n\nE06000001\n\n\nHartlepool\n\n\nMULTIPOLYGON (((450154.599 525938.201, 450140….\n\n\n\n\n1\n\n\nE06000002\n\n\nMiddlesbrough\n\n\nMULTIPOLYGON (((446854.700 517192.700, 446854….\n\n\n\n\n2\n\n\nE06000003\n\n\nRedcar and Cleveland\n\n\nMULTIPOLYGON (((451747.397 520561.100, 451792….\n\n\n\n\n3\n\n\nE06000004\n\n\nStockton-on-Tees\n\n\nMULTIPOLYGON (((447177.704 517811.797, 447176….\n\n\n\n\n4\n\n\nE06000005\n\n\nDarlington\n\n\nPOLYGON ((423496.602 524724.299, 423497.204 52…\n\n\n\n\n\n\n2.2. Age\nAge data needs to be pivoted to create columns of the datapoints by LA and then aggregate these to the required age group levels. The percentage of the total population is then calculated to indicate where target populations are highest and therefore service provision may be in demand.\n# Rename columns in Age dataframe to align to other data sources\nAge = Age.rename(columns={'Upper tier local authorities Code': 'CTYUA22CD', 'Upper tier local authorities': 'CTYUA22NM'})\n# Double check the number of unique CTYUA22CDs is the same as the length of the LAs_Eng dataframe\nAge['CTYUA22CD'].nunique() == len(LAs_Eng)\nTrue\n# Define a function that reformats the Census Age data into a usable dataframe for analysis\ndef Age_data_setup(dataframe):\n  \n    # Drop columns that are not required\n    dropped = dataframe.drop(dataframe.columns[[1,2]], axis=1)\n    \n    # Pivot the dataframe on column[0], making the options from [1] the new column headers and the data that from [2]\n    pivot = dropped.pivot_table(index=dropped.columns[0],\n                                columns=dropped.columns[1],\n                                values=dropped.columns[2]).reset_index()\n    pivot.columns.name = None\n    \n    # Create a Total column from new data columns\n    pivot['Total'] = pivot.iloc[:, 1:9].sum(axis=1)\n    \n    # Loop through each 'Aged' column (excluding 'Total')\n    for col in pivot.columns[1:-1]:\n        # Create a new column with percentage values\n        new_col_name = col + ' PC'\n        pivot[new_col_name] = pivot[col] / pivot['Total']\n    \n    # Select only the required Age groups for analysis\n    grouped = pivot[['CTYUA22CD', 'Aged 4 years and under', 'Aged 65 to 74 years', 'Aged 75 years and over',\n                       'Aged 4 years and under PC', 'Aged 65 to 74 years PC', 'Aged 75 years and over PC']].copy()\n\n    # Create new 'Aged 65 years and over' column by summing the two older Age columns using .loc\n    grouped.loc[:, 'Aged 65 years and over'] = grouped['Aged 65 to 74 years'] + grouped['Aged 75 years and over']\n    grouped.loc[:, 'Aged 65 years and over PC'] = grouped['Aged 65 to 74 years PC'] + grouped['Aged 75 years and over PC']\n\n    # Drop the Age columns no longer required\n    cols = [2,3,5,6]\n    Age_final = grouped.drop(grouped.columns[cols], axis=1)\n    \n    return Age_final\n# Use function to create Age_final dataframe\nAge_final = Age_data_setup(Age)\n# Double check length is still correct and no LAs or data has been lost\nlen(Age_final) == len(LAs_Eng)\nTrue\n# Visual check of dataframe\nAge_final.head()\n\n\n\n\n\n\nCTYUA22CD\n\n\nAged 4 years and under\n\n\nAged 4 years and under PC\n\n\nAged 65 years and over\n\n\nAged 65 years and over PC\n\n\n\n\n\n\n0\n\n\nE06000001\n\n\n4982.0\n\n\n0.059003\n\n\n18233.0\n\n\n0.215936\n\n\n\n\n1\n\n\nE06000002\n\n\n8928.0\n\n\n0.066919\n\n\n24178.0\n\n\n0.181223\n\n\n\n\n2\n\n\nE06000003\n\n\n6814.0\n\n\n0.055873\n\n\n31747.0\n\n\n0.260315\n\n\n\n\n3\n\n\nE06000004\n\n\n10733.0\n\n\n0.059477\n\n\n37184.0\n\n\n0.206056\n\n\n\n\n4\n\n\nE06000005\n\n\n5496.0\n\n\n0.056254\n\n\n22040.0\n\n\n0.225591\n\n\n\n\n\n\n2.3. Care Homes (with or without nursing)\nThe CQC directory needs to be filtered to isolate only locations required for this analysis. This includes active care homes (with or without nursing) that provide services for older people and are registered as social care organisations. Other locations are not considered. Some of the care homes are missing latitude and longitude data, which is created using the postcode that is listed. It is acknowledged that this methodology will not provide the exact location, but is sufficient for this analysis as information is aggregated to LA level. To aggregate the data, the care home geodataframe is spatially matched to the LA geodataframe before calculating care home counts and total beds by each LA.\n\n2.3.1. Clean care homes dataframe\n# Filter CQC Directory to only locations that meet given criteria for analysis\nCQC_directory_filtered = CQC_directory_with_filters[(\n    CQC_directory_with_filters['Dormant (Y/N)'] == 'N') &                       # Remove any dormant locations\n    (CQC_directory_with_filters['Care home?'] == 'Y') &                         # Keep only Care Homes\n    (CQC_directory_with_filters['Service user band - Older People'] == 'Y') &   # Keep only Users include \"Older People\"\n    (CQC_directory_with_filters['Location Type/Sector'] == 'Social Care Org')]  # Keep only Social Care Organisations\n# Keep only columns required for analysis\nCQC_CareHomes = CQC_directory_filtered[[\n    'Location ID', 'Care homes beds', 'Location Latitude', 'Location Longitude']].copy().reset_index(drop=True)\n\n\n2.3.2. Fix missing latitude and longitude\n# Check all rows have a Lat/Long\nmissing_latlong = CQC_CareHomes[CQC_CareHomes[['Location Latitude', 'Location Longitude']].isnull().any(axis=1)]\nlen(missing_latlong)\n22\n# Bring through postcode for these rows\nmissing_latlong = pd.merge(\n    missing_latlong, CQC_directory_filtered[['Location ID','Location Postal Code']], on='Location ID', how='left')\n# Function to geocode postcode to get latitude and longitude\ndef geocode_postcode(postcode):\n    try:\n        geolocator = Nominatim(user_agent=\"my_geocoder\")\n        location = geolocator.geocode(postcode)\n        if location:\n            return location.latitude, location.longitude\n        else:\n            return None, None\n    except GeocoderTimedOut:\n        return None, None\n\n# Apply the geocode function to get latitude and longitude\nmissing_latlong[[\n    'Location Latitude', 'Location Longitude']] = missing_latlong[\n    'Location Postal Code'].apply(lambda x: pd.Series(geocode_postcode(x)))\n# Check if any rows are still missing a Lat/Long\nmissing_latlong[missing_latlong[['Location Latitude', 'Location Longitude']].isnull().any(axis=1)]\n\n\n\n\n\n\nLocation ID\n\n\nCare homes beds\n\n\nLocation Latitude\n\n\nLocation Longitude\n\n\nLocation Postal Code\n\n\n\n\n\n\n15\n\n\n1-15659436285\n\n\n66.0\n\n\nNaN\n\n\nNaN\n\n\nNG24 3YT\n\n\n\n\n20\n\n\n1-18790374800\n\n\n3.0\n\n\nNaN\n\n\nNaN\n\n\nBL11 5PE\n\n\n\n\nAs there are so few postcodes for which the latitude and longitude could not be found, these will be ignored and dropped in the subsequent analysis.\n# Drop Postcode column ready for merging\nmissing_latlong.drop('Location Postal Code', axis=1, inplace=True)\n# Update the CQC_CareHomes dataframe with the additional lat/long data\nCQC_CareHomes = CQC_CareHomes.set_index('Location ID').combine_first(missing_latlong.set_index('Location ID')).reset_index()\n# Convert the Latitude and Longitude columns into Points\ngeometry = [Point(xy) for xy in zip(CQC_CareHomes['Location Longitude'], CQC_CareHomes['Location Latitude'])]\n\n# Convert CQC_CareHomes to a GeoDataFrame\nCQC_CareHomes = gpd.GeoDataFrame(CQC_CareHomes, geometry=geometry).set_crs('epsg:4326')\n\n\n2.3.3. Spatially match to LA geodataframe\n# Change CQC_CareHomes crs to match LA data\nCQC_CareHomes = CQC_CareHomes.to_crs(27700)\n# Spatially match CQC Care Homes to LA boundaries\nCareHomes_LAs = gpd.sjoin(CQC_CareHomes, LAs_Eng, predicate='within')\n# Check that there are only the missing locations as expected\nmissing = pd.merge(CQC_CareHomes, CareHomes_LAs, how='outer', suffixes=('','_y'), indicator=True)\nrows_in_df1_not_in_df2 = missing[missing['_merge']=='left_only'][CQC_CareHomes.columns]\n\nrows_in_df1_not_in_df2\n\n\n\n\n\n\nLocation ID\n\n\nCare homes beds\n\n\nLocation Latitude\n\n\nLocation Longitude\n\n\ngeometry\n\n\n\n\n\n\n6108\n\n\n1-15659436285\n\n\n66.0\n\n\nNaN\n\n\nNaN\n\n\nPOINT EMPTY\n\n\n\n\n6708\n\n\n1-18790374800\n\n\n3.0\n\n\nNaN\n\n\nNaN\n\n\nPOINT EMPTY\n\n\n\n\n# Group by 'CTYUA22CD' and aggregate\nCareHomes_LAs_agg = CareHomes_LAs.groupby('CTYUA22CD').agg({\n    'Location ID': 'count',\n    'Care homes beds': 'sum'\n}).reset_index()\n\n# Rename columns for clarity\nCareHomes_LAs_agg.columns = ['CTYUA22CD', 'Count of CareHome Locations', 'Sum of CareHome beds']\n# Check length of CareHomes_LAs_agg contains all 152 LAs\nprint(len(CareHomes_LAs_agg))\nprint(len(CareHomes_LAs_agg) == len(LAs_Eng))\n151 False\n# Check missing LAs\nmissing_LAs = pd.merge(LAs_Eng, CareHomes_LAs_agg, how='outer', suffixes=('','_y'), indicator=True)\nrows_missing = missing_LAs[missing_LAs['_merge']=='left_only'][LAs_Eng.columns]\n\nrows_missing\n\n\n\n\n\n\nCTYUA22CD\n\n\nCTYUA22NM\n\n\ngeometry\n\n\n\n\n\n\n95\n\n\nE09000001\n\n\nCity of London\n\n\nMULTIPOLYGON (((531664.450 180555.150, 531664….\n\n\n\n\nIt is unsurprising that City of London does not have any care home locations meeting the requirements to be included in this analysis. The City of London operates very differently to other LAs, with a small population and mostly serving as a business centre. This LA will be excluded from the analysis, which is handled in section 2.5 as part of the creation of the final dataframe for analysis.\n# Visual check of final dataframe\nCareHomes_LAs_agg.head()\n\n\n\n\n\n\nCTYUA22CD\n\n\nCount of CareHome Locations\n\n\nSum of CareHome beds\n\n\n\n\n\n\n0\n\n\nE06000001\n\n\n20\n\n\n884.0\n\n\n\n\n1\n\n\nE06000002\n\n\n30\n\n\n1498.0\n\n\n\n\n2\n\n\nE06000003\n\n\n30\n\n\n1111.0\n\n\n\n\n3\n\n\nE06000004\n\n\n41\n\n\n1830.0\n\n\n\n\n4\n\n\nE06000005\n\n\n23\n\n\n1069.0\n\n\n\n\n\n\n\n2.4. Childcare Providers (Early Years Services (EYS))\nThe Ofsted childcare providers data also needs to be filtered to isolate only locations required for this analysis. This includes only businesses that are on non-domestic premises (i.e. excludes childminders caring for children in their home), and only those that are registered to provide early years services. Early years services (EYS) are those for pre-school aged children, which are the focus of this analysis. Some of the EYS locations are missing latitude and longitude data, as well as postcode, due to data privacy issues. As there are so few affected by this issue, the locations will simply be excluded from the analysis. Again, the data will be aggregated to LA level. To aggregate the data, the EYS geodataframe is spatially matched to the LA geodataframe before calculating EYS location counts and total places by each LA.\n\n2.4.1. Clean childcare providers dataframe\n# Filter Ofsted data to only locations that meet given criteria for analysis\nEarlyYearsServices = Childcare_Providers[(\n    Childcare_Providers['Provider type'] == 'Childcare on non-domestic premises') &    # Keep only business locations\n    (Childcare_Providers['Provider Early Years Register Flag'] == 'Y')]      # Keep only locations registered for Early Years\n# Keep only columns required for analysis\nEarlyYearsServices_Trim = EarlyYearsServices[[\n    'Provider URN', 'Provider name', 'Postcode',\n    'Places including estimates', 'pcds', 'lat', 'long']].copy().reset_index(drop=True)\n\n\n2.4.2. Geocode the EYS data\n# Create a copy of the dataframe for geocoding\nEarlyYearsServices_Locations = EarlyYearsServices_Trim.copy()\n# Check for any missing lat/long information\nEarlyYearsServices_Locations[EarlyYearsServices_Locations[['lat', 'long']].isnull().any(axis=1)]\n\n\n\n\n\n\nProvider URN\n\n\nProvider name\n\n\nPostcode\n\n\nPlaces including estimates\n\n\npcds\n\n\nlat\n\n\nlong\n\n\n\n\n\n\n1104\n\n\nEY496734\n\n\nREDACTED\n\n\nREDACTED\n\n\n43.0\n\n\nNaN\n\n\nNaN\n\n\nNaN\n\n\n\n\n3523\n\n\nEY372207\n\n\nREDACTED\n\n\nREDACTED\n\n\n33.0\n\n\nNaN\n\n\nNaN\n\n\nNaN\n\n\n\n\n5674\n\n\nEY292960\n\n\nREDACTED\n\n\nREDACTED\n\n\n20.0\n\n\nNaN\n\n\nNaN\n\n\nNaN\n\n\n\n\n5918\n\n\n229140\n\n\nREDACTED\n\n\nREDACTED\n\n\n20.0\n\n\nNaN\n\n\nNaN\n\n\nNaN\n\n\n\n\n6195\n\n\nEY251447\n\n\nREDACTED\n\n\nREDACTED\n\n\n8.0\n\n\nNaN\n\n\nNaN\n\n\nNaN\n\n\n\n\n8581\n\n\nEY400304\n\n\nREDACTED\n\n\nREDACTED\n\n\n40.0\n\n\nNaN\n\n\nNaN\n\n\nNaN\n\n\n\n\n8635\n\n\nEY243486\n\n\nREDACTED\n\n\nREDACTED\n\n\n12.0\n\n\nNaN\n\n\nNaN\n\n\nNaN\n\n\n\n\n15395\n\n\nEY366430\n\n\nREDACTED\n\n\nREDACTED\n\n\n4.0\n\n\nNaN\n\n\nNaN\n\n\nNaN\n\n\n\n\nlen(EarlyYearsServices_Locations[EarlyYearsServices_Locations[['lat', 'long']].isnull().any(axis=1)])\n8\nAs there are only 8 locations with missing lat and long, these will be ignored. The issue is that their information is being redacted by Ofsted, likely due to privacy/GDPR issues.\n# Convert the Latitude and Longitude columns into Points\ngeometry = [Point(xy) for xy in zip(EarlyYearsServices_Locations['long'], EarlyYearsServices_Locations['lat'])]\n\n# Convert EarlyYearsServices_Locations to a GeoDataFrame\nEarlyYearsServices_Locations = gpd.GeoDataFrame(EarlyYearsServices_Locations, geometry=geometry).set_crs('epsg:4326')\n\n\n2.4.3. Spatially match to LA geodataframe\n# Change EarlyYearsServices_Locations crs to match other data\nEarlyYearsServices_Locations = EarlyYearsServices_Locations.to_crs(27700)\n# Spatially match EarlyYearsServices_Locations to LA boundaries\nEarlyYearsServices_LAs = gpd.sjoin(EarlyYearsServices_Locations, LAs_Eng, predicate='within')\n# Check that there are only the 8 missing locations as expected\nmissing_locations = pd.merge(\n    EarlyYearsServices_Locations, EarlyYearsServices_LAs, how='outer', suffixes=('','_y'), indicator=True)\n\nrows_in_df1_not_in_df2 = missing_locations[missing_locations['_merge']=='left_only'][EarlyYearsServices_Locations.columns]\n\nrows_in_df1_not_in_df2\n\n\n\n\n\n\nProvider URN\n\n\nProvider name\n\n\nPostcode\n\n\nPlaces including estimates\n\n\npcds\n\n\nlat\n\n\nlong\n\n\ngeometry\n\n\n\n\n\n\n2173\n\n\n229140\n\n\nREDACTED\n\n\nREDACTED\n\n\n20.0\n\n\nNaN\n\n\nNaN\n\n\nNaN\n\n\nPOINT EMPTY\n\n\n\n\n10782\n\n\nEY243486\n\n\nREDACTED\n\n\nREDACTED\n\n\n12.0\n\n\nNaN\n\n\nNaN\n\n\nNaN\n\n\nPOINT EMPTY\n\n\n\n\n10881\n\n\nEY251447\n\n\nREDACTED\n\n\nREDACTED\n\n\n8.0\n\n\nNaN\n\n\nNaN\n\n\nNaN\n\n\nPOINT EMPTY\n\n\n\n\n11801\n\n\nEY292960\n\n\nREDACTED\n\n\nREDACTED\n\n\n20.0\n\n\nNaN\n\n\nNaN\n\n\nNaN\n\n\nPOINT EMPTY\n\n\n\n\n13478\n\n\nEY366430\n\n\nREDACTED\n\n\nREDACTED\n\n\n4.0\n\n\nNaN\n\n\nNaN\n\n\nNaN\n\n\nPOINT EMPTY\n\n\n\n\n13665\n\n\nEY372207\n\n\nREDACTED\n\n\nREDACTED\n\n\n33.0\n\n\nNaN\n\n\nNaN\n\n\nNaN\n\n\nPOINT EMPTY\n\n\n\n\n14407\n\n\nEY400304\n\n\nREDACTED\n\n\nREDACTED\n\n\n40.0\n\n\nNaN\n\n\nNaN\n\n\nNaN\n\n\nPOINT EMPTY\n\n\n\n\n19331\n\n\nEY496734\n\n\nREDACTED\n\n\nREDACTED\n\n\n43.0\n\n\nNaN\n\n\nNaN\n\n\nNaN\n\n\nPOINT EMPTY\n\n\n\n\n# Group by 'CTYUA22CD' and aggregate\nEarlyYearsServices_LAs_agg = EarlyYearsServices_LAs.groupby('CTYUA22CD').agg({\n    'Provider URN': 'count',\n    'Places including estimates': 'sum'\n}).reset_index()\n\n# Rename columns for clarity\nEarlyYearsServices_LAs_agg.columns = ['CTYUA22CD', 'Count of EarlyYears Services', 'Sum of EarlyYears Places']\n# Check for any missing LAs\nmissing_LAs = pd.merge(LAs_Eng, EarlyYearsServices_LAs_agg, how='outer', suffixes=('','_y'), indicator=True)\nrows_missing = missing_LAs[missing_LAs['_merge']=='left_only'][LAs_Eng.columns]\n\nrows_missing\n\n\n\n\n\n\nCTYUA22CD\n\n\nCTYUA22NM\n\n\ngeometry\n\n\n\n\n\n\n# Visual check of final dataframe\nEarlyYearsServices_LAs_agg.head()\n\n\n\n\n\n\nCTYUA22CD\n\n\nCount of EarlyYears Services\n\n\nSum of EarlyYears Places\n\n\n\n\n\n\n0\n\n\nE06000001\n\n\n16\n\n\n940.43\n\n\n\n\n1\n\n\nE06000002\n\n\n35\n\n\n1975.00\n\n\n\n\n2\n\n\nE06000003\n\n\n29\n\n\n1395.00\n\n\n\n\n3\n\n\nE06000004\n\n\n52\n\n\n2922.00\n\n\n\n\n4\n\n\nE06000005\n\n\n31\n\n\n1619.00\n\n\n\n\n\n\n\n2.5. Combined dataframe\nThe wrangled dataframes must now be combined into a single dataframe for analysis. The issue with the City of London must also be addressed at this stage by removing it from the final dataframe for analysis.\n# Merge the wrangled dataframes together on CTYUA22CD\nanalysis_df = LAs_Eng.merge(\n    Age_final, on='CTYUA22CD', how='outer').merge(\n    CareHomes_LAs_agg, on='CTYUA22CD', how='outer').merge(\n    EarlyYearsServices_LAs_agg, on='CTYUA22CD', how='outer')\nThere was one LA (City of London) which did not have any care homes, and has a very low number of early years services as well as population in both focus age groups. The City of London operates very differntly to other county and unitary authorities and has a very small population, primarily operating as a financial district. From this point forwards, for the purposes of this analysis, City of London will be excluded to avoid any biases and incorrect interpretation of service provision in this area.\n# Visual check of City of London\nanalysis_df[analysis_df['CTYUA22CD'] == 'E09000001']\n\n\n\n\n\n\nCTYUA22CD\n\n\nCTYUA22NM\n\n\ngeometry\n\n\nAged 4 years and under\n\n\nAged 4 years and under PC\n\n\nAged 65 years and over\n\n\nAged 65 years and over PC\n\n\nCount of CareHome Locations\n\n\nSum of CareHome beds\n\n\nCount of EarlyYears Services\n\n\nSum of EarlyYears Places\n\n\n\n\n\n\n95\n\n\nE09000001\n\n\nCity of London\n\n\nMULTIPOLYGON (((531664.450 180555.150, 531664….\n\n\n213.0\n\n\n0.026329\n\n\n1209.0\n\n\n0.149444\n\n\nNaN\n\n\nNaN\n\n\n8\n\n\n474.0\n\n\n\n\n# Delete the City of London row\nanalysis_df = analysis_df[analysis_df['CTYUA22CD'] != 'E09000001']\n# Check length of analysis_df is now 151\nprint(len(analysis_df))\nprint(len(analysis_df) == 151)\n151 True\n# Visual check of final dataframe\nanalysis_df.head()\n\n\n\n\n\n\nCTYUA22CD\n\n\nCTYUA22NM\n\n\ngeometry\n\n\nAged 4 years and under\n\n\nAged 4 years and under PC\n\n\nAged 65 years and over\n\n\nAged 65 years and over PC\n\n\nCount of CareHome Locations\n\n\nSum of CareHome beds\n\n\nCount of EarlyYears Services\n\n\nSum of EarlyYears Places\n\n\n\n\n\n\n0\n\n\nE06000001\n\n\nHartlepool\n\n\nMULTIPOLYGON (((450154.599 525938.201, 450140….\n\n\n4982.0\n\n\n0.059003\n\n\n18233.0\n\n\n0.215936\n\n\n20.0\n\n\n884.0\n\n\n16\n\n\n940.43\n\n\n\n\n1\n\n\nE06000002\n\n\nMiddlesbrough\n\n\nMULTIPOLYGON (((446854.700 517192.700, 446854….\n\n\n8928.0\n\n\n0.066919\n\n\n24178.0\n\n\n0.181223\n\n\n30.0\n\n\n1498.0\n\n\n35\n\n\n1975.00\n\n\n\n\n2\n\n\nE06000003\n\n\nRedcar and Cleveland\n\n\nMULTIPOLYGON (((451747.397 520561.100, 451792….\n\n\n6814.0\n\n\n0.055873\n\n\n31747.0\n\n\n0.260315\n\n\n30.0\n\n\n1111.0\n\n\n29\n\n\n1395.00\n\n\n\n\n3\n\n\nE06000004\n\n\nStockton-on-Tees\n\n\nMULTIPOLYGON (((447177.704 517811.797, 447176….\n\n\n10733.0\n\n\n0.059477\n\n\n37184.0\n\n\n0.206056\n\n\n41.0\n\n\n1830.0\n\n\n52\n\n\n2922.00\n\n\n\n\n4\n\n\nE06000005\n\n\nDarlington\n\n\nPOLYGON ((423496.602 524724.299, 423497.204 52…\n\n\n5496.0\n\n\n0.056254\n\n\n22040.0\n\n\n0.225591\n\n\n23.0\n\n\n1069.0\n\n\n31\n\n\n1619.00\n\n\n\n\n\n\n2.6. Feature engineering\nThe final step in the wrangling process is to engineer the additional features which will be used in the analysis. This includes measures that illustrate provision in each LA. To do this, the ratio of people to each relevant service (including locations and beds/places) is used. This gives a view of, for example, how many children there are per each EYS place in a given LA. Compared across LAs, this then gives a view of provision and whether more services are needed.\nAs a K-means clustering model will be created, and in order to compare features in the analysis, all features must be standardised to ensure comparability. This is achieved by calculating Z-scores. Calculating Z-scores gives a mean of 0 and standard deviation of 1 for all features, highlighting where datapoints deviate from the mean and by how much. The features have normal distributions (with some tails due to outliers) as shown in Figure 4.1.2. later in this notebook, making Z-scores a suitable standardisation method.\n# Make a copy of the analysis_df\nfinal_df = analysis_df.copy()\n# To keep the code tidier, rename the columns to shorthand abbreviations\n# Create a dictionary for renaming columns\nrename_dict = {\n    'CTYUA22CD': 'CTYUA22CD',\n    'CTYUA22NM': 'CTYUA22NM',\n    'geometry': 'geometry',\n    'Aged 4 years and under': 'U4',\n    'Aged 4 years and under PC': '%U4',\n    'Aged 65 years and over': 'O65',\n    'Aged 65 years and over PC': '%O65',\n    'Count of CareHome Locations': 'CH Locs',\n    'Sum of CareHome beds': 'CH Beds',\n    'Count of EarlyYears Services': 'EYS Locs',\n    'Sum of EarlyYears Places': 'EYS Places'\n}\n\n# Rename columns using the dictionary\nfinal_df = final_df.rename(columns=rename_dict)\n\n2.6.1. Calculate ratios\n# Calculate ratio of U4 to EYS Locs\nfinal_df['U4:EYS Locs'] = final_df['U4'] / final_df['EYS Locs']\n# Calculate ratio of U4 to EYS Places\nfinal_df['U4:EYS Places'] = final_df['U4'] / final_df['EYS Places']\n# Calculate ratio of O65 to CH Locs\nfinal_df['O65:CH Locs'] = final_df['O65'] / final_df['CH Locs']\n# Calculate ratio of O65 to CH Beds\nfinal_df['O65:CH Beds'] = final_df['O65'] / final_df['CH Beds']\n\n\n2.6.2. Calculate Z-Scores\n# Calculate z-score for U4\nfinal_df['U4z'] = stats.zscore(final_df['U4'])\n# Calculate z-score for O65\nfinal_df['O65z'] = stats.zscore(final_df['O65'])\n# Calculate z-score for %U4\nfinal_df['%U4z'] = stats.zscore(final_df['%U4'])\n# Calculate z-score for %O65\nfinal_df['%O65z'] = stats.zscore(final_df['%O65'])\n# Calculate z-score for U4:EYS Locs\nfinal_df['U4:EYS Locsz'] = stats.zscore(final_df['U4:EYS Locs'])\n# Calculate z-score for U4:EYS Places\nfinal_df['U4:EYS Placesz'] = stats.zscore(final_df['U4:EYS Places'])\n# Calculate z-score for O65:CH Locs\nfinal_df['O65:CH Locsz'] = stats.zscore(final_df['O65:CH Locs'])\n# Calculate z-score for O65:CH Beds\nfinal_df['O65:CH Bedsz'] = stats.zscore(final_df['O65:CH Beds'])\n# Visual check of final dataframe\nfinal_df.head()\n\n\n\n\n\n\nCTYUA22CD\n\n\nCTYUA22NM\n\n\ngeometry\n\n\nU4\n\n\n%U4\n\n\nO65\n\n\n%O65\n\n\nCH Locs\n\n\nCH Beds\n\n\nEYS Locs\n\n\n…\n\n\nO65:CH Locs\n\n\nO65:CH Beds\n\n\nU4z\n\n\nO65z\n\n\n%U4z\n\n\n%O65z\n\n\nU4:EYS Locsz\n\n\nU4:EYS Placesz\n\n\nO65:CH Locsz\n\n\nO65:CH Bedsz\n\n\n\n\n\n\n0\n\n\nE06000001\n\n\nHartlepool\n\n\nMULTIPOLYGON (((450154.599 525938.201, 450140….\n\n\n4982.0\n\n\n0.059003\n\n\n18233.0\n\n\n0.215936\n\n\n20.0\n\n\n884.0\n\n\n16\n\n\n…\n\n\n911.650000\n\n\n20.625566\n\n\n-1.027892\n\n\n-0.819890\n\n\n-0.136987\n\n\n0.370894\n\n\n3.117648\n\n\n2.429352\n\n\n-0.418458\n\n\n-0.688230\n\n\n\n\n1\n\n\nE06000002\n\n\nMiddlesbrough\n\n\nMULTIPOLYGON (((446854.700 517192.700, 446854….\n\n\n8928.0\n\n\n0.066919\n\n\n24178.0\n\n\n0.181223\n\n\n30.0\n\n\n1498.0\n\n\n35\n\n\n…\n\n\n805.933333\n\n\n16.140187\n\n\n-0.764405\n\n\n-0.723640\n\n\n0.999636\n\n\n-0.221698\n\n\n2.000264\n\n\n1.570449\n\n\n-0.621618\n\n\n-1.069629\n\n\n\n\n2\n\n\nE06000003\n\n\nRedcar and Cleveland\n\n\nMULTIPOLYGON (((451747.397 520561.100, 451792….\n\n\n6814.0\n\n\n0.055873\n\n\n31747.0\n\n\n0.260315\n\n\n30.0\n\n\n1111.0\n\n\n29\n\n\n…\n\n\n1058.233333\n\n\n28.575158\n\n\n-0.905563\n\n\n-0.601098\n\n\n-0.586407\n\n\n1.128486\n\n\n1.600863\n\n\n1.972871\n\n\n-0.136764\n\n\n-0.012264\n\n\n\n\n3\n\n\nE06000004\n\n\nStockton-on-Tees\n\n\nMULTIPOLYGON (((447177.704 517811.797, 447176….\n\n\n10733.0\n\n\n0.059477\n\n\n37184.0\n\n\n0.206056\n\n\n41.0\n\n\n1830.0\n\n\n52\n\n\n…\n\n\n906.926829\n\n\n20.319126\n\n\n-0.643879\n\n\n-0.513073\n\n\n-0.068850\n\n\n0.202227\n\n\n1.033892\n\n\n0.633879\n\n\n-0.427535\n\n\n-0.714288\n\n\n\n\n4\n\n\nE06000005\n\n\nDarlington\n\n\nPOLYGON ((423496.602 524724.299, 423497.204 52…\n\n\n5496.0\n\n\n0.056254\n\n\n22040.0\n\n\n0.225591\n\n\n23.0\n\n\n1069.0\n\n\n31\n\n\n…\n\n\n958.260870\n\n\n20.617399\n\n\n-0.993571\n\n\n-0.758254\n\n\n-0.531585\n\n\n0.535709\n\n\n0.455967\n\n\n0.326072\n\n\n-0.328885\n\n\n-0.688925\n\n\n\n\n\n5 rows × 23 columns"
  },
  {
    "objectID": "projects/intergenerational_care_facilities/index.html#feature-engineering",
    "href": "projects/intergenerational_care_facilities/index.html#feature-engineering",
    "title": "Identifying potential priority local authorities in England for intergenerational care facilities",
    "section": "Feature engineering",
    "text": "Feature engineering\n\nAggregate EV car ownership data to LAD\n# Aggregate to LAD and calculate total number of PiVs\nPiVs_LAD = PiVs_FILTERED.groupby('LAD22CD')['2023 Q4'].agg('sum').reset_index()\n\n# Rename column to PiVs\nPiVs_LAD.rename(columns={'2023 Q4': 'PiVs'}, inplace=True)\n# Visual check\nPiVs_LAD.head()\n\n\n\n\n\n\nLAD22CD\n\n\nPiVs\n\n\n\n\n\n\n0\n\n\nE06000001\n\n\n410.0\n\n\n\n\n1\n\n\nE06000002\n\n\n391.0\n\n\n\n\n2\n\n\nE06000003\n\n\n580.0\n\n\n\n\n3\n\n\nE06000004\n\n\n1308.0\n\n\n\n\n4\n\n\nE06000005\n\n\n719.0\n\n\n\n\nlen(PiVs_LAD)\n331\n\n\nCalculate distance to nearest chargepoint\n\nUsing the OA PWCs, calculate the nearest EV chargepoint.\nAggregate this to LAD level to get the average distance to the nearest chargepoint by LAD.\n\n# Find the nearest chargepoint to each OA PWC and calculate distance in metres\nOA_nearest_chargepoint = OA_PWC.sjoin_nearest(chargepoints_gdf, distance_col='distance', how='left')\n# Merge on OA_lookup_Leeds to get the LAD22CDs\nOA_nearest_chargepoint = pd.merge(OA_nearest_chargepoint, OA_lookup, on='OA21CD', how='inner')\n# Keep only required columns\nOA_nearest_chargepoint = OA_nearest_chargepoint[['OA21CD','geometry','distance','LAD22CD']].copy()\n# Aggregate to LAD and calculate average distance to nearest chargepoint\navg_dist_nearest_chargepoint_LAD = OA_nearest_chargepoint.groupby('LAD22CD')['distance'].agg('mean').reset_index()\n\n# Rename column to avg_distance\navg_dist_nearest_chargepoint_LAD.rename(columns={'distance': 'avg_distance'}, inplace=True)\n# Visual check\navg_dist_nearest_chargepoint_LAD.head()\n\n\n\n\n\n\nLAD22CD\n\n\navg_distance\n\n\n\n\n\n\n0\n\n\nE06000001\n\n\n1649.498055\n\n\n\n\n1\n\n\nE06000002\n\n\n1088.305461\n\n\n\n\n2\n\n\nE06000003\n\n\n1454.585351\n\n\n\n\n3\n\n\nE06000004\n\n\n911.259807\n\n\n\n\n4\n\n\nE06000005\n\n\n1188.923394\n\n\n\n\nlen(avg_dist_nearest_chargepoint_LAD)\n331\n\n\nCalculate number of EV chargepoints in each LAD\n\nAggregate the EV chargepoint data to LAD level to get total number of chargepoints in each LAD.\n\n# Spatially match\nchargepoints_LAD = gpd.sjoin(LADs, chargepoints_gdf, predicate='intersects')\n# Aggregate to LAD and calculate total number of chargepoints\nchargepoints_LAD = chargepoints_LAD.groupby('LAD22CD')['index_right'].agg('count').reset_index()\n\n# Rename column to total_chargepoints\nchargepoints_LAD.rename(columns={'index_right': 'total_chargepoints'}, inplace=True)\n# Visual check\nchargepoints_LAD.head()\n\n\n\n\n\n\nLAD22CD\n\n\ntotal_chargepoints\n\n\n\n\n\n\n0\n\n\nE06000001\n\n\n42\n\n\n\n\n1\n\n\nE06000002\n\n\n51\n\n\n\n\n2\n\n\nE06000003\n\n\n46\n\n\n\n\n3\n\n\nE06000004\n\n\n167\n\n\n\n\n4\n\n\nE06000005\n\n\n76\n\n\n\n\nlen(chargepoints_LAD)\n373\n\n\nCalculate ratio of EV cars to chargepoints in each LAD\n# Merge PiVs and chargepoint count dataframes\nratio_PiVs_to_chargepoints = pd.merge(PiVs_LAD, chargepoints_LAD, on='LAD22CD', how='left')\n\n# Replace NaN values with 0 where there are no chargepoints in an MSOA\nratio_PiVs_to_chargepoints.fillna({'total_chargepoints': 0}, inplace=True)\n# Calculate ratio\nratio_PiVs_to_chargepoints['ratio_PiVs_to_chargepoints'] = (\n    ratio_PiVs_to_chargepoints['PiVs'] / ratio_PiVs_to_chargepoints['total_chargepoints'])\n\n# Replace 'inf' with NaN\nratio_PiVs_to_chargepoints['ratio_PiVs_to_chargepoints'] = ratio_PiVs_to_chargepoints[\n    'ratio_PiVs_to_chargepoints'].replace([np.inf, -np.inf], np.nan)\n\n\nAdd average distance and LAD polygon to final dataframe\n# Add nearest chargepoint\nfinal_df = pd.merge(ratio_PiVs_to_chargepoints,avg_dist_nearest_chargepoint_LAD, on='LAD22CD')\n\n# Add LAD polygons\nfinal_df = pd.merge(LADs, final_df, on='LAD22CD')\n# Visual check\nfinal_df.head()\n\n\n\n\n\n\nLAD22CD\n\n\nLAD22NM\n\n\ngeometry\n\n\nPiVs\n\n\ntotal_chargepoints\n\n\nratio_PiVs_to_chargepoints\n\n\navg_distance\n\n\n\n\n\n\n0\n\n\nE06000001\n\n\nHartlepool\n\n\nMULTIPOLYGON (((450154.599 525938.201, 450140….\n\n\n410.0\n\n\n42.0\n\n\n9.761905\n\n\n1649.498055\n\n\n\n\n1\n\n\nE06000002\n\n\nMiddlesbrough\n\n\nMULTIPOLYGON (((446854.700 517192.700, 446854….\n\n\n391.0\n\n\n51.0\n\n\n7.666667\n\n\n1088.305461\n\n\n\n\n2\n\n\nE06000003\n\n\nRedcar and Cleveland\n\n\nMULTIPOLYGON (((451747.397 520561.100, 451792….\n\n\n580.0\n\n\n46.0\n\n\n12.608696\n\n\n1454.585351\n\n\n\n\n3\n\n\nE06000004\n\n\nStockton-on-Tees\n\n\nMULTIPOLYGON (((447177.704 517811.797, 447176….\n\n\n1308.0\n\n\n167.0\n\n\n7.832335\n\n\n911.259807\n\n\n\n\n4\n\n\nE06000005\n\n\nDarlington\n\n\nPOLYGON ((423496.602 524724.299, 423497.204 52…\n\n\n719.0\n\n\n76.0\n\n\n9.460526\n\n\n1188.923394\n\n\n\n\n\n\nHandle outliers\nThere are some outliers on the top end, so anything outside 2sd will be amended.\n# Loop through each variable and calculate mean, std, and threshold\nfor variable in ['PiVs', 'total_chargepoints', 'ratio_PiVs_to_chargepoints', 'avg_distance']:\n    mean_var = final_df[variable].mean()\n    std_var = final_df[variable].std()\n    threshold_var = mean_var + 2 * std_var\n    \n    # Create new column based on outlier condition\n    final_df[f'amended_{variable}'] = final_df[variable].apply(lambda x: threshold_var if x &gt; threshold_var else x)\n# Visual check\nfinal_df.head()\n\n\n\n\n\n\nLAD22CD\n\n\nLAD22NM\n\n\ngeometry\n\n\nPiVs\n\n\ntotal_chargepoints\n\n\nratio_PiVs_to_chargepoints\n\n\navg_distance\n\n\namended_PiVs\n\n\namended_total_chargepoints\n\n\namended_ratio_PiVs_to_chargepoints\n\n\namended_avg_distance\n\n\n\n\n\n\n0\n\n\nE06000001\n\n\nHartlepool\n\n\nMULTIPOLYGON (((450154.599 525938.201, 450140….\n\n\n410.0\n\n\n42.0\n\n\n9.761905\n\n\n1649.498055\n\n\n410.0\n\n\n42.0\n\n\n9.761905\n\n\n1649.498055\n\n\n\n\n1\n\n\nE06000002\n\n\nMiddlesbrough\n\n\nMULTIPOLYGON (((446854.700 517192.700, 446854….\n\n\n391.0\n\n\n51.0\n\n\n7.666667\n\n\n1088.305461\n\n\n391.0\n\n\n51.0\n\n\n7.666667\n\n\n1088.305461\n\n\n\n\n2\n\n\nE06000003\n\n\nRedcar and Cleveland\n\n\nMULTIPOLYGON (((451747.397 520561.100, 451792….\n\n\n580.0\n\n\n46.0\n\n\n12.608696\n\n\n1454.585351\n\n\n580.0\n\n\n46.0\n\n\n12.608696\n\n\n1454.585351\n\n\n\n\n3\n\n\nE06000004\n\n\nStockton-on-Tees\n\n\nMULTIPOLYGON (((447177.704 517811.797, 447176….\n\n\n1308.0\n\n\n167.0\n\n\n7.832335\n\n\n911.259807\n\n\n1308.0\n\n\n167.0\n\n\n7.832335\n\n\n911.259807\n\n\n\n\n4\n\n\nE06000005\n\n\nDarlington\n\n\nPOLYGON ((423496.602 524724.299, 423497.204 52…\n\n\n719.0\n\n\n76.0\n\n\n9.460526\n\n\n1188.923394\n\n\n719.0\n\n\n76.0\n\n\n9.460526\n\n\n1188.923394"
  },
  {
    "objectID": "projects/intergenerational_care_facilities/index.html#visualising-data",
    "href": "projects/intergenerational_care_facilities/index.html#visualising-data",
    "title": "Identifying potential priority local authorities in England for intergenerational care facilities",
    "section": "Visualising data",
    "text": "Visualising data\nThe following code builds an interactive map of the features in final_df. This is saved down into an Outputs/ folder as an HTML file.\n# Create the individual layers\nm = final_df.explore(\n    column=\"amended_PiVs\",\n    scheme=\"naturalbreaks\",\n    cmap='YlOrRd',\n    legend=False,\n    k=10,\n    tooltip=[\"LAD22CD\", \"LAD22NM\", \"amended_PiVs\"],\n    style_kwds=dict(color='black', weight=0.5, fillOpacity=0.8),\n    highlight_kwds=dict(fillOpacity=1),\n    name=\"Plug-in Vehicles\",\n    show=True)\n\nfinal_df.explore(\n    m=m,\n    column=\"amended_total_chargepoints\",\n    scheme=\"naturalbreaks\",\n    cmap='YlOrRd',\n    legend=False,\n    k=10,\n    tooltip=[\"LAD22CD\", \"LAD22NM\", \"amended_total_chargepoints\"],\n    style_kwds=dict(color='black', weight=0.5, fillOpacity=0.8),\n    highlight_kwds=dict(fillOpacity=1),\n    name=\"Total EV Chargepoints\",\n    show=False)\n\nfinal_df.explore(\n    m=m,\n    column=\"amended_ratio_PiVs_to_chargepoints\",\n    scheme=\"naturalbreaks\",\n    cmap='YlOrRd',\n    legend=False,\n    k=10,\n    tooltip=[\"LAD22CD\", \"LAD22NM\", \"amended_ratio_PiVs_to_chargepoints\"],\n    style_kwds=dict(color='black', weight=0.5, fillOpacity=0.8),\n    highlight_kwds=dict(fillOpacity=1),\n    name=\"Ratio of Plug-in Vehicles to EV Chargepoints\",\n    show=False)\n\nfinal_df.explore(\n    m=m,\n    column=\"amended_avg_distance\",\n    scheme=\"naturalbreaks\",\n    cmap='YlOrRd',\n    legend=False,\n    k=10,\n    tooltip=[\"LAD22CD\", \"LAD22NM\", \"amended_avg_distance\"],\n    style_kwds=dict(color='black', weight=0.5, fillOpacity=0.8),\n    highlight_kwds=dict(fillOpacity=1),\n    name=\"Average distance to nearest EV Chargepoint by OA\",\n    show=False)\n\n# Add the map base\nfolium.TileLayer(\"CartoDB positron\", show=True).add_to(m)\n\n# Add layer control to map\nfolium.LayerControl().add_to(m)\n\n# Save the map to an HTML file\nm.save(\"Outputs/Interactive_map.html\")\nThe following code builds four choropleth maps of the features in final_df. This is saved down into an Outputs/ folder as a .png file.\n# Create 2x2 subplots\nfig, axs = plt.subplots(2, 2, figsize=(12, 12))\n\n# Define aliases for column names\ncolumn_aliases = {\n    'amended_PiVs': 'Plug-in Vehicles (PiVs)',\n    'amended_total_chargepoints': 'Total EV Chargepoints (EVCs)',\n    'amended_ratio_PiVs_to_chargepoints': 'Ratio of PiVs to EVCs',\n    'amended_avg_distance': 'Average distance to nearest EVC by OA (metres)'\n}\n\n# Loop through each variable and corresponding subplot\nvariables = list(column_aliases.keys())\nfor variable, ax in zip(variables, axs.flatten()):\n    final_df.plot(column=variable, cmap='YlOrRd', ax=ax, legend=True)\n    ax.set_title(f'{column_aliases[variable]}')  # Set the title using the alias\n    ax.set_axis_off()  # Turn off axis\n    \nplt.subplots_adjust(wspace=0.05, hspace=0.05)  # Adjust space between subplots\nplt.tight_layout()  # Adjust layout to prevent overlap\n\n# Export to Outputs/ folder\nplt.savefig('Outputs/Feature_choropleths.png', dpi=300)\n\n# Close figure\nplt.close(fig)\nThe following code creates two scatterplots with regression lines for selected features from the final_df. This is saved down into an Outputs/ folder as a .png file.\n# Create a subplot grid with 1 row and 2 columns\nfig, axs = plt.subplots(1, 2, figsize=(15, 6))\n\n# Plot the first scatter plot\nsns.scatterplot(x='amended_PiVs', y='amended_total_chargepoints', data=final_df, ax=axs[0])\nsns.regplot(x='amended_PiVs', y='amended_total_chargepoints', data=final_df, scatter=False, ax=axs[0])\naxs[0].set_xlabel(column_aliases['amended_PiVs'])  # Set x-axis label using alias\naxs[0].set_ylabel(column_aliases['amended_total_chargepoints'])  # Set y-axis label using alias\naxs[0].set_title('Plug-in Vehicles vs. Total EV Chargepoints')  # Set the title\n\n# Plot the second scatter plot\nsns.scatterplot(x='amended_ratio_PiVs_to_chargepoints', y='amended_avg_distance', data=final_df, ax=axs[1])\nsns.regplot(x='amended_ratio_PiVs_to_chargepoints', y='amended_avg_distance', data=final_df, scatter=False, ax=axs[1])\naxs[1].set_xlabel(column_aliases['amended_ratio_PiVs_to_chargepoints'])  # Set x-axis label using alias\naxs[1].set_ylabel(column_aliases['amended_avg_distance'])  # Set y-axis label using alias\naxs[1].set_title('Ratio vs. Distance')  # Set the title\n\n# Adjust layout\nplt.tight_layout()\n\n# Export to Outputs/ folder\nplt.savefig('Outputs/Scatter_Reg_Plots.png', dpi=300)\n\n# Close figure\nplt.close(fig)\nThe following code creates a correlation matrix heatmap of the features in final_df. This is saved down into an Outputs/ folder as a .png file.\n# Calculate the correlation matrix\ncorrelation_matrix = final_df[[\n    'amended_PiVs','amended_total_chargepoints',\n    'amended_ratio_PiVs_to_chargepoints','amended_avg_distance']].corr()\n\n# Generate a mask for the diagonal cells\nmask = np.triu(np.ones_like(correlation_matrix, dtype=bool))\n\n# Generate a heatmap\nplt.figure(figsize=(8, 7))\nsns.heatmap(correlation_matrix, cmap='RdYlGn', annot=True, \n            fmt=\".2f\", mask=mask, vmin=-1, vmax=1, cbar_kws={\"shrink\": 0.75}, linewidth=.5)\n\n# Rotate axis labels\nplt.xticks(rotation=45)\nplt.yticks(rotation=45)\n\n# Set axis labels using aliases and wrap text\nplt.xticks(ticks=range(len(correlation_matrix.columns)), \n           labels=['Plug-in Vehicles', \n                   'Total EV Chargepoints', \n                   'Ratio of PiVs to EVCs', \n                   'Avg dist to nearest EVC'])\n\nplt.yticks(ticks=np.arange(len(correlation_matrix.columns))+0.5, \n           labels=['Plug-in Vehicles', \n                   'Total EV Chargepoints', \n                   'Ratio of PiVs to EVCs', \n                   'Avg dist to nearest EVC'])\n\nplt.title('Correlation Matrix Heatmap', fontweight='bold')\n\nplt.tight_layout()\n\n# Export to Outputs/ folder\nplt.savefig('Outputs/CorrelationMatrix.png', dpi=300)\n\n# Close figure\nplt.close()"
  },
  {
    "objectID": "projects/intergenerational_care_facilities/index.html#creating-a-ranking",
    "href": "projects/intergenerational_care_facilities/index.html#creating-a-ranking",
    "title": "Identifying potential priority local authorities in England for intergenerational care facilities",
    "section": "Creating a ranking",
    "text": "Creating a ranking\nThe following code creates a simple ranking of LADs based on:\n\nAverage distance to the nearest EV chargepoint\nRatio of Plug-in vehicles to EV chargepoints\n\nThe idea is that if the average distance is high, and the ratio is high, then these areas may be lacking public EV infrastructure.\n# Create a new dataframe containing just the amended features for the ranking\nranking = final_df.drop(final_df.columns[3:9], axis=1)\n# Rank the values in each column\nranking['rank_ratio'] = ranking['amended_ratio_PiVs_to_chargepoints'].rank(ascending=True, method='min')\nranking['rank_distance'] = ranking['amended_avg_distance'].rank(ascending=True, method='min')\n# Compute the combined ranking\nranking['combined_rank'] = (ranking['rank_ratio'] + ranking['rank_distance']) / 2\nranking['combined_rank'] = ranking['combined_rank'].rank(ascending=True, method='min')\nThe following code creates a map of the top 10 combined_rank LADs in ranking. This is saved down into an Outputs/ folder as a .png file.\n# Get top 10 areas\nranking[['LAD22CD','LAD22NM','combined_rank']].sort_values(by='combined_rank', ascending=True).head(10)\n\n\n\n\n\n\nLAD22CD\n\n\nLAD22NM\n\n\ncombined_rank\n\n\n\n\n\n\n288\n\n\nE09000013\n\n\nHammersmith and Fulham\n\n\n1.0\n\n\n\n\n308\n\n\nE09000033\n\n\nWestminster\n\n\n2.0\n\n\n\n\n303\n\n\nE09000028\n\n\nSouthwark\n\n\n3.0\n\n\n\n\n276\n\n\nE09000001\n\n\nCity of London\n\n\n4.0\n\n\n\n\n295\n\n\nE09000020\n\n\nKensington and Chelsea\n\n\n4.0\n\n\n\n\n299\n\n\nE09000024\n\n\nMerton\n\n\n6.0\n\n\n\n\n307\n\n\nE09000032\n\n\nWandsworth\n\n\n6.0\n\n\n\n\n282\n\n\nE09000007\n\n\nCamden\n\n\n8.0\n\n\n\n\n287\n\n\nE09000012\n\n\nHackney\n\n\n9.0\n\n\n\n\n294\n\n\nE09000019\n\n\nIslington\n\n\n9.0\n\n\n\n\n# Sort and select the top 10 areas\ntop_10_areas = ranking.sort_values(by='combined_rank', ascending=True).head(10)\n\n# Plot only the top 10 areas\nfig, ax = plt.subplots(1, 1, figsize=(8, 10))\ntop_10_areas.plot(ax=ax, color='#88C88E', edgecolor='black', linewidth=1, legend=True, alpha=0.5)\n\n# Annotate the top 10 areas with their rank\nfor idx, row in top_10_areas.iterrows():\n    plt.annotate(text=int(row['combined_rank']), xy=(row.geometry.centroid.x, row.geometry.centroid.y),\n                 horizontalalignment='center', fontsize=12, weight='bold', color='black')\n\n# Plot basemap\nctx.add_basemap(ax, crs=top_10_areas.crs.to_string(), source=ctx.providers.CartoDB.Positron)\n\n# Set title and remove axis\nax.set_title('Top 10 Areas by Combined Rank', fontsize=12, fontweight='bold')\nax.set_axis_off()\n\n# Adjust layout\nplt.tight_layout()\n\n# Export to Outputs/ folder\nplt.savefig('Outputs/Map_of_Top_10.png', dpi=300)\n\n# Close figure\nplt.close(fig)\nThe following code creates a map of the bottom 10 combined_rank LADs in ranking. This is saved down into an Outputs/ folder as a .png file.\n# Show bottom 10 areas\nranking[['LAD22CD','LAD22NM','combined_rank']].sort_values(by='combined_rank', ascending=False).head(10)\n\n\n\n\n\n\nLAD22CD\n\n\nLAD22NM\n\n\ncombined_rank\n\n\n\n\n\n\n99\n\n\nE07000074\n\n\nMaldon\n\n\n330.0\n\n\n\n\n89\n\n\nE07000064\n\n\nRother\n\n\n329.0\n\n\n\n\n235\n\n\nE07000242\n\n\nEast Hertfordshire\n\n\n328.0\n\n\n\n\n16\n\n\nE06000017\n\n\nRutland\n\n\n327.0\n\n\n\n\n100\n\n\nE07000075\n\n\nRochford\n\n\n326.0\n\n\n\n\n85\n\n\nE07000047\n\n\nWest Devon\n\n\n325.0\n\n\n\n\n177\n\n\nE07000169\n\n\nSelby\n\n\n324.0\n\n\n\n\n160\n\n\nE07000139\n\n\nNorth Kesteven\n\n\n323.0\n\n\n\n\n82\n\n\nE07000044\n\n\nSouth Hams\n\n\n321.0\n\n\n\n\n199\n\n\nE07000198\n\n\nStaffordshire Moorlands\n\n\n321.0\n\n\n\n\n# Sort and select the bottom 10 areas\nbottom_10_areas = ranking.sort_values(by='combined_rank', ascending=False).head(10)\n\n# Plot only the top 10 areas\nfig, ax = plt.subplots(1, 1, figsize=(10, 10))\nbottom_10_areas.plot(ax=ax, color='#DE6464', edgecolor='black', linewidth=1, legend=True, alpha=0.5)\n\n# Annotate the top 10 areas with their rank\nfor idx, row in bottom_10_areas.iterrows():\n    plt.annotate(text=int(row['combined_rank']), xy=(row.geometry.centroid.x, row.geometry.centroid.y),\n                 horizontalalignment='center', fontsize=12, weight='bold', color='black')\n\n# Plot basemap\nctx.add_basemap(ax, crs=bottom_10_areas.crs.to_string(), source=ctx.providers.CartoDB.Positron)\n\n# Set title and remove axis\nax.set_title('Bottom 10 Areas by Combined Rank', fontsize=12, fontweight='bold')\nax.set_axis_off()\n\n# Adjust layout\nplt.tight_layout()\n\n# Export to Outputs/ folder\nplt.savefig('Outputs/Map_of_Bottom_10.png', dpi=300)\n\n# Close figure\nplt.close(fig)"
  },
  {
    "objectID": "projects/intergenerational_care_facilities/index.html#accompanying-policy-briefing",
    "href": "projects/intergenerational_care_facilities/index.html#accompanying-policy-briefing",
    "title": "Identifying potential priority local authorities in England for intergenerational care facilities",
    "section": "Accompanying Policy Briefing",
    "text": "Accompanying Policy Briefing\nThis notebook contains the detailed analysis for this project. An accompanying policy briefing was written to discuss the findings in context.\nYou can read this policy briefing here."
  },
  {
    "objectID": "projects/intergenerational_care_facilities/index.html#data",
    "href": "projects/intergenerational_care_facilities/index.html#data",
    "title": "Identifying potential priority local authorities in England for intergenerational care facilities",
    "section": "1. Data",
    "text": "1. Data\n\n1.1. Import libraries\n# Data importing / wrangling\nimport pandas as pd\nimport geopandas as gpd\nfrom shapely.geometry import Point\nfrom geopy.geocoders import Nominatim\nfrom geopy.exc import GeocoderTimedOut\nfrom scipy import stats\nimport numpy as np\n\n# Visualisations\nimport matplotlib.pyplot as plt\nimport matplotlib.colors as clrs\nimport seaborn as sns\nimport folium\nimport contextily as cx\n\n# Clustering\nfrom sklearn.cluster import KMeans\nimport sklearn.metrics as metrics\nimport statsmodels.api as sm\nfrom statsmodels.formula.api import ols\nfrom tabulate import tabulate\n\n\n1.2. Import raw datafiles\nEach datafile is imported in this section. The full reference and link to the raw data is provided in the reference list at the end of this Notebook, with the code stating which reference refers to which datafile.\nSome datafiles, notably shapefiles, are too large to be included in this repository. The raw files can be accessed using the links provided to download locally.\n\n1.2.1. Counties and Unitary Authorities\nCounties and Unitary Authorities are referred to by the Office for National Statistics (ONS) as “Upper-Tier Local Authorities”. These change periodically, but for consistency across data sources the December 2022 boundaries will be used which align to the 2021 Census as they remained unchanged in this period. According to the ONS (2023a): “There are 152 upper tier local authorities in England made up of 59 unitary authorities, 36 metropolitan districts, 33 London boroughs (including City of London) and 24 counties”.\nThese will be referred to as local authorities (LAs) for ease within this Notebook.\n# ONS Counties and Unitary Authority boundaries, Dec 2022, polygons (ONS, 2023b)\nLAs = gpd.read_file('Data/Counties_and_Unitary_Authorities_December_2022_UK_BFC_-4274932253738652752/CTYUA_DEC_2022_UK_BFC.shp')\n\n\n1.2.2. Census Age by LA\nCensus Age by LA can be obtained via a custom ONS query (ONS, 2023c). This query includes only LAs in England and selects nine age groups to ensure that the under 4 and over 65 age groups for analysis can be created. The output file from the query contains the number of people in each given age group, which can then be transformed to create and isolate the under 4 and over 65 categories (see Section 2.2).\n# Census 2021, Age by LA (ONS, 2023c)\nAge = pd.read_csv('Data/Census-England-UpperTierLocalAuthorities-Age.csv')\n\n\n1.2.3. CQC Directory\nThe Care Quality Commission (CQC) care directory with filters (CQC, 2024) contains information about all active locations registered with CQC, including care home bed numbers, regulated activities, service types and service users. The file is updated monthly, with the file used in this analysis being the March 2024 version of the data. This can be accessed from the archive available on the CQC website (2024) alongside the latest file.\n# CQC Care directory with filters as at 01 March 2024 (CQC, 2024). Data is on second sheet.\nCQC_directory_with_filters = pd.read_excel(\"Data/01_March_2024_HSCA_Active_Locations-v2.xlsx\", sheet_name=1)\n\n\n1.2.4. Ofsted Childcare Providers\nOfsted (Office for Standards in Education, Children’s Services and Skills) are the regulator for children’s care and education services (Ofsted, 2024). The childcare providers and inspections data is updated periodically, with the latest available being August 2023 (Ofsted, 2023). This dataset does not include a geometry but does include postcode. The ONS Postcode directory (ONS, 2024) is imported here and used to add latitude and longitude information to the dataset.\nDue to the size of the ONS Postcode directory, this is not included in this repository. Instead, the updated Ofsted Childcare Provider dataset, including the latitude and longitude information, is exported to a csv and included in the data folder.\n# Ofsted Childcare Providers data (Ofsted, 2023), list of all providers\nChildcare_Providers = pd.read_excel('Data/Childcare_provider_level_data_as_of_31_August_2023.xlsx', sheet_name=3, header=3)\n\n# ONS Postcode Directory (ONS, 2024) - this file is NOT included in the Zip folder due to its size.\n# Instead, an updated version of the above Ofsted Childcare Providers data with lat/long added from Postcodes\nPostcodes = pd.read_csv('Data/ONSPD_FEB_2024_UK/Data/ONSPD_FEB_2024_UK.csv', low_memory=False)\n\n# Trim to only required columns for ease of use\nPostcodes_trim = Postcodes[['pcds','lat','long']].copy()\n\n# Merge dataframes on Postcodes and add lat and long information to Childcare_Providers dataframe\nChildcare_Providers = Childcare_Providers.merge(Postcodes_trim, left_on='Postcode', right_on='pcds', how='left')\n\n# Export the DataFrame to CSV\nChildcare_Providers.to_csv('Data/Childcare_provider_level_data_as_of_31_August_2023_including_latlong.csv', index=False)"
  },
  {
    "objectID": "projects/intergenerational_care_facilities/index.html#visualisation-and-statistics",
    "href": "projects/intergenerational_care_facilities/index.html#visualisation-and-statistics",
    "title": "Identifying potential priority local authorities in England for intergenerational care facilities",
    "section": "3. Visualisation and Statistics",
    "text": "3. Visualisation and Statistics\nFirstly, some of the variables will be edited to improve the subsequent visualisations. This includes rounding numbers to a smaller amount of decimal places, converting percentages to a readable format, and renaming variables so they are clear.\n# Create a copy of final_df so data and column names can be made visual-appropriate\nchoropleth = final_df.copy()\n\n# Convert percentages to readable format\nchoropleth[['%U4', '%O65']] = (\n    choropleth[['%U4', '%O65']])*100\n\n# Round the other variables to appropriate dp for ease of reading\nchoropleth[['U4:EYS Locs', 'U4:EYS Places', 'O65:CH Locs', 'O65:CH Beds']] = (\n    choropleth[['U4:EYS Locs', 'U4:EYS Places', 'O65:CH Locs', 'O65:CH Beds']]).round(1)\nchoropleth[['%U4z', '%O65z', 'U4:EYS Locsz', 'U4:EYS Placesz', 'O65:CH Locsz', 'O65:CH Bedsz']] = (\n    choropleth[['%U4z', '%O65z', 'U4:EYS Locsz', 'U4:EYS Placesz', 'O65:CH Locsz', 'O65:CH Bedsz']]).round(3)\n    \n# Create a dictionary of current variable column names with long-form for use on map\ncolumns_dict = {\"CTYUA22CD\": \"LA Code\",\n                \"CTYUA22NM\": \"LA Name\",\n                \"U4\": \"Population Aged Under 4\",\n                \"O65\": \"Population Aged Over 65\",\n                \"%U4\": \"% Population Aged Under 4\",\n                \"%O65\": \"% Population Aged Over 65\",\n                \"CH Locs\": \"Care Home Locations\",\n                \"CH Beds\": \"Care Home Beds\",\n                \"EYS Locs\": \"EYS Locations\",\n                \"EYS Places\": \"EYS Places\",\n                \"U4:EYS Locs\": \"Ratio of Under 4 Population to EYS Locations\",\n                \"U4:EYS Places\": \"Ratio of Under 4 Population to EYS Places\",\n                \"O65:CH Locs\": \"Ratio of Over 65 Population to Care Home Locations\",\n                \"O65:CH Beds\": \"Ratio of Over 65 Population to Care Home Beds\",\n                \"U4z\": \"Population Aged Under 4 as Z-Score\",\n                \"O65z\": \"Population Aged Over 65 as Z-Score\",\n                \"%U4z\": \"% Population Aged Under 4 as Z-Score\",\n                \"%O65z\": \"% Population Aged Over 65 as Z-Score\",\n                \"U4:EYS Locsz\": \"Ratio of Under 4 Population to EYS Locations as Z-Score\",\n                \"U4:EYS Placesz\": \"Ratio of Under 4 Population to EYS Places as Z-Score\",\n                \"O65:CH Locsz\": \"Ratio of Over 65 Population to Care Home Locations as Z-Score\",\n                \"O65:CH Bedsz\": \"Ratio of Over 65 Population to Care Home Beds as Z-Score\"}\n    \n# Rename the columns to the long-form version\nrenamed_columns = choropleth.rename(columns=columns_dict, inplace=True)\n# Set display format to 2 decimal places for ease of reading\npd.set_option('display.float_format', lambda x: '%.2f' % x)\n\n3.1. Geographical context\nTo begin, a geographical context map is created for use in the accompanying policy briefing. This context map shows the LA boundaries that have been used as the geography for this analysis.\n\nFigure 3.1.1. Context Map showing LA boundaries in England\nData Source: ONS, 2023b\nfig, ax = plt.subplots(figsize=(10, 10))\n\n# Plot the boundaries, make slightly transparent to see context beneath\nfinal_df.plot(ax=ax, edgecolor='black', linewidth=0.5, alpha=0.5)\n\n# Add OpenStreetMap basemap and make slightly paler. Use crs from dataframe\ncx.add_basemap(ax, source=cx.providers.OpenStreetMap.Mapnik, alpha=0.7, crs=final_df.crs.to_string())\n\n# Set title\nplt.title('Map of Local Authority Boundaries, England')\n\n# Add source below the chart\nplt.text(0.001, -0.02, 'Source: ONS, 2023b', transform=ax.transAxes, fontsize=10, color='gray')\n\n# Turn off the axes\nax.axis('off')\n\n# Save as .png for use in Policy Brief\n#plt.savefig('Exports - Images/context_map.png')\n\n# Show the plot\nplt.show()\n\n\n\n\n3.2. ‘Raw’ variables\nFigure 3.2.1. presents choropleths of the ‘raw’ variables that are not included in the analysis, but give an idea of the spatial patterns in the data. There are higher proportions of under 4s in more urban LAs, particularly those that include major cities and towns or are within Greater London. Contrastingly, the proportion of the population aged 65 or over is highest around coastal areas that are typically more rural.\nThe spatial distribution of the total number of EYS and care home locations is overall similar across LAs. The highest numbers are again in the more urban LAs, particularly those around major cities and towns. However, the city-centre LAs such as Derby, Leicester and Nottingham have relatively fewer locations across both EYS and care homes. This likely reflects the more commercial nature of these LAs. Interestingly when comparing the spatial pattern between locations and proportions of each relevant population, there is no clearly visible relationship.\nThe results for EYS places and care home beds is largely similar to the total number of locations for each, with the noticable exception of EYS locations and places in Devon, where there appears to be fewer places compared to the number of locations as indicated by the lighter shade of green. This puts the number of places around the average for the dataset compared to the above average number of locations.\n\nFigure 3.2.1. Choropleth of “raw” variables by LA\nData Source: CQC, 2024; Ofsted, 2023; ONS, 2023c\n# Create a 2x3 grid of subplots for the maps of the \"raw\" variables\nfig, axes = plt.subplots(3, 2, figsize=(12, 20))\n\n# Plot each variable on a different subplot\n### 0,0\nchoropleth.plot(ax=axes[0,0],                                                         # Identify subplot\n                column='% Population Aged Under 4',                                    # Select variable to plot\n                cmap='OrRd', linewidth=0.5, edgecolor='black',                        # Amend map visuals\n                legend=True, scheme='natural_breaks', legend_kwds={'frameon':False})  # Amend legend\n\naxes[0,0].set_title(\"% Population Aged Under 4\", fontsize=14)                          # Add title\naxes[0,0].set_axis_off()                                                              # Remove axis\n\n### 0,1\nchoropleth.plot(ax=axes[0,1],\n                column='% Population Aged Over 65',\n                cmap='OrRd', linewidth=0.5, edgecolor='black',\n                legend=True, scheme='natural_breaks', legend_kwds={'frameon':False})\n\naxes[0,1].set_title(\"% Population Aged Over 65\", fontsize=14)\naxes[0,1].set_axis_off()\n\n### 1,0\nchoropleth.plot(ax=axes[1,0],\n                column='EYS Locations',\n                cmap='BuPu', linewidth=0.5, edgecolor='black',\n                legend=True, scheme='natural_breaks', legend_kwds={'frameon':False})\n\naxes[1,0].set_title(\"Count of EYS Locations\", fontsize=14)\naxes[1,0].set_axis_off()\n\n### 1,1\nchoropleth.plot(ax=axes[1,1],\n                column='Care Home Locations',\n                cmap='BuPu', linewidth=0.5, edgecolor='black',\n                legend=True, scheme='natural_breaks', legend_kwds={'frameon':False})\n\naxes[1,1].set_title(\"Count of Care Home Locations\", fontsize=14)\naxes[1,1].set_axis_off()\n\n### 2,0\nchoropleth.plot(ax=axes[2,0],\n                column='EYS Places',\n                cmap='YlGn', linewidth=0.5, edgecolor='black',\n                legend=True, scheme='natural_breaks', legend_kwds={'frameon':False})\n\naxes[2,0].set_title(\"Count of EYS Places\", fontsize=14)\naxes[2,0].set_axis_off()\n\n### 2,1\nchoropleth.plot(ax=axes[2,1],\n                column='Care Home Beds',\n                cmap='YlGn', linewidth=0.5, edgecolor='black',\n                legend=True, scheme='natural_breaks', legend_kwds={'frameon':False})\n\naxes[2,1].set_title(\"Count of Care Home Beds\", fontsize=14)\naxes[2,1].set_axis_off()\n\n\n\n# Add a title to each row of subplots\nfor i, title in enumerate([\"Population\", \"Locations\", \"Places/Beds\"]):\n    axes[i, 0].annotate(title, (0, 0.5), xytext=(-axes[i, 0].yaxis.labelpad -0, 0),\n                        xycoords=axes[i, 0].yaxis.label, textcoords='offset points',\n                        fontsize=16, ha='right', va='center', rotation=90, fontweight='bold')\n\n\n# Add a title to each column of subplots\nfor j, title in enumerate([\"Children\", \"Older People\"]):\n    axes[0, j].annotate(title, (0.5, 1), xytext=(0, 40),\n                        xycoords='axes fraction', textcoords='offset points',\n                        fontsize=16, ha='center', va='baseline', fontweight='bold')\n\n\n# Add an overall title for the whole axes\nfig.suptitle(\"Choropleths of 'Raw' Variables\", fontsize=18, fontweight='bold')\n\n# Show the plot\nplt.tight_layout(rect=[0, 0.03, 1, 0.95], h_pad=4.0, w_pad=4.0)\nplt.show()\n\n\n\n\n3.3. Features for analysis\nThe statistics and visualisations in this section include the features that will be used in the analysis. These are sometimes used in both ‘raw’ and standardised (Z-score) formats depending on the statistic or visualisation.\nFigure 3.3.1. shows some simple descriptive statistics of the ‘raw’ version of the features. This highlights that the range of values are on vastly different scales, emphasising the importance of standardising the data. The range is smallest for the percentage of people aged under 4, with the 25th, 50th and 75th percentiles showing that the majority of the data lies around 6%.\nThere is also clearly a difference in the number of children to EYS locations and places compared to the number of older people to care home locations and beds. This is likely influenced by the fact most children attend an EYS, particularly in the year before starting school. By contrast, not all older people will want or need to live in a care home and so the number of people aged 65 and over to each care home location and bed is much higher. This is a limitation within this analysis and is discussed in more depth in the accompanying policy briefing.\nLastly, care homes are also generally larger than EYS locations, which could be influencing the higher number of older people to care home locations compared to children to EYS locations.\n\nFigure 3.3.1. Descriptive statistics of ‘raw’ features\nData Source: CQC, 2024; Ofsted, 2023; ONS, 2023c; Author’s calculations\n# Descriptive statistics for \"raw\" version of features\nfinal_df[['%U4', '%O65', 'U4:EYS Locs', 'U4:EYS Places', 'O65:CH Locs', 'O65:CH Beds']].describe()\n\n\n\n\n\n\n%U4\n\n\n%O65\n\n\nU4:EYS Locs\n\n\nU4:EYS Places\n\n\nO65:CH Locs\n\n\nO65:CH Beds\n\n\n\n\n\n\ncount\n\n\n151.00\n\n\n151.00\n\n\n151.00\n\n\n151.00\n\n\n151.00\n\n\n151.00\n\n\n\n\nmean\n\n\n0.06\n\n\n0.19\n\n\n154.32\n\n\n3.10\n\n\n1129.40\n\n\n28.72\n\n\n\n\nstd\n\n\n0.01\n\n\n0.06\n\n\n50.54\n\n\n0.91\n\n\n522.09\n\n\n11.80\n\n\n\n\nmin\n\n\n0.04\n\n\n0.06\n\n\n76.57\n\n\n1.49\n\n\n512.12\n\n\n16.14\n\n\n\n\n25%\n\n\n0.06\n\n\n0.15\n\n\n119.53\n\n\n2.47\n\n\n874.22\n\n\n22.67\n\n\n\n\n50%\n\n\n0.06\n\n\n0.19\n\n\n140.57\n\n\n2.86\n\n\n1026.10\n\n\n25.65\n\n\n\n\n75%\n\n\n0.06\n\n\n0.23\n\n\n182.29\n\n\n3.59\n\n\n1188.83\n\n\n30.17\n\n\n\n\nmax\n\n\n0.08\n\n\n0.35\n\n\n324.88\n\n\n6.53\n\n\n4101.00\n\n\n105.70\n\n\n\n\nFigure 3.3.2. illustrates the relationship between each population group and the relevant service location and beds/places ratios. This gives an early indication of how many LAs are potentially under- or over-served by different services, shown by the distance from the regression line.\nFor the “% Population Aged Under 4” charts, those above the line are potentially under-served and may benefit from additional provision being introduced. This is where the proportion of the population in the target age group is high but the number of EYS locations and places is low.\nFor the “% Population Aged Over 65” charts, the relationship is negative, meaning LAs with a higher percentage of older people have lower ratios of people to care home locations and beds. However, the charts show there is a cluster of outliers with very high ratios to the bottom right of the chart. These are all in central London, an issue that will be investigated further in section 4.\n\n\nFigure 3.3.2. Scatterplots of percentage of the population by age group compared to service locations and beds/places ratios, by LA\nData Source: CQC, 2024; Ofsted, 2023; ONS, 2023c; Author’s calculations\n# Set up a figure with 2 rows and 2 columns for subplots\nfig, axs = plt.subplots(2, 2, figsize=(15, 10))\n\n# Add title, x-axis subtitle, and y-axis labels\nplt.title('% Population by age group compared to service locations and beds/places ratios', \n          x=-0.15, y=2.25, fontweight='bold', fontsize=16)\nfig.suptitle('Services', y=0.05, fontweight='bold', fontsize=14)\naxs[0,0].set_ylabel('% Population aged under 4', fontweight='bold', fontsize=14)\naxs[1,0].set_ylabel('% Population aged over 65', fontweight='bold', fontsize=14)\n\n\n### U4, EYS Locs scatter\naxs[0,0].scatter(x=final_df['U4:EYS Locs'], y=final_df['%U4'], s=3, c='#09435a') # add data and set colour\naxs[0,0].set_xlabel('U4:EYS Locations', fontsize=12) # add axis label and set font\n\n# Calculate the regression line\nslope0, intercept0 = np.polyfit(final_df['U4:EYS Locs'], final_df['%U4'], 1)\nregression_line0 = np.polyval([slope0, intercept0], final_df['U4:EYS Locs'])\n\n# Add this to the chart\naxs[0,0].plot(final_df['U4:EYS Locs'], regression_line0, color='red')\n\n\n### U4, EYS Places scatter\naxs[0,1].scatter(x=final_df['U4:EYS Places'], y=final_df['%U4'], s=3, c='#4e7d8e') # add data and set colour\naxs[0,1].set_xlabel('U4:EYS Places', fontsize=12) # add axis label and set font\n\n# Calculate the regression line\nslope1, intercept1 = np.polyfit(final_df['U4:EYS Places'], final_df['%U4'], 1)\nregression_line1 = np.polyval([slope1, intercept1], final_df['U4:EYS Places'])\n\n# Add this to the chart\naxs[0,1].plot(final_df['U4:EYS Places'], regression_line1, color='red')\n\n\n### O65, CH Locs scatter\naxs[1,0].scatter(x=final_df['O65:CH Locs'], y=final_df['%O65'], s=3, c='#9cc2b8') # add data and set colour\naxs[1,0].set_xlabel('O65:Care Home Locations', fontsize=12) # add axis label and set font\n\n# Calculate the regression line\nslope2, intercept2 = np.polyfit(final_df['O65:CH Locs'], final_df['%O65'], 1)\nregression_line2 = np.polyval([slope2, intercept2], final_df['O65:CH Locs'])\n\n# Add this to the chart\naxs[1,0].plot(final_df['O65:CH Locs'], regression_line2, color='red')\n\n\n### O65, CH Beds scatter\naxs[1,1].scatter(x=final_df['O65:CH Beds'], y=final_df['%O65'], s=3, c='#90958f') # add data and set colour\naxs[1,1].set_xlabel('O65:Care Home Beds', fontsize=12) # add axis label and set font\n\n# Calculate the regression line\nslope3, intercept3 = np.polyfit(final_df['O65:CH Beds'], final_df['%O65'], 1)\nregression_line3 = np.polyval([slope3, intercept3], final_df['O65:CH Beds'])\n\n# Add this to the chart\naxs[1,1].plot(final_df['O65:CH Beds'], regression_line3, color='red')\n\n\n# Add a footnote with the data source and specify location on visual\nplt.text(-0.15, -0.3,\n         \"Data Source: CQC, 2024; Ofsted, 2023; ONS, 2023c; Author's calculations\",\n         ha=\"center\", va=\"center\", transform=plt.gca().transAxes,\n         fontsize=10, color=\"gray\")\n\n\nplt.show()\n\nFigure 3.3.3. presents choropleths again, but this time using the features that will be used in the analysis. The population features are the same as before (figure 3.2.1.), but the provision is now shown as ratios.\nThe spatial distribution of the ratio of locations to beds/places presents a very different view compared to the raw counts in figure 3.2.1. LAs including large towns and cities have the highest ratio of children to EYS locations and places, with Wakefield, Doncaster and Rotherham being noticeably high ratios across both measures. Derby, Nottingham and Leicester also show high ratios as city-centre LAs.\nBy contrast, these areas all have a lower ratio of older people to care home locations and beds, as shown by the paler colour on the choropleth. This is partly influenced by the noticeable skewing of the data towards London, with all of the highest ratios for locations and beds being within London. This is an issue investigated further in section 4.\n\n\nFigure 3.3.3. Choropleths of ‘raw’ features for analysis, by LA\nData Source: CQC, 2024; Ofsted, 2023; ONS, 2023c; Author’s calculations\n# Create a 2x3 grid of subplots for the maps of the features\nfig, axes = plt.subplots(3, 2, figsize=(12, 20))\n\n# Plot each feature on a different subplot\n### 0,0\nchoropleth.plot(ax=axes[0,0],                                                         # Identify subplot\n                column='% Population Aged Under 4',                                    # Select variable to plot\n                cmap='OrRd', linewidth=0.5, edgecolor='black',                        # Amend map visuals\n                legend=True, scheme='natural_breaks', legend_kwds={'frameon':False})  # Amend legend\n\naxes[0,0].set_title(\"% Population Aged Under 4\", fontsize=14)                          # Add title\naxes[0,0].set_axis_off()                                                              # Remove axis\n\n### 0,1\nchoropleth.plot(ax=axes[0,1],\n                column='% Population Aged Over 65',\n                cmap='OrRd', linewidth=0.5, edgecolor='black',\n                legend=True, scheme='natural_breaks', legend_kwds={'frameon':False})\n\naxes[0,1].set_title(\"% Population Aged Over 65\", fontsize=14)\naxes[0,1].set_axis_off()\n\n### 1,0\nchoropleth.plot(ax=axes[1,0],\n                column='Ratio of Under 4 Population to EYS Locations',\n                cmap='BuPu', linewidth=0.5, edgecolor='black',\n                legend=True, scheme='natural_breaks', legend_kwds={'frameon':False})\n\naxes[1,0].set_title(\"Ratio of Under 4 Population to EYS Locations\", fontsize=14)\naxes[1,0].set_axis_off()\n\n### 1,1\nchoropleth.plot(ax=axes[1,1],\n                column='Ratio of Over 65 Population to Care Home Locations',\n                cmap='BuPu', linewidth=0.5, edgecolor='black',\n                legend=True, scheme='natural_breaks', legend_kwds={'frameon':False})\n\naxes[1,1].set_title(\"Ratio of Over 65 Population to Care Home Locations\", fontsize=14)\naxes[1,1].set_axis_off()\n\n### 2,0\nchoropleth.plot(ax=axes[2,0],\n                column='Ratio of Under 4 Population to EYS Places',\n                cmap='YlGn', linewidth=0.5, edgecolor='black',\n                legend=True, scheme='natural_breaks', legend_kwds={'frameon':False})\n\naxes[2,0].set_title(\"Ratio of Under 4 Population to EYS Places\", fontsize=14)\naxes[2,0].set_axis_off()\n\n### 2,1\nchoropleth.plot(ax=axes[2,1],\n                column='Ratio of Over 65 Population to Care Home Beds',\n                cmap='YlGn', linewidth=0.5, edgecolor='black',\n                legend=True, scheme='natural_breaks', legend_kwds={'frameon':False})\n\naxes[2,1].set_title(\"Ratio of Over 65 Population to Care Home Beds\", fontsize=14)\naxes[2,1].set_axis_off()\n\n\n\n# Add a title to each row of subplots\nfor i, title in enumerate([\"Population\", \"Locations\", \"Places/Beds\"]):\n    axes[i, 0].annotate(title, (0, 0.5), xytext=(-axes[i, 0].yaxis.labelpad -0, 0),\n                        xycoords=axes[i, 0].yaxis.label, textcoords='offset points',\n                        fontsize=16, ha='right', va='center', rotation=90, fontweight='bold')\n\n\n# Add a title to each column of subplots\nfor j, title in enumerate([\"Children\", \"Older People\"]):\n    axes[0, j].annotate(title, (0.5, 1), xytext=(0, 40),\n                        xycoords='axes fraction', textcoords='offset points',\n                        fontsize=16, ha='center', va='baseline', fontweight='bold')\n\n\n# Add an overall title for the whole axes\nfig.suptitle(\"Choropleths of 'Raw' Features for Analysis\", fontsize=18, fontweight='bold')\n\n# Show the plot\nplt.tight_layout(rect=[0, 0.03, 1, 0.95], h_pad=4.0, w_pad=4.0)\nplt.show()\n\nMoving on, figure 3.3.4. shows the same descriptive statistics as before but using the standardised (Z-score) version of the features. Here the range is much more comparable across the features, with some outliers on the top end as shown by the higher maximum compared to the minimum Z-score. This is illustrated more clearly in figure 3.3.5.\n\n\nFigure 3.3.4. Descriptive statistics of Z-score features\nData Source: CQC, 2024; Ofsted, 2023; ONS, 2023c; Author’s calculations\n# Descriptive statistics for Z-Score version of features\nfinal_df[['%U4z', '%O65z', 'U4:EYS Locsz', 'U4:EYS Placesz', 'O65:CH Locsz', 'O65:CH Bedsz']].describe()\n\n\n\n\n\n\n%U4z\n\n\n%O65z\n\n\nU4:EYS Locsz\n\n\nU4:EYS Placesz\n\n\nO65:CH Locsz\n\n\nO65:CH Bedsz\n\n\n\n\n\n\ncount\n\n\n151.00\n\n\n151.00\n\n\n151.00\n\n\n151.00\n\n\n151.00\n\n\n151.00\n\n\n\n\nmean\n\n\n-0.00\n\n\n-0.00\n\n\n0.00\n\n\n-0.00\n\n\n0.00\n\n\n0.00\n\n\n\n\nstd\n\n\n1.00\n\n\n1.00\n\n\n1.00\n\n\n1.00\n\n\n1.00\n\n\n1.00\n\n\n\n\nmin\n\n\n-2.29\n\n\n-2.33\n\n\n-1.54\n\n\n-1.78\n\n\n-1.19\n\n\n-1.07\n\n\n\n\n25%\n\n\n-0.63\n\n\n-0.69\n\n\n-0.69\n\n\n-0.70\n\n\n-0.49\n\n\n-0.51\n\n\n\n\n50%\n\n\n-0.02\n\n\n-0.02\n\n\n-0.27\n\n\n-0.26\n\n\n-0.20\n\n\n-0.26\n\n\n\n\n75%\n\n\n0.68\n\n\n0.61\n\n\n0.56\n\n\n0.55\n\n\n0.11\n\n\n0.12\n\n\n\n\nmax\n\n\n3.15\n\n\n2.59\n\n\n3.39\n\n\n3.80\n\n\n5.71\n\n\n6.55\n\n\n\n\nFigure 3.3.5. is a boxplot of the Z-scores of the features. There are extreme outliers on the top end of each feature, as illustrated by the circles lying beyond the upper extreme (horizontal line, or end of the whisker). The ‘%U4z’ and ‘%O65z’ features have the largest range between the upper and lower extremes, as well as the largest range between the upper and lower quartiles, highlighting the greatest spread in the data. However, these variables have the fewest extreme outliers and these outliers are also relatively close to the upper extreme compared to the other features.\nBy contrast, the ‘O65:CH Locsz’ and ‘O65:CH Bedsz’ features have the smallest range between the upper and lower extremes, as well as the smallest range between the upper and lower quartile. However, these features have the most extreme outliers both in terms of quantity and distance from the upper extreme. This confirms the findings in figure 3.3.2. that was causing the negative gradient regression line.\n\n\nFigure 3.3.5. Boxplot of Z-Score features, by LA\nData Source: CQC, 2024; Ofsted, 2023; ONS, 2023c; Author’s calculations\n# Box plot of Z-score features\nplt.figure(figsize=(12, 8))\nsns.boxplot(data=final_df[['%U4z', '%O65z', 'U4:EYS Locsz', 'U4:EYS Placesz', 'O65:CH Locsz', 'O65:CH Bedsz']])\nplt.title('Boxplot of Z-Score Features, by LA', fontweight='bold')\nplt.ylabel('Z-Score', fontweight='bold')\nplt.xlabel('Feature', fontweight='bold')\nplt.xticks(rotation=45, ha='right')\nplt.grid(True, axis='y', linestyle='--', linewidth=0.5)\nplt.show()"
  },
  {
    "objectID": "projects/intergenerational_care_facilities/index.html#analysis-results",
    "href": "projects/intergenerational_care_facilities/index.html#analysis-results",
    "title": "Identifying potential priority local authorities in England for intergenerational care facilities",
    "section": "4. Analysis & Results",
    "text": "4. Analysis & Results\n\n4.1. Establishing feature collinearity\nBefore building the K-means clustering model, the relationship between the features must be further understood. To begin, the features are isolated into a new dataframe, with the CTYUA22CD converted into the index. This dataframe will be used for the subsequent analysis in this section. The Z-score version of the features are used as they are standardised and therefore comparable.\n# Isolate the features to include in the K-means modelling\nkmeanData = final_df.set_index('CTYUA22CD')[[\n    '%U4z', '%O65z', 'U4:EYS Locsz', 'U4:EYS Placesz', 'O65:CH Locsz', 'O65:CH Bedsz']].copy()\nNext, a correlation matrix (figure 4.1.1.) is created to check for collinearity and illustrate the relationship between the features going into more depth than the visualisations in Section 3. Strong collinearity (positive or negative) is not necessarily an issue when building a clustering model, but it is important to understand the features and their relationships in more depth before commencing the more advanced analysis.\nFigure 4.1.1. shows collinearity between the location and beds/places ratios for the respective age groups. This is to be expected as generally more locations means more places/beds. There is some collinearity between the proportion of the population aged under 4 and the EYS ratio features, whilst the opposite is true for the over 65 population and care home ratios, as was found in Section 3.\n\nFigure 4.1.1. Correlation matrix heatmap of Z-Score features\nData Source: CQC, 2024; Ofsted, 2023; ONS, 2023c; Author’s calculations\n# Generate correlation matrix of features\ncorrelation_matrix = kmeanData.corr()\n\n# Generate a mask for the diagonal cells\nmask = np.triu(np.ones_like(correlation_matrix, dtype=bool))\n\n# Generate a heatmap\nplt.figure(figsize=(10, 8))\nsns.heatmap(correlation_matrix, cmap='RdYlGn', annot=True, \n            fmt=\".2f\", mask=mask, vmin=-1, vmax=1, cbar_kws={\"shrink\": 0.75}, linewidth=.5)\nplt.title('Correlation Matrix Heatmap of Z-Score Features', fontweight='bold')\n\n# Rotate axis labels\nplt.xticks(rotation=45)\nplt.yticks(rotation=45)\n\n# Save as .png for use in Policy Brief\n#plt.savefig('Exports - Images/correlation_matrix_heatmap.png')\n\nplt.show()\n\nAnother way of viewing this is as a scatter matrix (figure 4.1.2.) which gives a clearer view of the spread across the datapoints, as well as the distribution of data for each feature (shown on the diagonal). As mentioned in section 2.6., the features are normally distributed with some tails due to outliers, but showing that Z-scores as a standardisation approach is acceptable to use.\n\n\nFigure 4.1.2. Scatter matrix of Z-Score features\nData Source: CQC, 2024; Ofsted, 2023; ONS, 2023c; Author’s calculations\n# Correlation matrix\ncorr_matrix = pd.plotting.scatter_matrix(kmeanData, alpha=0.5, figsize=(12, 12), diagonal='kde')\n\n\n\n\n4.2. Identifying the optimal number of K-means clusters\nThere are a number of ways to determine the optimal number of K-means clusters; however, here the elbow method is used. This tests a range of K values (1 to 10) to identify where the distortions drop significantly between clusters, thus creating an “elbow” in the chart (figure 4.2.1.). The yellow line on figure 4.2.1. highlights that for this data, the optimal number of K clusters is 4.\nIt should be noted that 3 clusters were also tested during this analysis given the visible drop in the chart here also, but this model performed worse overall compared to the 4 cluster version.\ndistortions = [] # Empty list for distortion data\nK = range(1,10) # K value range for cluster tests\nfor k in K:\n    kmeanModel = KMeans(n_clusters=k, n_init=10) # Build K-Means clustering model\n    kmeanModel.fit(kmeanData) # Fit model to the data\n    distortions.append(kmeanModel.inertia_) # Add results to distortions list\n\nFigure 4.2.1. Elbow chart showing optimal k (number of clusters)\nData Source: Author’s calculations\n# Elbow chart\nplt.figure(figsize=(16,8))\nplt.plot(K, distortions, ls='-', lw=2, marker='o')\nplt.xlabel('k - Number of Clusters', fontsize=16)\nplt.ylabel('Distortion', fontsize=16)\nplt.title('The Elbow Method showing the optimal k (Number of Clusters)', fontsize=18, fontweight='bold')\nplt.axvline(x=4, color='Orange', linestyle='--')  # Adding a vertical dotted line at x=4\n\n# Save as .png for use in Policy Brief\n#plt.savefig('Exports - Images/elbow_chart.png')\n\nplt.show()\n\n\n\n\n4.3. K-means clustering model\nA K-means clustering model can now be run using the parameters outlined in the previous section. The model will calculate 4 clusters, and several pieces of information from the model will be stored for further testing and visualisation.\n# Determine number of clusters for the model and number of times the model is run from different centroid seeds\nkmeans = KMeans(n_clusters=4, n_init=10)\n# Fit the model above to the data\nkmeans.fit(kmeanData)\nKMeans(n_clusters=4, n_init=10)\n# Get cluster labels and distances from cluster centers\nkmeans_labels = kmeans.labels_\nkmeans_centres = kmeans.cluster_centers_\n# Calculate distances from each data point to its assigned cluster center\ndistances = np.linalg.norm(kmeanData - kmeans_centres[kmeans_labels], axis=1)\n\n# Add cluster labels and distances to the kmean dataframe\nkmeanData['Cluster'] = kmeans_labels\nkmeanData['Distance'] = distances\nA check of the number iterations that were performed before the model stabilised shows that this model performed well with a low number of iterations, which is preferable over more iterations.\n# Get the number of iterations\nkmeans.n_iter_\n6\nThe silhouette score, or silhouette coefficient, indicates the performance of the model in terms of overlapping clusters or misassignment of clusters. The closer the score is to 1, the better the model, whilst 0 indicates overlapping clusters, and negative values imply that incorrect clusters have been assigned (scikit-learn, 2024). The silhouette score for this model is acceptable, but highlights the model is not perfect.\nmetrics.silhouette_score(X=kmeanData, labels=kmeans_labels) #specify the data and the labels\n0.38730634175097434\nIt is also important to check the distribution of LAs in each cluster. It is unlikely that each cluster will contain the same number of LAs, but a K-means clustering model with the majority of LAs in one cluster would likely need reviewing. This model has a small number of LAs in one cluster, but as discussed in the policy briefing this is a sensible and justifiable result in these circumstances.\n# Count the occurrences of each cluster label\ncluster_counts = kmeanData['Cluster'].value_counts().reset_index()\n\n# Rename the columns to 'Cluster' and 'Count'\ncluster_counts.columns = ['Cluster', 'Count']\n\n# Sort the DataFrame by cluster label\ncluster_counts.sort_values(by='Cluster', inplace=True)\ncluster_counts\n\n\n\n\n\n\nCluster\n\n\nCount\n\n\n\n\n\n\n0\n\n\n0\n\n\n57\n\n\n\n\n2\n\n\n1\n\n\n33\n\n\n\n\n1\n\n\n2\n\n\n51\n\n\n\n\n3\n\n\n3\n\n\n10\n\n\n\n\nIt is also important to understand the statistics behind the model, and ensure that the model is robust. ANOVA tables show the significance of each feature, as well as its F-statistic. All of the features are statistically significant as shown in figure 4.3.1., and the range in F-statistic is relatively small compared to when the K-means model was re-run with different cluster numbers. However, the age features have a lower F-statistic than the other features such that these are contributing less to the model overall.\n# Create a copy of the kmeanData df including features and cluster labels\ndf_for_ANOVA = kmeanData.copy()\n\n# Dictionary mapping original column names to ASCII compliant names\nascii_column_names = {\n    '%U4z': 'PC_U4z',\n    '%O65z': 'PC_O65z',\n    'U4:EYS Locsz': 'U4_EYS_Locsz',\n    'U4:EYS Placesz': 'U4_EYS_Placesz',\n    'O65:CH Locsz': 'O65_CH_Locsz',\n    'O65:CH Bedsz': 'O65_CH_Bedsz',\n    'Cluster': 'Cluster',\n    'Distance': 'Distance'\n}\n\n# Rename columns\ndf_for_ANOVA = df_for_ANOVA.rename(columns=ascii_column_names)\n\n# Perform ANOVA for each feature\nanova_results = []\nfor column in df_for_ANOVA.columns[:-2]:  # Exclude 'Cluster' and 'Distance' columns\n    formula = f\"{column} ~ C(Cluster)\"\n    model = ols(formula, data=df_for_ANOVA).fit()\n    anova_table = sm.stats.anova_lm(model, typ=2)\n    anova_results.append((column, anova_table))\n\nFigure 4.3.1. ANOVA tables for features in the K-means cluster model\nData Source: Author’s calculations\n# Print ANOVA tables in a formatted way\nfor column, table in anova_results:\n    print(f\"ANOVA Table for '{column}':\")\n    print(tabulate(table, headers='keys', tablefmt='pretty'))\n    print()\nANOVA Table for 'PC_U4z':\n+------------+-------------------+-------+-------------------+------------------------+\n|            |      sum_sq       |  df   |         F         |         PR(&gt;F)         |\n+------------+-------------------+-------+-------------------+------------------------+\n| C(Cluster) | 73.97417079222865 |  3.0  | 47.05868675612382 | 2.2326909650674807e-21 |\n|  Residual  | 77.02582920777132 | 147.0 |        nan        |          nan           |\n+------------+-------------------+-------+-------------------+------------------------+\n\nANOVA Table for 'PC_O65z':\n+------------+-------------------+-------+-------------------+------------------------+\n|            |      sum_sq       |  df   |         F         |         PR(&gt;F)         |\n+------------+-------------------+-------+-------------------+------------------------+\n| C(Cluster) | 87.32843117619302 |  3.0  | 67.20571216761822 | 2.0239329302714326e-27 |\n|  Residual  | 63.67156882380687 | 147.0 |        nan        |          nan           |\n+------------+-------------------+-------+-------------------+------------------------+\n\nANOVA Table for 'U4_EYS_Locsz':\n+------------+-------------------+-------+-------------------+-----------------------+\n|            |      sum_sq       |  df   |         F         |        PR(&gt;F)         |\n+------------+-------------------+-------+-------------------+-----------------------+\n| C(Cluster) | 98.70417115561074 |  3.0  | 92.48355927231522 | 1.120837363066649e-33 |\n|  Residual  | 52.29582884438926 | 147.0 |        nan        |          nan          |\n+------------+-------------------+-------+-------------------+-----------------------+\n\nANOVA Table for 'U4_EYS_Placesz':\n+------------+-------------------+-------+-------------------+-----------------------+\n|            |      sum_sq       |  df   |         F         |        PR(&gt;F)         |\n+------------+-------------------+-------+-------------------+-----------------------+\n| C(Cluster) | 90.76027183013207 |  3.0  | 73.82591945195729 | 3.513831946635027e-29 |\n|  Residual  | 60.23972816986792 | 147.0 |        nan        |          nan          |\n+------------+-------------------+-------+-------------------+-----------------------+\n\nANOVA Table for 'O65_CH_Locsz':\n+------------+--------------------+-------+--------------------+------------------------+\n|            |       sum_sq       |  df   |         F          |         PR(&gt;F)         |\n+------------+--------------------+-------+--------------------+------------------------+\n| C(Cluster) | 108.92284279457296 |  3.0  | 126.84362850077896 | 1.3508317738081858e-40 |\n|  Residual  | 42.07715720542714  | 147.0 |        nan         |          nan           |\n+------------+--------------------+-------+--------------------+------------------------+\n\nANOVA Table for 'O65_CH_Bedsz':\n+------------+-------------------+-------+-------------------+------------------------+\n|            |      sum_sq       |  df   |         F         |         PR(&gt;F)         |\n+------------+-------------------+-------+-------------------+------------------------+\n| C(Cluster) | 90.53224276482536 |  3.0  | 73.36273244306693 | 4.6328666850279454e-29 |\n|  Residual  | 60.46775723517465 | 147.0 |        nan        |          nan           |\n+------------+-------------------+-------+-------------------+------------------------+\nThe boxplot below (figure 4.3.2.) shows the distribution of each LA from the cluster centre to which it has been assigned, and helps to identify any outliers. There are a few extreme outliers at the top end for each cluster, but the majority of the data falls within the upper and lower quartiles.\nCluster 0 has the most outliers but these are fairly close to the upper extreme. This cluster has a relatively low mean and the smallest range between the upper and lower quartiles. This shows that most LAs in this cluster are near to the centre despite the outliers on the top end.\nCluster 1 only has one outliers, but has the largest range between the upper and lower extremes and quartiles, suggesting some variance in the results. However, the mean is still fairly low, highlighting that LAs in this cluster mostly have values closer to the cluster centre.\nCluster 2, has the lowest mean and fairly small range between the upper and lower quartiles. There are two outliers on the top end, but overall this cluster performs well.\nCluster 3 has the highest mean and one extreme outlier on the top end, but does have a relatively small range between the quartiles. As highlighted above, this cluster has the fewest LAs within it and as shown in the visualisations in section 4.4. all the LAs in this cluster are in London. This issue is discussed in depth in the policy briefing.\n\n\nFigure 4.3.2. Boxplot of distance from cluster centres\nData Source: Author’s calculations\npalette = ['#ed6a5a','#f4f1bb','#9bc1bc', '#81a4cd']\n\n# Plot boxplot\nplt.figure(figsize=(8, 5))\nsns.boxplot(data=kmeanData, x=\"Cluster\", y=\"Distance\", hue=\"Cluster\", legend=False, palette=palette, linewidth=1.25)\nplt.title('Distance from Cluster Centre', fontsize=14, fontweight='bold')\nplt.xlabel('Cluster', fontsize=12, fontweight='bold')\nplt.ylabel('Distance', fontsize=12, fontweight='bold')\nplt.grid(True, axis='y', linestyle='--', linewidth=0.5)\n\n# Save as .png for use in Policy Brief\n#plt.savefig('Exports - Images/distance_from_cluster_centres.png')\n\nplt.show()\n\nTo present the final cluster centres, the results must be transformed into a useable dataframe.\n# Variable names\nvariable_names = ['%U4z', '%O65z', 'U4:EYS Locsz', 'U4:EYS Placesz', 'O65:CH Locsz', 'O65:CH Bedsz']\n\n# Reshape the cluster centres array to match variable names\ncluster_centres_reshaped = kmeans_centres.T\n\n# Create a list to store the data\ndata = []\n\n# Iterate through each cluster and variable, and append the data\nfor i, cluster_centre in enumerate(cluster_centres_reshaped):\n    for j, centre in enumerate(cluster_centre):\n        data.append([j, variable_names[i], centre])\n\n# Create Cluster centres dataframe\ndf_cluster_centres = pd.DataFrame(data, columns=['Cluster Number', 'Feature Name', 'Centre'])\nThese clusters centres can then be used to present, by feature, each cluster which makes it easier to identify what each cluster represents. Figure 4.3.3. illstrates these results, with the below discussion developing pen portraits for each cluster. These pen portraits are used and discussed further in the policy briefing.\nCluster 0 has an above average proportion of the population aged under 4, but all other features are around the average. This implies there is a large target under 4 population in these LAs but provision is currently in line with the average. This cluster is “Young areas with adequate provision”.\nCluster 1 has an above average proportion of the population aged under 4 as well as EYS ratios for both locations and places. This implies there are a large number of children in these areas compared to the average across all LAs, and that current provision is poor with more children to each EYS location and place. This cluster is “EYS opportunity”.\nCluster 2 has an above average proportion of the population aged over 65, whilst having a below average proportion of children. The ratio features are all slightly below average. These are areas with provision in line with the average despite the older population. This cluster is “Older areas with adequate provision”.\nCluster 3 shows significantly above average ratios for older people to care home locations and beds. This implies provision is significantly lacking in these areas. Meanwhile the proportion of the population over 65 is actually very below average, but the population that does live in these areas is lacking services. This cluster is “Care home opportunity”.\n\n\nFigure 4.3.3. K-means cluster model centres by feature\nData Source: Author’s calculations\npalette = ['#003049','#6b2c39','#d62828','#f77f00','#fcbf49','#eae2b7']\n\n# Plot the bar chart\nplt.figure(figsize=(10, 6))\n\n# Set seaborn style\nsns.despine()\nsns.set_style(\"whitegrid\")\n\n# Add data and format\nsns.barplot(data=df_cluster_centres, x='Cluster Number', y='Centre', hue='Feature Name', palette=palette)\nplt.title('K-means Cluster Model Centres by Feature', fontsize=14, fontweight='bold')\nplt.xlabel('Cluster Number', fontsize=12, fontweight='bold')\nplt.ylabel('Z-Score Value of Cluster Centre', fontsize=12, fontweight='bold')\nplt.legend(title='Feature Name', bbox_to_anchor=(1.05, 1), loc='upper left')\n\n# Add black line behind the x-axis and show grid for ease of reading\nplt.axhline(0, color='black', linewidth=1)\nplt.grid(True, axis='y', linestyle='--', linewidth=0.5)\n\n# Save as .png for use in Policy Brief\n#plt.savefig('Exports - Images/cluster_centres_by_feature.png')\n\nplt.show()\n\n\n\n\n4.4. Visualising results\nThe resulting clusters can be visualised on a map to highlight any spatial patterns that have been found in the data. Figure 4.4.1. shows that cluster 3 (“Care home opportunity”), the smallest cluster as discussed above, contains only LAs in London.\nAcross the other clusters, there is more geographic variation. However, cluster 2 (“Older areas with adequate provision”) covers more geographically large LAs, which primarily include large amounts of rural land such as North Yorkshire, Derbyshire and Devon. In contrast, Cluster 1 (“EYS opportunity”) includes a number of mostly urban LAs, such as Derby and Leicester, as well as areas known for their large young populations, like Bradford (CBMDC, 2024).\nCluster 0 (“Young areas with adequate provision”) is perhaps the most geographically varied covering rural-urban mix LAs such as Essex, Bedfordshire and Leeds, as well as mostly urban LAs such as those around outer London and City of Bristol.\n# Reset index to get CTYUA22CD as a column for merging\nkmeanData.reset_index(inplace=True)\n# Add K-means clustering results to final_df for visualising\nfinal_df = pd.merge(final_df, kmeanData[['CTYUA22CD', 'Cluster', 'Distance']], on='CTYUA22CD', how='left')\n\nFigure 4.4.1. Map of K-means cluster model results\nData Source: Author’s calculations\nfig, axes = plt.subplots(figsize=(12, 10))\n\n# Plot clusters\ncmap = clrs.ListedColormap(['#ed6a5a','#f4f1bb','#9bc1bc', '#81a4cd'])\nfinal_df.plot(ax=axes,\n              column='Cluster',\n              categorical=True,\n              cmap=cmap,\n              linewidth=0.5, edgecolor='black',\n              legend=True, legend_kwds={'frameon':False})\n\naxes.set_title(\"Map of K-means cluster model results\", fontsize=14)\naxes.set_axis_off()\n\n# Save as .png for use in Policy Brief\n#plt.savefig('Exports - Images/map_cluster_results.png')\n\n# Show the plot\nplt.show()\n\nTo provide further clarity on cluster 3, figure 4.4.2. shows a focused map of the LAs included in this cluster. This is discussed further in the policy briefing.\nCluster_London = final_df[final_df['Cluster'] == 3]\n\n\nFigure 4.4.2. Map of cluster containing London LAs only\nData Source: Author’s calculations\nfig, ax = plt.subplots(1,figsize=(10,10))\nCluster_London.plot(ax = ax, edgecolor = 'black', linewidth = 0.5, alpha = 0.5, color='#81a4cd')\ncx.add_basemap(ax, crs = Cluster_London.crs, source=cx.providers.CartoDB.Positron)\n\n# Loop through each polygon and add label\nfor idx, row in Cluster_London.iterrows():\n    centroid_x, centroid_y = row.geometry.centroid.x, row.geometry.centroid.y\n    bbox_props = dict(boxstyle=\"round,pad=0.3\", fc=\"#F7F7F7\", lw=0.5)\n    ax.annotate(text=row['CTYUA22NM'], xy=(centroid_x, centroid_y), xycoords='data', ha='center', va='center',\n                fontsize=8, color='black', bbox=bbox_props)\n\nax.set_title(\"Map of cluster containing London LAs only\", fontsize=14)\nax.set_axis_off()\n\n# Save as .png for use in Policy Brief\n#plt.savefig('Exports - Images/map_London_cluster.png')\n\nplt.show()\n\nSimple descriptives confirm the difference between the London cluster and the dataset as a whole as being driven by low numbers of care homes in these LAs.\nCluster_London.describe()\n\n\n\n\n\n\nU4\n\n\n%U4\n\n\nO65\n\n\n%O65\n\n\nCH Locs\n\n\nCH Beds\n\n\nEYS Locs\n\n\nEYS Places\n\n\nU4:EYS Locs\n\n\nU4:EYS Places\n\n\n…\n\n\nU4z\n\n\nO65z\n\n\n%U4z\n\n\n%O65z\n\n\nU4:EYS Locsz\n\n\nU4:EYS Placesz\n\n\nO65:CH Locsz\n\n\nO65:CH Bedsz\n\n\nCluster\n\n\nDistance\n\n\n\n\n\n\ncount\n\n\n10.00\n\n\n10.00\n\n\n10.00\n\n\n10.00\n\n\n10.00\n\n\n10.00\n\n\n10.00\n\n\n10.00\n\n\n10.00\n\n\n10.00\n\n\n…\n\n\n10.00\n\n\n10.00\n\n\n10.00\n\n\n10.00\n\n\n10.00\n\n\n10.00\n\n\n10.00\n\n\n10.00\n\n\n10.00\n\n\n10.00\n\n\n\n\nmean\n\n\n13378.50\n\n\n0.06\n\n\n24177.30\n\n\n0.11\n\n\n9.40\n\n\n421.20\n\n\n105.00\n\n\n5105.52\n\n\n125.89\n\n\n2.61\n\n\n…\n\n\n-0.47\n\n\n-0.72\n\n\n-0.71\n\n\n-1.50\n\n\n-0.56\n\n\n-0.54\n\n\n3.14\n\n\n2.85\n\n\n3.00\n\n\n2.43\n\n\n\n\nstd\n\n\n4521.43\n\n\n0.01\n\n\n4929.80\n\n\n0.03\n\n\n3.37\n\n\n140.85\n\n\n23.53\n\n\n1411.07\n\n\n30.59\n\n\n0.56\n\n\n…\n\n\n0.30\n\n\n0.08\n\n\n1.13\n\n\n0.48\n\n\n0.61\n\n\n0.62\n\n\n1.40\n\n\n1.80\n\n\n0.00\n\n\n0.93\n\n\n\n\nmin\n\n\n6171.00\n\n\n0.04\n\n\n17472.00\n\n\n0.06\n\n\n5.00\n\n\n194.00\n\n\n70.00\n\n\n2852.00\n\n\n88.16\n\n\n2.10\n\n\n…\n\n\n-0.95\n\n\n-0.83\n\n\n-2.29\n\n\n-2.33\n\n\n-1.31\n\n\n-1.11\n\n\n1.10\n\n\n1.31\n\n\n3.00\n\n\n1.15\n\n\n\n\n25%\n\n\n9314.25\n\n\n0.05\n\n\n20528.75\n\n\n0.09\n\n\n7.25\n\n\n338.25\n\n\n90.25\n\n\n4308.82\n\n\n104.26\n\n\n2.18\n\n\n…\n\n\n-0.74\n\n\n-0.78\n\n\n-1.43\n\n\n-1.83\n\n\n-0.99\n\n\n-1.01\n\n\n2.33\n\n\n1.66\n\n\n3.00\n\n\n2.02\n\n\n\n\n50%\n\n\n15098.50\n\n\n0.05\n\n\n24803.50\n\n\n0.11\n\n\n9.50\n\n\n382.00\n\n\n105.50\n\n\n5105.50\n\n\n121.61\n\n\n2.32\n\n\n…\n\n\n-0.35\n\n\n-0.71\n\n\n-0.93\n\n\n-1.45\n\n\n-0.65\n\n\n-0.86\n\n\n2.72\n\n\n2.01\n\n\n3.00\n\n\n2.31\n\n\n\n\n75%\n\n\n16305.25\n\n\n0.06\n\n\n26865.50\n\n\n0.12\n\n\n11.00\n\n\n513.50\n\n\n118.00\n\n\n5749.18\n\n\n137.31\n\n\n2.95\n\n\n…\n\n\n-0.27\n\n\n-0.68\n\n\n0.29\n\n\n-1.19\n\n\n-0.34\n\n\n-0.17\n\n\n3.65\n\n\n3.54\n\n\n3.00\n\n\n2.61\n\n\n\n\nmax\n\n\n19090.00\n\n\n0.07\n\n\n33949.00\n\n\n0.15\n\n\n16.00\n\n\n625.00\n\n\n147.00\n\n\n7279.43\n\n\n185.34\n\n\n3.65\n\n\n…\n\n\n-0.09\n\n\n-0.57\n\n\n0.95\n\n\n-0.69\n\n\n0.62\n\n\n0.61\n\n\n5.71\n\n\n6.55\n\n\n3.00\n\n\n4.72\n\n\n\n\n\n8 rows × 22 columns\n\nfinal_df.describe()\n\n\n\n\n\n\nU4\n\n\n%U4\n\n\nO65\n\n\n%O65\n\n\nCH Locs\n\n\nCH Beds\n\n\nEYS Locs\n\n\nEYS Places\n\n\nU4:EYS Locs\n\n\nU4:EYS Places\n\n\n…\n\n\nU4z\n\n\nO65z\n\n\n%U4z\n\n\n%O65z\n\n\nU4:EYS Locsz\n\n\nU4:EYS Placesz\n\n\nO65:CH Locsz\n\n\nO65:CH Bedsz\n\n\nCluster\n\n\nDistance\n\n\n\n\n\n\ncount\n\n\n151.00\n\n\n151.00\n\n\n151.00\n\n\n151.00\n\n\n151.00\n\n\n151.00\n\n\n151.00\n\n\n151.00\n\n\n151.00\n\n\n151.00\n\n\n…\n\n\n151.00\n\n\n151.00\n\n\n151.00\n\n\n151.00\n\n\n151.00\n\n\n151.00\n\n\n151.00\n\n\n151.00\n\n\n151.00\n\n\n151.00\n\n\n\n\nmean\n\n\n20375.75\n\n\n0.06\n\n\n68874.81\n\n\n0.19\n\n\n71.11\n\n\n2756.13\n\n\n149.15\n\n\n7229.03\n\n\n154.32\n\n\n3.10\n\n\n…\n\n\n-0.00\n\n\n-0.00\n\n\n-0.00\n\n\n-0.00\n\n\n0.00\n\n\n-0.00\n\n\n0.00\n\n\n0.00\n\n\n1.09\n\n\n1.38\n\n\n\n\nstd\n\n\n15025.87\n\n\n0.01\n\n\n61972.17\n\n\n0.06\n\n\n69.11\n\n\n2676.12\n\n\n134.77\n\n\n6260.93\n\n\n50.54\n\n\n0.91\n\n\n…\n\n\n1.00\n\n\n1.00\n\n\n1.00\n\n\n1.00\n\n\n1.00\n\n\n1.00\n\n\n1.00\n\n\n1.00\n\n\n0.99\n\n\n0.67\n\n\n\n\nmin\n\n\n96.00\n\n\n0.04\n\n\n580.00\n\n\n0.06\n\n\n1.00\n\n\n14.00\n\n\n1.00\n\n\n32.00\n\n\n76.57\n\n\n1.49\n\n\n…\n\n\n-1.35\n\n\n-1.11\n\n\n-2.29\n\n\n-2.33\n\n\n-1.54\n\n\n-1.78\n\n\n-1.19\n\n\n-1.07\n\n\n0.00\n\n\n0.25\n\n\n\n\n25%\n\n\n11161.00\n\n\n0.06\n\n\n31390.00\n\n\n0.15\n\n\n29.00\n\n\n1141.00\n\n\n75.00\n\n\n3847.50\n\n\n119.53\n\n\n2.47\n\n\n…\n\n\n-0.62\n\n\n-0.61\n\n\n-0.63\n\n\n-0.69\n\n\n-0.69\n\n\n-0.70\n\n\n-0.49\n\n\n-0.51\n\n\n0.00\n\n\n0.96\n\n\n\n\n50%\n\n\n16316.00\n\n\n0.06\n\n\n43282.00\n\n\n0.19\n\n\n51.00\n\n\n1735.00\n\n\n107.00\n\n\n5397.43\n\n\n140.57\n\n\n2.86\n\n\n…\n\n\n-0.27\n\n\n-0.41\n\n\n-0.02\n\n\n-0.02\n\n\n-0.27\n\n\n-0.26\n\n\n-0.20\n\n\n-0.26\n\n\n1.00\n\n\n1.20\n\n\n\n\n75%\n\n\n23909.50\n\n\n0.06\n\n\n79181.50\n\n\n0.23\n\n\n80.00\n\n\n3225.00\n\n\n166.00\n\n\n8205.22\n\n\n182.29\n\n\n3.59\n\n\n…\n\n\n0.24\n\n\n0.17\n\n\n0.68\n\n\n0.61\n\n\n0.56\n\n\n0.55\n\n\n0.11\n\n\n0.12\n\n\n2.00\n\n\n1.65\n\n\n\n\nmax\n\n\n87286.00\n\n\n0.08\n\n\n319328.00\n\n\n0.35\n\n\n340.00\n\n\n13268.00\n\n\n763.00\n\n\n34881.58\n\n\n324.88\n\n\n6.53\n\n\n…\n\n\n4.47\n\n\n4.05\n\n\n3.15\n\n\n2.59\n\n\n3.39\n\n\n3.80\n\n\n5.71\n\n\n6.55\n\n\n3.00\n\n\n4.72\n\n\n\n\n\n8 rows × 22 columns\n\nFull discussion of the results outlined in this notebook is conducted in the accompanying policy brief."
  },
  {
    "objectID": "projects/intergenerational_care_facilities/index.html#references",
    "href": "projects/intergenerational_care_facilities/index.html#references",
    "title": "Identifying potential priority local authorities in England for intergenerational care facilities",
    "section": "References",
    "text": "References\nCare Quality Commission (CQC). 2024. Care directory with filters (01 March 2024). Care Quality Commission. [Online]. [Accessed 29 April 2024]. Available from: https://www.cqc.org.uk/about-us/transparency/using-cqc-data\nCity of Bradford Metropolitan District Council (CBMDC). 2024. 1.1 Demographics of Bradford District - 2024. [Online]. [Accessed 29 April 2024]. Available from: https://jsna.bradford.gov.uk/The%20population%20of%20Bradford%20District.asp\nOffice for National Statistics (ONS). 2023a. Area type definitions Census 2021. [Online]. [Accessed 29 April 2024]. Available from: https://www.ons.gov.uk/census/census2021dictionary/areatypedefinitions\nOffice for National Statistics (ONS). 2023b. Counties and Unitary Authorities (December 2022) Boundaries UK BFC. Open Geography Portal. [Online]. [Accessed 29 April 2024]. Available from: https://geoportal.statistics.gov.uk/datasets/e204895bba5646a486da29b5ed382db1_0/explore\nOffice for National Statistics (ONS). 2023c. Age of all usual residents, by Upper Tier Local Authority in England. Office for National Statistics. [Online]. [Accessed 29 April 2024]. Available from: https://www.ons.gov.uk/datasets/create/filter-outputs/b42f8cd2-78e3-4c54-8d71-de05f72930eb#get-data\nOffice for National Statistics (ONS). 2024. ONS Postcode Directory (February 2024). Open Geography Portal. [Online]. [Accessed 29 April 2024]. Available from: https://geoportal.statistics.gov.uk/datasets/e14b1475ecf74b58804cf667b6740706/about\nOfsted. 2023. Childcare providers and inspections as at 31 August 2023. Ofsted. [Online]. [Accessed 29 April 2024]. Available from: https://www.gov.uk/government/statistics/childcare-providers-and-inspections-as-at-31-august-2023\nOfsted. 2024. What we do. [Online]. [Accessed 29 April 2024]. Available from: https://www.gov.uk/government/organisations/ofsted\nscikit-learn. 2024. sklearn.metrics.silhouette_score. [Online]. [Accessed 29 April 2024]. Available from: https://scikit-learn.org/stable/modules/generated/sklearn.metrics.silhouette_score.html#sklearn.metrics.silhouette_score"
  },
  {
    "objectID": "projects/index.html#intergenerational-care-facilities",
    "href": "projects/index.html#intergenerational-care-facilities",
    "title": "Projects",
    "section": "Intergenerational care facilities",
    "text": "Intergenerational care facilities\n\n\n\nIdentifying potential priority local authorities in England for intergenerational care facilities\nUsing k-means clustering to analyse current provision of care homes and pre-school facilities to identify opportunity clusters for increasing provision via intergenerational facilities."
  }
]